{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Module 1] Train a Keras Sequential Model\n",
    "\n",
    "본 노트북(notebook)은 SageMaker 상에서 Keras Sequential model을 학습하는 방법을 단계별로 설명합니다. 본 노트북에서 사용한 모델은 간단한 deep CNN(Convolutional Neural Network) 모델로 [the Keras examples](https://github.com/keras-team/keras/blob/master/examples/cifar10_cnn.py)에 소개된 모델과 동일합니다.\n",
    "- 참고로, 본 모델은 25 epoch 학습 후에 검증셋의 정확도(accuracy)가 약 75%이고 50 epoch 학습 후에 검증셋의 정확도가 약 79% 입니다.\n",
    "- 본 워크샵 과정에서는 시간 관계상 5 epoch까지만 학습합니다. (단, Horovod 기반 분산 학습은 10 epoch까지 학습합니다.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The dataset\n",
    "\n",
    "[CIFAR-10 dataset](https://www.cs.toronto.edu/~kriz/cifar.html)은 머신 러닝에서 가장 유명한 데이터셋 중 하나입니다.\n",
    "이 데이터셋은 10개의 다른 클래스로 구성된(클래스당 6,000장) 60,000장의 32x32 픽셀 이미지들로 구성되어 있습니다.\n",
    "아래 그림은 클래스당 10장의 이미지들을 랜덤으로 추출한 결과입니다. \n",
    "\n",
    "![cifar10](https://maet3608.github.io/nuts-ml/_images/cifar10.png)\n",
    "\n",
    "본 실습에서 여러분들은 deep CNN을 학습하여 영상 분류(image classification) 작업을 수행합니다. 다음 노트북들에서\n",
    "여러분들은 File Mode, Pipe Mode와 Horovod 기반 분산 학습(distributed training) 결과를 비교할 것입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "role = get_execution_role()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "role"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estimator가 처음 실행될 때 Amazon ECR 리포지토리(repository)에서 컨테이너 이미지를 다운로드해야 하지만 학습을 즉시 시작할 수 있습니다. 즉, 별도의 학습 클러스터가 프로비저닝 될 때까지 기다릴 필요가 없습니다. 또한 반복 및 테스트시 필요할 수 있는 후속 실행에서 MXNet 또는 TensorFlow 스크립트에 대한 수정 사항이 즉시 실행되기 시작합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using SageMaker for faster training time\n",
    "\n",
    "이번에는 로컬 모드를 사용하지 않고 SageMaker 학습에 GPU 학습 인스턴스를 생성하여 학습 시간을 단축해 봅니다.<br>\n",
    "로컬 모드와 다른 점들은 (1) `train_instance_type`이 로컬 모드의 ‘local’ 대신 여러분이 원하는 특정 인스턴스 유형으로 설정해야 하고, (2) 학습 데이터를 Amazon S3에 업로드 후 학습 경로를 S3 경로로 설정해야 합니다. \n",
    "\n",
    "SageMaker SDK는 S3 업로드를 위한 간단한 함수(`Session.upload_data()`)를 제공합니다. 이 함수를 통해 리턴되는 값은 데이터가 저장된 S3 경로입니다.\n",
    "좀 더 자세한 설정이 필요하다면 SageMaker SDK 대신 boto3를 사용하시면 됩니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://sagemaker-ap-northeast-2-029498593638/data/DEMO-cifar10-new'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset_location = sagemaker_session.upload_data(path='data', key_prefix='data/DEMO-cifar10-new')\n",
    "display(dataset_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train_instance_type has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "train_instance_count has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "train_instance_type has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.tensorflow import TensorFlow\n",
    "estimator = TensorFlow(base_job_name='cifar10',\n",
    "                       entry_point='cifar10_keras_sm.py',\n",
    "                       source_dir='training_script',\n",
    "                       role=role,\n",
    "                       framework_version='1.14.0',\n",
    "                       py_version='py3',\n",
    "                       script_mode=True,                       \n",
    "                       hyperparameters={'epochs': 1},\n",
    "                       train_instance_count=1, \n",
    "                       train_instance_type='ml.p2.xlarge')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "학습을 수행합니다. 이번에는 각각의 채널(`train, validation, eval`)에 S3의 데이터 저장 위치를 지정합니다.<br>\n",
    "학습 완료 후 Billable seconds도 확인해 보세요. Billable seconds는 실제로 학습 수행 시 과금되는 시간입니다.\n",
    "```\n",
    "Billable seconds: <time>\n",
    "```\n",
    "\n",
    "참고로, `ml.p2.xlarge` 인스턴스로 5 epoch 학습 시 전체 6분~7분이 소요되고, 실제 학습에 소요되는 시간은 3분~4분이 소요됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-03-02 07:26:12 Starting - Starting the training job...\n",
      "2021-03-02 07:26:37 Starting - Launching requested ML instancesProfilerReport-1614669972: InProgress\n",
      "......\n",
      "2021-03-02 07:27:38 Starting - Preparing the instances for training.........\n",
      "2021-03-02 07:28:59 Downloading - Downloading input data...\n",
      "2021-03-02 07:29:39 Training - Downloading the training image...\n",
      "2021-03-02 07:30:07 Training - Training image download completed. Training in progress..\u001b[34m/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\u001b[0m\n",
      "\u001b[34m/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\u001b[0m\n",
      "\u001b[34m/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\u001b[0m\n",
      "\u001b[34m/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\u001b[0m\n",
      "\u001b[34m/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\u001b[0m\n",
      "\u001b[34m/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\u001b[0m\n",
      "\u001b[34m/usr/local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\u001b[0m\n",
      "\u001b[34m/usr/local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\u001b[0m\n",
      "\u001b[34m/usr/local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\u001b[0m\n",
      "\u001b[34m/usr/local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\u001b[0m\n",
      "\u001b[34m/usr/local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\u001b[0m\n",
      "\u001b[34m/usr/local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\u001b[0m\n",
      "\u001b[34mUsing TensorFlow backend.\u001b[0m\n",
      "\u001b[34m/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\u001b[0m\n",
      "\u001b[34m/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\u001b[0m\n",
      "\u001b[34m/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\u001b[0m\n",
      "\u001b[34m/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\u001b[0m\n",
      "\u001b[34m/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\u001b[0m\n",
      "\u001b[34m/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\u001b[0m\n",
      "\u001b[34m/usr/local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\u001b[0m\n",
      "\u001b[34m/usr/local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\u001b[0m\n",
      "\u001b[34m/usr/local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\u001b[0m\n",
      "\u001b[34m/usr/local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\u001b[0m\n",
      "\u001b[34m/usr/local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\u001b[0m\n",
      "\u001b[34m/usr/local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\u001b[0m\n",
      "\u001b[34mWARNING: Logging before flag parsing goes to stderr.\u001b[0m\n",
      "\u001b[34mW0302 07:30:14.692715 139729407231744 deprecation_wrapper.py:119] From cifar10_keras_sm.py:31: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\u001b[0m\n",
      "\u001b[34mW0302 07:30:15.681962 139729407231744 deprecation_wrapper.py:119] From cifar10_keras_sm.py:35: The name tf.logging.set_verbosity is deprecated. Please use tf.compat.v1.logging.set_verbosity instead.\n",
      "\u001b[0m\n",
      "\u001b[34mW0302 07:30:15.682407 139729407231744 deprecation_wrapper.py:119] From cifar10_keras_sm.py:35: The name tf.logging.ERROR is deprecated. Please use tf.compat.v1.logging.ERROR instead.\n",
      "\u001b[0m\n",
      "\u001b[34mI0302 07:30:15.684483 139729407231744 cifar10_keras_sm.py:198] getting data\u001b[0m\n",
      "\u001b[34mI0302 07:30:15.991431 139729407231744 cifar10_keras_sm.py:203] configuring model\u001b[0m\n",
      "\u001b[34mI0302 07:30:18.458668 139729407231744 cifar10_keras_sm.py:210] Starting training\u001b[0m\n",
      "\u001b[34mTrain on 128 samples, validate on 128 samples\u001b[0m\n",
      "\u001b[34mEpoch 1/1\u001b[0m\n",
      "\u001b[34m  1/312 [..............................] - ETA: 46:16 - loss: 3.7935 - acc: 0.1094\n",
      "  2/312 [..............................] - ETA: 23:14 - loss: 3.8296 - acc: 0.1016\n",
      "  3/312 [..............................] - ETA: 15:33 - loss: 3.6427 - acc: 0.1094\n",
      "  4/312 [..............................] - ETA: 11:43 - loss: 3.6184 - acc: 0.1035\n",
      "  5/312 [..............................] - ETA: 9:24 - loss: 3.5527 - acc: 0.1016 \n",
      "  6/312 [..............................] - ETA: 7:52 - loss: 3.4744 - acc: 0.1094\n",
      "  7/312 [..............................] - ETA: 6:46 - loss: 3.4116 - acc: 0.1116\n",
      "  8/312 [..............................] - ETA: 5:57 - loss: 3.3258 - acc: 0.1162\n",
      "  9/312 [..............................] - ETA: 5:18 - loss: 3.2727 - acc: 0.1181\n",
      " 10/312 [..............................] - ETA: 4:48 - loss: 3.1971 - acc: 0.1172\n",
      " 11/312 [>.............................] - ETA: 4:23 - loss: 3.1375 - acc: 0.1229\n",
      " 12/312 [>.............................] - ETA: 4:02 - loss: 3.0851 - acc: 0.1257\n",
      " 13/312 [>.............................] - ETA: 3:44 - loss: 3.0380 - acc: 0.1322\u001b[0m\n",
      "\u001b[34m 14/312 [>.............................] - ETA: 3:29 - loss: 2.9910 - acc: 0.1373\n",
      " 15/312 [>.............................] - ETA: 3:15 - loss: 2.9457 - acc: 0.1443\n",
      " 16/312 [>.............................] - ETA: 3:04 - loss: 2.9201 - acc: 0.1484\n",
      " 17/312 [>.............................] - ETA: 2:53 - loss: 2.8853 - acc: 0.1494\n",
      " 18/312 [>.............................] - ETA: 2:44 - loss: 2.8610 - acc: 0.1515\n",
      " 19/312 [>.............................] - ETA: 2:36 - loss: 2.8335 - acc: 0.1525\n",
      " 20/312 [>.............................] - ETA: 2:29 - loss: 2.8070 - acc: 0.1539\n",
      " 21/312 [=>............................] - ETA: 2:22 - loss: 2.7723 - acc: 0.1592\n",
      " 22/312 [=>............................] - ETA: 2:16 - loss: 2.7446 - acc: 0.1602\n",
      " 23/312 [=>............................] - ETA: 2:11 - loss: 2.7290 - acc: 0.1600\n",
      " 24/312 [=>............................] - ETA: 2:05 - loss: 2.7114 - acc: 0.1618\n",
      " 25/312 [=>............................] - ETA: 2:01 - loss: 2.6887 - acc: 0.1619\n",
      " 26/312 [=>............................] - ETA: 1:57 - loss: 2.6682 - acc: 0.1644\n",
      " 27/312 [=>............................] - ETA: 1:52 - loss: 2.6518 - acc: 0.1655\u001b[0m\n",
      "\u001b[34m 28/312 [=>............................] - ETA: 1:49 - loss: 2.6356 - acc: 0.1682\n",
      " 29/312 [=>............................] - ETA: 1:45 - loss: 2.6179 - acc: 0.1705\n",
      " 30/312 [=>............................] - ETA: 1:42 - loss: 2.6014 - acc: 0.1716\n",
      " 31/312 [=>............................] - ETA: 1:39 - loss: 2.5825 - acc: 0.1749\n",
      " 32/312 [==>...........................] - ETA: 1:36 - loss: 2.5713 - acc: 0.1746\n",
      " 33/312 [==>...........................] - ETA: 1:33 - loss: 2.5564 - acc: 0.1757\n",
      " 34/312 [==>...........................] - ETA: 1:31 - loss: 2.5447 - acc: 0.1762\n",
      " 35/312 [==>...........................] - ETA: 1:29 - loss: 2.5302 - acc: 0.1775\n",
      " 36/312 [==>...........................] - ETA: 1:26 - loss: 2.5164 - acc: 0.1803\n",
      " 37/312 [==>...........................] - ETA: 1:24 - loss: 2.5027 - acc: 0.1822\n",
      " 38/312 [==>...........................] - ETA: 1:22 - loss: 2.4937 - acc: 0.1819\n",
      " 39/312 [==>...........................] - ETA: 1:20 - loss: 2.4860 - acc: 0.1837\n",
      " 40/312 [==>...........................] - ETA: 1:18 - loss: 2.4759 - acc: 0.1865\n",
      " 41/312 [==>...........................] - ETA: 1:17 - loss: 2.4649 - acc: 0.1865\n",
      " 42/312 [===>..........................] - ETA: 1:15 - loss: 2.4531 - acc: 0.1881\u001b[0m\n",
      "\u001b[34m 43/312 [===>..........................] - ETA: 1:13 - loss: 2.4463 - acc: 0.1884\n",
      " 44/312 [===>..........................] - ETA: 1:12 - loss: 2.4362 - acc: 0.1896\n",
      " 45/312 [===>..........................] - ETA: 1:10 - loss: 2.4280 - acc: 0.1901\n",
      " 46/312 [===>..........................] - ETA: 1:09 - loss: 2.4200 - acc: 0.1899\n",
      " 47/312 [===>..........................] - ETA: 1:08 - loss: 2.4122 - acc: 0.1920\n",
      " 48/312 [===>..........................] - ETA: 1:06 - loss: 2.4031 - acc: 0.1932\n",
      " 49/312 [===>..........................] - ETA: 1:05 - loss: 2.3970 - acc: 0.1934\n",
      " 50/312 [===>..........................] - ETA: 1:04 - loss: 2.3909 - acc: 0.1953\n",
      " 51/312 [===>..........................] - ETA: 1:03 - loss: 2.3845 - acc: 0.1958\n",
      " 52/312 [====>.........................] - ETA: 1:02 - loss: 2.3785 - acc: 0.1971\n",
      " 53/312 [====>.........................] - ETA: 1:01 - loss: 2.3707 - acc: 0.1984\n",
      " 54/312 [====>.........................] - ETA: 1:00 - loss: 2.3644 - acc: 0.1991\n",
      " 55/312 [====>.........................] - ETA: 59s - loss: 2.3569 - acc: 0.2004 \n",
      " 56/312 [====>.........................] - ETA: 58s - loss: 2.3520 - acc: 0.2008\n",
      " 57/312 [====>.........................] - ETA: 57s - loss: 2.3457 - acc: 0.2019\u001b[0m\n",
      "\u001b[34m 58/312 [====>.........................] - ETA: 56s - loss: 2.3384 - acc: 0.2033\n",
      " 59/312 [====>.........................] - ETA: 55s - loss: 2.3320 - acc: 0.2042\n",
      " 60/312 [====>.........................] - ETA: 54s - loss: 2.3256 - acc: 0.2062\n",
      " 61/312 [====>.........................] - ETA: 53s - loss: 2.3200 - acc: 0.2080\n",
      " 62/312 [====>.........................] - ETA: 52s - loss: 2.3129 - acc: 0.2098\n",
      " 63/312 [=====>........................] - ETA: 52s - loss: 2.3076 - acc: 0.2107\n",
      " 64/312 [=====>........................] - ETA: 51s - loss: 2.3030 - acc: 0.2124\n",
      " 65/312 [=====>........................] - ETA: 50s - loss: 2.2979 - acc: 0.2133\n",
      " 66/312 [=====>........................] - ETA: 49s - loss: 2.2934 - acc: 0.2133\n",
      " 67/312 [=====>........................] - ETA: 49s - loss: 2.2850 - acc: 0.2149\n",
      " 68/312 [=====>........................] - ETA: 48s - loss: 2.2793 - acc: 0.2162\n",
      " 69/312 [=====>........................] - ETA: 47s - loss: 2.2747 - acc: 0.2163\n",
      " 70/312 [=====>........................] - ETA: 47s - loss: 2.2710 - acc: 0.2164\n",
      " 71/312 [=====>........................] - ETA: 46s - loss: 2.2655 - acc: 0.2182\u001b[0m\n",
      "\u001b[34m 72/312 [=====>........................] - ETA: 46s - loss: 2.2599 - acc: 0.2190\n",
      " 73/312 [======>.......................] - ETA: 45s - loss: 2.2553 - acc: 0.2192\n",
      " 74/312 [======>.......................] - ETA: 44s - loss: 2.2489 - acc: 0.2204\n",
      " 75/312 [======>.......................] - ETA: 44s - loss: 2.2457 - acc: 0.2205\n",
      " 76/312 [======>.......................] - ETA: 43s - loss: 2.2421 - acc: 0.2211\n",
      " 77/312 [======>.......................] - ETA: 43s - loss: 2.2377 - acc: 0.2216\n",
      " 78/312 [======>.......................] - ETA: 42s - loss: 2.2336 - acc: 0.2219\n",
      " 79/312 [======>.......................] - ETA: 42s - loss: 2.2290 - acc: 0.2225\n",
      " 80/312 [======>.......................] - ETA: 41s - loss: 2.2248 - acc: 0.2229\n",
      " 81/312 [======>.......................] - ETA: 41s - loss: 2.2185 - acc: 0.2249\n",
      " 82/312 [======>.......................] - ETA: 40s - loss: 2.2143 - acc: 0.2253\n",
      " 83/312 [======>.......................] - ETA: 40s - loss: 2.2101 - acc: 0.2256\n",
      " 84/312 [=======>......................] - ETA: 39s - loss: 2.2066 - acc: 0.2255\n",
      " 85/312 [=======>......................] - ETA: 39s - loss: 2.2024 - acc: 0.2266\u001b[0m\n",
      "\u001b[34m 86/312 [=======>......................] - ETA: 38s - loss: 2.1982 - acc: 0.2274\n",
      " 87/312 [=======>......................] - ETA: 38s - loss: 2.1939 - acc: 0.2280\n",
      " 88/312 [=======>......................] - ETA: 37s - loss: 2.1902 - acc: 0.2290\n",
      " 89/312 [=======>......................] - ETA: 37s - loss: 2.1858 - acc: 0.2300\n",
      " 90/312 [=======>......................] - ETA: 37s - loss: 2.1845 - acc: 0.2301\n",
      " 91/312 [=======>......................] - ETA: 36s - loss: 2.1806 - acc: 0.2313\n",
      " 92/312 [=======>......................] - ETA: 36s - loss: 2.1776 - acc: 0.2313\n",
      " 93/312 [=======>......................] - ETA: 35s - loss: 2.1728 - acc: 0.2324\n",
      " 94/312 [========>.....................] - ETA: 35s - loss: 2.1681 - acc: 0.2330\n",
      " 95/312 [========>.....................] - ETA: 35s - loss: 2.1652 - acc: 0.2335\n",
      " 96/312 [========>.....................] - ETA: 34s - loss: 2.1609 - acc: 0.2349\n",
      " 97/312 [========>.....................] - ETA: 34s - loss: 2.1558 - acc: 0.2359\n",
      " 98/312 [========>.....................] - ETA: 34s - loss: 2.1536 - acc: 0.2361\n",
      " 99/312 [========>.....................] - ETA: 33s - loss: 2.1502 - acc: 0.2371\u001b[0m\n",
      "\u001b[34m100/312 [========>.....................] - ETA: 33s - loss: 2.1483 - acc: 0.2373\u001b[0m\n",
      "\u001b[34m101/312 [========>.....................] - ETA: 33s - loss: 2.1456 - acc: 0.2376\u001b[0m\n",
      "\u001b[34m102/312 [========>.....................] - ETA: 32s - loss: 2.1433 - acc: 0.2382\u001b[0m\n",
      "\u001b[34m103/312 [========>.....................] - ETA: 32s - loss: 2.1401 - acc: 0.2386\u001b[0m\n",
      "\u001b[34m104/312 [=========>....................] - ETA: 31s - loss: 2.1359 - acc: 0.2397\u001b[0m\n",
      "\u001b[34m105/312 [=========>....................] - ETA: 31s - loss: 2.1316 - acc: 0.2404\u001b[0m\n",
      "\u001b[34m106/312 [=========>....................] - ETA: 31s - loss: 2.1284 - acc: 0.2408\u001b[0m\n",
      "\u001b[34m107/312 [=========>....................] - ETA: 31s - loss: 2.1244 - acc: 0.2423\u001b[0m\n",
      "\u001b[34m108/312 [=========>....................] - ETA: 30s - loss: 2.1220 - acc: 0.2428\u001b[0m\n",
      "\u001b[34m109/312 [=========>....................] - ETA: 30s - loss: 2.1196 - acc: 0.2435\u001b[0m\n",
      "\u001b[34m110/312 [=========>....................] - ETA: 30s - loss: 2.1166 - acc: 0.2446\u001b[0m\n",
      "\u001b[34m111/312 [=========>....................] - ETA: 29s - loss: 2.1154 - acc: 0.2447\u001b[0m\n",
      "\u001b[34m112/312 [=========>....................] - ETA: 29s - loss: 2.1125 - acc: 0.2454\u001b[0m\n",
      "\u001b[34m113/312 [=========>....................] - ETA: 29s - loss: 2.1087 - acc: 0.2467\u001b[0m\n",
      "\u001b[34m114/312 [=========>....................] - ETA: 28s - loss: 2.1057 - acc: 0.2475\u001b[0m\n",
      "\u001b[34m115/312 [==========>...................] - ETA: 28s - loss: 2.1030 - acc: 0.2482\u001b[0m\n",
      "\u001b[34m116/312 [==========>...................] - ETA: 28s - loss: 2.1006 - acc: 0.2489\u001b[0m\n",
      "\u001b[34m117/312 [==========>...................] - ETA: 28s - loss: 2.0989 - acc: 0.2493\u001b[0m\n",
      "\u001b[34m118/312 [==========>...................] - ETA: 27s - loss: 2.0969 - acc: 0.2493\u001b[0m\n",
      "\u001b[34m119/312 [==========>...................] - ETA: 27s - loss: 2.0942 - acc: 0.2501\u001b[0m\n",
      "\u001b[34m120/312 [==========>...................] - ETA: 27s - loss: 2.0906 - acc: 0.2509\u001b[0m\n",
      "\u001b[34m121/312 [==========>...................] - ETA: 27s - loss: 2.0891 - acc: 0.2512\u001b[0m\n",
      "\u001b[34m122/312 [==========>...................] - ETA: 26s - loss: 2.0869 - acc: 0.2516\u001b[0m\n",
      "\u001b[34m123/312 [==========>...................] - ETA: 26s - loss: 2.0853 - acc: 0.2518\u001b[0m\n",
      "\u001b[34m124/312 [==========>...................] - ETA: 26s - loss: 2.0840 - acc: 0.2523\u001b[0m\n",
      "\u001b[34m125/312 [===========>..................] - ETA: 26s - loss: 2.0809 - acc: 0.2532\u001b[0m\n",
      "\u001b[34m126/312 [===========>..................] - ETA: 25s - loss: 2.0787 - acc: 0.2537\u001b[0m\n",
      "\u001b[34m127/312 [===========>..................] - ETA: 25s - loss: 2.0755 - acc: 0.2547\u001b[0m\n",
      "\u001b[34m128/312 [===========>..................] - ETA: 25s - loss: 2.0729 - acc: 0.2559\u001b[0m\n",
      "\u001b[34m129/312 [===========>..................] - ETA: 25s - loss: 2.0708 - acc: 0.2567\u001b[0m\n",
      "\u001b[34m130/312 [===========>..................] - ETA: 24s - loss: 2.0691 - acc: 0.2572\u001b[0m\n",
      "\u001b[34m131/312 [===========>..................] - ETA: 24s - loss: 2.0660 - acc: 0.2581\u001b[0m\n",
      "\u001b[34m132/312 [===========>..................] - ETA: 24s - loss: 2.0643 - acc: 0.2583\u001b[0m\n",
      "\u001b[34m133/312 [===========>..................] - ETA: 24s - loss: 2.0630 - acc: 0.2586\u001b[0m\n",
      "\u001b[34m134/312 [===========>..................] - ETA: 24s - loss: 2.0608 - acc: 0.2594\u001b[0m\n",
      "\u001b[34m135/312 [===========>..................] - ETA: 23s - loss: 2.0587 - acc: 0.2603\u001b[0m\n",
      "\u001b[34m136/312 [============>.................] - ETA: 23s - loss: 2.0567 - acc: 0.2608\u001b[0m\n",
      "\u001b[34m137/312 [============>.................] - ETA: 23s - loss: 2.0555 - acc: 0.2610\u001b[0m\n",
      "\u001b[34m138/312 [============>.................] - ETA: 23s - loss: 2.0541 - acc: 0.2612\u001b[0m\n",
      "\u001b[34m139/312 [============>.................] - ETA: 22s - loss: 2.0526 - acc: 0.2611\u001b[0m\n",
      "\u001b[34m140/312 [============>.................] - ETA: 22s - loss: 2.0512 - acc: 0.2613\u001b[0m\n",
      "\u001b[34m141/312 [============>.................] - ETA: 22s - loss: 2.0493 - acc: 0.2616\u001b[0m\n",
      "\u001b[34m142/312 [============>.................] - ETA: 22s - loss: 2.0476 - acc: 0.2620\u001b[0m\n",
      "\u001b[34m143/312 [============>.................] - ETA: 22s - loss: 2.0462 - acc: 0.2622\u001b[0m\n",
      "\u001b[34m144/312 [============>.................] - ETA: 21s - loss: 2.0446 - acc: 0.2625\u001b[0m\n",
      "\u001b[34m145/312 [============>.................] - ETA: 21s - loss: 2.0427 - acc: 0.2635\u001b[0m\n",
      "\u001b[34m146/312 [=============>................] - ETA: 21s - loss: 2.0415 - acc: 0.2636\u001b[0m\n",
      "\u001b[34m147/312 [=============>................] - ETA: 21s - loss: 2.0405 - acc: 0.2636\u001b[0m\n",
      "\u001b[34m148/312 [=============>................] - ETA: 21s - loss: 2.0383 - acc: 0.2639\u001b[0m\n",
      "\u001b[34m149/312 [=============>................] - ETA: 20s - loss: 2.0360 - acc: 0.2648\u001b[0m\n",
      "\u001b[34m150/312 [=============>................] - ETA: 20s - loss: 2.0342 - acc: 0.2655\u001b[0m\n",
      "\u001b[34m151/312 [=============>................] - ETA: 20s - loss: 2.0320 - acc: 0.2662\u001b[0m\n",
      "\u001b[34m152/312 [=============>................] - ETA: 20s - loss: 2.0311 - acc: 0.2661\u001b[0m\n",
      "\u001b[34m153/312 [=============>................] - ETA: 20s - loss: 2.0294 - acc: 0.2665\u001b[0m\n",
      "\u001b[34m155/312 [=============>................] - ETA: 19s - loss: 2.0262 - acc: 0.2672\u001b[0m\n",
      "\u001b[34m157/312 [==============>...............] - ETA: 19s - loss: 2.0230 - acc: 0.2680\u001b[0m\n",
      "\u001b[34m159/312 [==============>...............] - ETA: 18s - loss: 2.0194 - acc: 0.2688\u001b[0m\n",
      "\u001b[34m161/312 [==============>...............] - ETA: 18s - loss: 2.0162 - acc: 0.2694\u001b[0m\n",
      "\u001b[34m163/312 [==============>...............] - ETA: 18s - loss: 2.0131 - acc: 0.2702\u001b[0m\n",
      "\u001b[34m165/312 [==============>...............] - ETA: 17s - loss: 2.0104 - acc: 0.2707\u001b[0m\n",
      "\u001b[34m167/312 [===============>..............] - ETA: 17s - loss: 2.0071 - acc: 0.2718\u001b[0m\n",
      "\u001b[34m169/312 [===============>..............] - ETA: 17s - loss: 2.0042 - acc: 0.2726\u001b[0m\n",
      "\u001b[34m171/312 [===============>..............] - ETA: 16s - loss: 2.0016 - acc: 0.2738\u001b[0m\n",
      "\u001b[34m173/312 [===============>..............] - ETA: 16s - loss: 1.9980 - acc: 0.2746\u001b[0m\n",
      "\u001b[34m175/312 [===============>..............] - ETA: 15s - loss: 1.9940 - acc: 0.2756\u001b[0m\n",
      "\u001b[34m177/312 [================>.............] - ETA: 15s - loss: 1.9904 - acc: 0.2763\u001b[0m\n",
      "\u001b[34m179/312 [================>.............] - ETA: 15s - loss: 1.9868 - acc: 0.2771\u001b[0m\n",
      "\u001b[34m181/312 [================>.............] - ETA: 14s - loss: 1.9839 - acc: 0.2782\u001b[0m\n",
      "\u001b[34m183/312 [================>.............] - ETA: 14s - loss: 1.9822 - acc: 0.2788\u001b[0m\n",
      "\u001b[34m185/312 [================>.............] - ETA: 14s - loss: 1.9791 - acc: 0.2797\u001b[0m\n",
      "\u001b[34m187/312 [================>.............] - ETA: 13s - loss: 1.9771 - acc: 0.2802\u001b[0m\n",
      "\u001b[34m189/312 [=================>............] - ETA: 13s - loss: 1.9748 - acc: 0.2807\u001b[0m\n",
      "\u001b[34m191/312 [=================>............] - ETA: 13s - loss: 1.9705 - acc: 0.2821\u001b[0m\n",
      "\u001b[34m193/312 [=================>............] - ETA: 13s - loss: 1.9677 - acc: 0.2824\u001b[0m\n",
      "\u001b[34m195/312 [=================>............] - ETA: 12s - loss: 1.9655 - acc: 0.2837\u001b[0m\n",
      "\u001b[34m197/312 [=================>............] - ETA: 12s - loss: 1.9621 - acc: 0.2845\u001b[0m\n",
      "\u001b[34m199/312 [==================>...........] - ETA: 12s - loss: 1.9588 - acc: 0.2855\u001b[0m\n",
      "\u001b[34m201/312 [==================>...........] - ETA: 11s - loss: 1.9566 - acc: 0.2864\u001b[0m\n",
      "\u001b[34m203/312 [==================>...........] - ETA: 11s - loss: 1.9538 - acc: 0.2871\u001b[0m\n",
      "\u001b[34m205/312 [==================>...........] - ETA: 11s - loss: 1.9514 - acc: 0.2877\u001b[0m\n",
      "\u001b[34m207/312 [==================>...........] - ETA: 11s - loss: 1.9481 - acc: 0.2880\u001b[0m\n",
      "\u001b[34m209/312 [===================>..........] - ETA: 10s - loss: 1.9455 - acc: 0.2886\u001b[0m\n",
      "\u001b[34m211/312 [===================>..........] - ETA: 10s - loss: 1.9422 - acc: 0.2900\u001b[0m\n",
      "\u001b[34m213/312 [===================>..........] - ETA: 10s - loss: 1.9402 - acc: 0.2906\u001b[0m\n",
      "\u001b[34m215/312 [===================>..........] - ETA: 9s - loss: 1.9390 - acc: 0.2910 \u001b[0m\n",
      "\u001b[34m217/312 [===================>..........] - ETA: 9s - loss: 1.9366 - acc: 0.2916\u001b[0m\n",
      "\u001b[34m219/312 [====================>.........] - ETA: 9s - loss: 1.9353 - acc: 0.2917\u001b[0m\n",
      "\u001b[34m221/312 [====================>.........] - ETA: 9s - loss: 1.9328 - acc: 0.2922\u001b[0m\n",
      "\u001b[34m223/312 [====================>.........] - ETA: 8s - loss: 1.9300 - acc: 0.2930\u001b[0m\n",
      "\u001b[34m225/312 [====================>.........] - ETA: 8s - loss: 1.9283 - acc: 0.2939\u001b[0m\n",
      "\u001b[34m227/312 [====================>.........] - ETA: 8s - loss: 1.9257 - acc: 0.2947\u001b[0m\n",
      "\u001b[34m229/312 [=====================>........] - ETA: 8s - loss: 1.9226 - acc: 0.2959\u001b[0m\n",
      "\u001b[34m231/312 [=====================>........] - ETA: 8s - loss: 1.9200 - acc: 0.2968\u001b[0m\n",
      "\u001b[34m233/312 [=====================>........] - ETA: 7s - loss: 1.9169 - acc: 0.2982\u001b[0m\n",
      "\u001b[34m235/312 [=====================>........] - ETA: 7s - loss: 1.9142 - acc: 0.2992\u001b[0m\n",
      "\u001b[34m237/312 [=====================>........] - ETA: 7s - loss: 1.9130 - acc: 0.2996\u001b[0m\n",
      "\u001b[34m239/312 [=====================>........] - ETA: 7s - loss: 1.9120 - acc: 0.3003\u001b[0m\n",
      "\u001b[34m241/312 [======================>.......] - ETA: 6s - loss: 1.9098 - acc: 0.3010\u001b[0m\n",
      "\u001b[34m243/312 [======================>.......] - ETA: 6s - loss: 1.9075 - acc: 0.3011\u001b[0m\n",
      "\u001b[34m245/312 [======================>.......] - ETA: 6s - loss: 1.9056 - acc: 0.3018\u001b[0m\n",
      "\u001b[34m247/312 [======================>.......] - ETA: 6s - loss: 1.9040 - acc: 0.3023\u001b[0m\n",
      "\u001b[34m249/312 [======================>.......] - ETA: 5s - loss: 1.9012 - acc: 0.3030\u001b[0m\n",
      "\u001b[34m251/312 [=======================>......] - ETA: 5s - loss: 1.8993 - acc: 0.3034\u001b[0m\n",
      "\u001b[34m253/312 [=======================>......] - ETA: 5s - loss: 1.8972 - acc: 0.3040\u001b[0m\n",
      "\u001b[34m255/312 [=======================>......] - ETA: 5s - loss: 1.8951 - acc: 0.3048\u001b[0m\n",
      "\u001b[34m257/312 [=======================>......] - ETA: 5s - loss: 1.8931 - acc: 0.3054\u001b[0m\n",
      "\u001b[34m259/312 [=======================>......] - ETA: 4s - loss: 1.8911 - acc: 0.3061\u001b[0m\n",
      "\u001b[34m261/312 [========================>.....] - ETA: 4s - loss: 1.8884 - acc: 0.3070\u001b[0m\n",
      "\u001b[34m263/312 [========================>.....] - ETA: 4s - loss: 1.8858 - acc: 0.3079\u001b[0m\n",
      "\u001b[34m265/312 [========================>.....] - ETA: 4s - loss: 1.8832 - acc: 0.3085\u001b[0m\n",
      "\u001b[34m267/312 [========================>.....] - ETA: 4s - loss: 1.8804 - acc: 0.3094\u001b[0m\n",
      "\u001b[34m269/312 [========================>.....] - ETA: 3s - loss: 1.8787 - acc: 0.3097\u001b[0m\n",
      "\u001b[34m271/312 [=========================>....] - ETA: 3s - loss: 1.8757 - acc: 0.3109\u001b[0m\n",
      "\u001b[34m273/312 [=========================>....] - ETA: 3s - loss: 1.8733 - acc: 0.3121\u001b[0m\n",
      "\u001b[34m275/312 [=========================>....] - ETA: 3s - loss: 1.8721 - acc: 0.3125\u001b[0m\n",
      "\u001b[34m277/312 [=========================>....] - ETA: 3s - loss: 1.8704 - acc: 0.3133\u001b[0m\n",
      "\u001b[34m279/312 [=========================>....] - ETA: 2s - loss: 1.8683 - acc: 0.3141\u001b[0m\n",
      "\u001b[34m281/312 [==========================>...] - ETA: 2s - loss: 1.8664 - acc: 0.3146\u001b[0m\n",
      "\u001b[34m283/312 [==========================>...] - ETA: 2s - loss: 1.8645 - acc: 0.3150\u001b[0m\n",
      "\u001b[34m285/312 [==========================>...] - ETA: 2s - loss: 1.8623 - acc: 0.3158\u001b[0m\n",
      "\u001b[34m287/312 [==========================>...] - ETA: 2s - loss: 1.8604 - acc: 0.3165\u001b[0m\n",
      "\u001b[34m289/312 [==========================>...] - ETA: 2s - loss: 1.8593 - acc: 0.3167\u001b[0m\n",
      "\u001b[34m291/312 [==========================>...] - ETA: 1s - loss: 1.8582 - acc: 0.3172\u001b[0m\n",
      "\u001b[34m293/312 [===========================>..] - ETA: 1s - loss: 1.8570 - acc: 0.3177\u001b[0m\n",
      "\u001b[34m295/312 [===========================>..] - ETA: 1s - loss: 1.8552 - acc: 0.3182\u001b[0m\n",
      "\u001b[34m297/312 [===========================>..] - ETA: 1s - loss: 1.8535 - acc: 0.3189\u001b[0m\n",
      "\u001b[34m299/312 [===========================>..] - ETA: 1s - loss: 1.8515 - acc: 0.3195\u001b[0m\n",
      "\u001b[34m301/312 [===========================>..] - ETA: 0s - loss: 1.8502 - acc: 0.3199\u001b[0m\n",
      "\u001b[34m303/312 [============================>.] - ETA: 0s - loss: 1.8490 - acc: 0.3206\u001b[0m\n",
      "\u001b[34m305/312 [============================>.] - ETA: 0s - loss: 1.8474 - acc: 0.3213\u001b[0m\n",
      "\u001b[34m307/312 [============================>.] - ETA: 0s - loss: 1.8456 - acc: 0.3221\u001b[0m\n",
      "\u001b[34m309/312 [============================>.] - ETA: 0s - loss: 1.8443 - acc: 0.3223\u001b[0m\n",
      "\u001b[34m311/312 [============================>.] - ETA: 0s - loss: 1.8422 - acc: 0.3231\u001b[0m\n",
      "\u001b[34m312/312 [==============================] - 29s 93ms/step - loss: 1.8416 - acc: 0.3234 - val_loss: 2.0310 - val_acc: 0.2985\u001b[0m\n",
      "\u001b[34mI0302 07:30:52.174526 139729407231744 cifar10_keras_sm.py:219] Test loss:2.0227394745900082\u001b[0m\n",
      "\u001b[34mI0302 07:30:52.174749 139729407231744 cifar10_keras_sm.py:220] Test accuracy:0.30689102564102566\u001b[0m\n",
      "\u001b[34m2021-03-02 07:30:52.557052: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1412] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.\u001b[0m\n",
      "\u001b[34mI0302 07:30:52.872214 139729407231744 cifar10_keras_sm.py:194] Model successfully saved at: /opt/ml/model\u001b[0m\n",
      "\n",
      "2021-03-02 07:31:01 Uploading - Uploading generated training model\n",
      "2021-03-02 07:31:01 Completed - Training job completed\n",
      "Training seconds: 128\n",
      "Billable seconds: 128\n",
      "CPU times: user 637 ms, sys: 47.5 ms, total: 685 ms\n",
      "Wall time: 5min 12s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "estimator.fit({'train':'{}/train'.format(dataset_location),\n",
    "              'validation':'{}/validation'.format(dataset_location),\n",
    "              'eval':'{}/eval'.format(dataset_location)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**잘 하셨습니다.** \n",
    "\n",
    "SageMaker에서 GPU 인스턴스를 사용해 5 epoch를 정상적으로 학습할 수 있었습니다.<br>\n",
    "다음 노트북으로 계속 진행하기 전에 SageMaker 콘솔의 Training jobs 섹션을 살펴보고 여러분이 수행한 job을 찾아 configuration을 확인하세요.\n",
    "\n",
    "스크립트 모드 학습에 대한 자세한 내용은 아래의 AWS 블로그를 참조해 주세요.<br>\n",
    "[Using TensorFlow eager execution with Amazon SageMaker script mode](https://aws.amazon.com/ko/blogs/machine-learning/using-tensorflow-eager-execution-with-amazon-sagemaker-script-mode/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:ap-northeast-2:806072073708:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
