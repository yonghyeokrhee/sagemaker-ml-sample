{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Module 1] Train a Keras Sequential Model\n",
    "\n",
    "본 노트북(notebook)은 SageMaker 상에서 Keras Sequential model을 학습하는 방법을 단계별로 설명합니다. 본 노트북에서 사용한 모델은 간단한 deep CNN(Convolutional Neural Network) 모델로 [the Keras examples](https://github.com/keras-team/keras/blob/master/examples/cifar10_cnn.py)에 소개된 모델과 동일합니다.\n",
    "- 참고로, 본 모델은 25 epoch 학습 후에 검증셋의 정확도(accuracy)가 약 75%이고 50 epoch 학습 후에 검증셋의 정확도가 약 79% 입니다.\n",
    "- 본 워크샵 과정에서는 시간 관계상 5 epoch까지만 학습합니다. (단, Horovod 기반 분산 학습은 10 epoch까지 학습합니다.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The dataset\n",
    "\n",
    "[CIFAR-10 dataset](https://www.cs.toronto.edu/~kriz/cifar.html)은 머신 러닝에서 가장 유명한 데이터셋 중 하나입니다.\n",
    "이 데이터셋은 10개의 다른 클래스로 구성된(클래스당 6,000장) 60,000장의 32x32 픽셀 이미지들로 구성되어 있습니다.\n",
    "아래 그림은 클래스당 10장의 이미지들을 랜덤으로 추출한 결과입니다. \n",
    "\n",
    "![cifar10](https://maet3608.github.io/nuts-ml/_images/cifar10.png)\n",
    "\n",
    "본 실습에서 여러분들은 deep CNN을 학습하여 영상 분류(image classification) 작업을 수행합니다. 다음 노트북들에서\n",
    "여러분들은 File Mode, Pipe Mode와 Horovod 기반 분산 학습(distributed training) 결과를 비교할 것입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "role = get_execution_role()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estimator가 처음 실행될 때 Amazon ECR 리포지토리(repository)에서 컨테이너 이미지를 다운로드해야 하지만 학습을 즉시 시작할 수 있습니다. 즉, 별도의 학습 클러스터가 프로비저닝 될 때까지 기다릴 필요가 없습니다. 또한 반복 및 테스트시 필요할 수 있는 후속 실행에서 MXNet 또는 TensorFlow 스크립트에 대한 수정 사항이 즉시 실행되기 시작합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using SageMaker for faster training time\n",
    "\n",
    "이번에는 로컬 모드를 사용하지 않고 SageMaker 학습에 GPU 학습 인스턴스를 생성하여 학습 시간을 단축해 봅니다.<br>\n",
    "로컬 모드와 다른 점들은 (1) `train_instance_type`이 로컬 모드의 ‘local’ 대신 여러분이 원하는 특정 인스턴스 유형으로 설정해야 하고, (2) 학습 데이터를 Amazon S3에 업로드 후 학습 경로를 S3 경로로 설정해야 합니다. \n",
    "\n",
    "SageMaker SDK는 S3 업로드를 위한 간단한 함수(`Session.upload_data()`)를 제공합니다. 이 함수를 통해 리턴되는 값은 데이터가 저장된 S3 경로입니다.\n",
    "좀 더 자세한 설정이 필요하다면 SageMaker SDK 대신 boto3를 사용하시면 됩니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://sagemaker-ap-northeast-2-227927015541/data/DEMO-cifar10'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset_location = sagemaker_session.upload_data(path='data', key_prefix='data/DEMO-cifar10')\n",
    "display(dataset_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train_instance_type has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "train_instance_count has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "train_instance_type has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.tensorflow import TensorFlow\n",
    "estimator = TensorFlow(base_job_name='cifar10',\n",
    "                       entry_point='cifar10_keras_sm.py',\n",
    "                       source_dir='training_script',\n",
    "                       role=role,\n",
    "                       framework_version='1.14.0',\n",
    "                       py_version='py3',\n",
    "                       script_mode=True,                       \n",
    "                       hyperparameters={'epochs': 1},\n",
    "                       train_instance_count=1, \n",
    "                       train_instance_type='ml.p2.xlarge')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "학습을 수행합니다. 이번에는 각각의 채널(`train, validation, eval`)에 S3의 데이터 저장 위치를 지정합니다.<br>\n",
    "학습 완료 후 Billable seconds도 확인해 보세요. Billable seconds는 실제로 학습 수행 시 과금되는 시간입니다.\n",
    "```\n",
    "Billable seconds: <time>\n",
    "```\n",
    "\n",
    "참고로, `ml.p2.xlarge` 인스턴스로 5 epoch 학습 시 전체 6분~7분이 소요되고, 실제 학습에 소요되는 시간은 3분~4분이 소요됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-02-26 14:53:54 Starting - Starting the training job...\n",
      "2021-02-26 14:54:18 Starting - Launching requested ML instancesProfilerReport-1614351233: InProgress\n",
      "......\n",
      "2021-02-26 14:55:21 Starting - Preparing the instances for training.........\n",
      "2021-02-26 14:56:39 Downloading - Downloading input data...\n",
      "2021-02-26 14:57:20 Training - Downloading the training image...\n",
      "2021-02-26 14:57:52 Training - Training image download completed. Training in progress..\u001b[34m/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\u001b[0m\n",
      "\u001b[34m/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\u001b[0m\n",
      "\u001b[34m/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\u001b[0m\n",
      "\u001b[34m/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\u001b[0m\n",
      "\u001b[34m/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\u001b[0m\n",
      "\u001b[34m/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\u001b[0m\n",
      "\u001b[34m/usr/local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\u001b[0m\n",
      "\u001b[34m/usr/local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\u001b[0m\n",
      "\u001b[34m/usr/local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\u001b[0m\n",
      "\u001b[34m/usr/local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\u001b[0m\n",
      "\u001b[34m/usr/local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\u001b[0m\n",
      "\u001b[34m/usr/local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\u001b[0m\n",
      "\u001b[34mUsing TensorFlow backend.\u001b[0m\n",
      "\u001b[34m/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\u001b[0m\n",
      "\u001b[34m/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\u001b[0m\n",
      "\u001b[34m/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\u001b[0m\n",
      "\u001b[34m/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\u001b[0m\n",
      "\u001b[34m/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\u001b[0m\n",
      "\u001b[34m/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\u001b[0m\n",
      "\u001b[34m/usr/local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\u001b[0m\n",
      "\u001b[34m/usr/local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\u001b[0m\n",
      "\u001b[34m/usr/local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\u001b[0m\n",
      "\u001b[34m/usr/local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\u001b[0m\n",
      "\u001b[34m/usr/local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\u001b[0m\n",
      "\u001b[34m/usr/local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\u001b[0m\n",
      "\u001b[34mWARNING: Logging before flag parsing goes to stderr.\u001b[0m\n",
      "\u001b[34mW0226 14:57:59.425441 139953917273856 deprecation_wrapper.py:119] From cifar10_keras_sm.py:31: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\u001b[0m\n",
      "\u001b[34mW0226 14:58:00.190543 139953917273856 deprecation_wrapper.py:119] From cifar10_keras_sm.py:35: The name tf.logging.set_verbosity is deprecated. Please use tf.compat.v1.logging.set_verbosity instead.\n",
      "\u001b[0m\n",
      "\u001b[34mW0226 14:58:00.190817 139953917273856 deprecation_wrapper.py:119] From cifar10_keras_sm.py:35: The name tf.logging.ERROR is deprecated. Please use tf.compat.v1.logging.ERROR instead.\n",
      "\u001b[0m\n",
      "\u001b[34mI0226 14:58:00.192955 139953917273856 cifar10_keras_sm.py:198] getting data\u001b[0m\n",
      "\u001b[34mI0226 14:58:00.510879 139953917273856 cifar10_keras_sm.py:203] configuring model\u001b[0m\n",
      "\u001b[34mI0226 14:58:03.079663 139953917273856 cifar10_keras_sm.py:210] Starting training\u001b[0m\n",
      "\u001b[34mTrain on 128 samples, validate on 128 samples\u001b[0m\n",
      "\u001b[34mEpoch 1/1\u001b[0m\n",
      "\u001b[34m  1/312 [..............................] - ETA: 46:19 - loss: 4.3806 - acc: 0.1250\n",
      "  2/312 [..............................] - ETA: 23:15 - loss: 4.1684 - acc: 0.1016\n",
      "  3/312 [..............................] - ETA: 15:33 - loss: 3.9606 - acc: 0.1068\n",
      "  4/312 [..............................] - ETA: 11:42 - loss: 3.9804 - acc: 0.1055\n",
      "  5/312 [..............................] - ETA: 9:24 - loss: 4.0004 - acc: 0.1125 \n",
      "  6/312 [..............................] - ETA: 7:52 - loss: 3.8885 - acc: 0.1224\n",
      "  7/312 [..............................] - ETA: 6:46 - loss: 3.7914 - acc: 0.1239\n",
      "  8/312 [..............................] - ETA: 5:56 - loss: 3.7033 - acc: 0.1260\n",
      "  9/312 [..............................] - ETA: 5:18 - loss: 3.6075 - acc: 0.1276\n",
      " 10/312 [..............................] - ETA: 4:47 - loss: 3.5103 - acc: 0.1305\n",
      " 11/312 [>.............................] - ETA: 4:22 - loss: 3.4358 - acc: 0.1314\n",
      " 12/312 [>.............................] - ETA: 4:01 - loss: 3.3749 - acc: 0.1296\n",
      " 13/312 [>.............................] - ETA: 3:43 - loss: 3.3089 - acc: 0.1364\n",
      " 14/312 [>.............................] - ETA: 3:27 - loss: 3.2489 - acc: 0.1378\n",
      " 15/312 [>.............................] - ETA: 3:14 - loss: 3.1884 - acc: 0.1417\u001b[0m\n",
      "\u001b[34m 16/312 [>.............................] - ETA: 3:03 - loss: 3.1516 - acc: 0.1431\n",
      " 17/312 [>.............................] - ETA: 2:52 - loss: 3.1078 - acc: 0.1434\n",
      " 18/312 [>.............................] - ETA: 2:43 - loss: 3.0632 - acc: 0.1454\n",
      " 19/312 [>.............................] - ETA: 2:35 - loss: 3.0291 - acc: 0.1476\n",
      " 20/312 [>.............................] - ETA: 2:28 - loss: 2.9862 - acc: 0.1512\n",
      " 21/312 [=>............................] - ETA: 2:21 - loss: 2.9492 - acc: 0.1522\n",
      " 22/312 [=>............................] - ETA: 2:15 - loss: 2.9217 - acc: 0.1520\n",
      " 23/312 [=>............................] - ETA: 2:09 - loss: 2.8914 - acc: 0.1525\n",
      " 24/312 [=>............................] - ETA: 2:04 - loss: 2.8673 - acc: 0.1533\n",
      " 25/312 [=>............................] - ETA: 2:00 - loss: 2.8388 - acc: 0.1556\n",
      " 26/312 [=>............................] - ETA: 1:55 - loss: 2.8131 - acc: 0.1566\n",
      " 27/312 [=>............................] - ETA: 1:51 - loss: 2.7910 - acc: 0.1571\n",
      " 28/312 [=>............................] - ETA: 1:48 - loss: 2.7696 - acc: 0.1588\n",
      " 29/312 [=>............................] - ETA: 1:44 - loss: 2.7502 - acc: 0.1589\n",
      " 30/312 [=>............................] - ETA: 1:41 - loss: 2.7321 - acc: 0.1602\n",
      " 31/312 [=>............................] - ETA: 1:38 - loss: 2.7113 - acc: 0.1631\u001b[0m\n",
      "\u001b[34m 32/312 [==>...........................] - ETA: 1:35 - loss: 2.6909 - acc: 0.1658\n",
      " 33/312 [==>...........................] - ETA: 1:32 - loss: 2.6744 - acc: 0.1671\n",
      " 34/312 [==>...........................] - ETA: 1:30 - loss: 2.6591 - acc: 0.1687\n",
      " 35/312 [==>...........................] - ETA: 1:27 - loss: 2.6428 - acc: 0.1696\n",
      " 36/312 [==>...........................] - ETA: 1:25 - loss: 2.6296 - acc: 0.1697\n",
      " 37/312 [==>...........................] - ETA: 1:23 - loss: 2.6147 - acc: 0.1706\n",
      " 38/312 [==>...........................] - ETA: 1:21 - loss: 2.5990 - acc: 0.1721\n",
      " 39/312 [==>...........................] - ETA: 1:19 - loss: 2.5871 - acc: 0.1735\n",
      " 40/312 [==>...........................] - ETA: 1:17 - loss: 2.5746 - acc: 0.1752\n",
      " 41/312 [==>...........................] - ETA: 1:15 - loss: 2.5653 - acc: 0.1753\n",
      " 42/312 [===>..........................] - ETA: 1:14 - loss: 2.5542 - acc: 0.1752\n",
      " 43/312 [===>..........................] - ETA: 1:12 - loss: 2.5422 - acc: 0.1773\n",
      " 44/312 [===>..........................] - ETA: 1:11 - loss: 2.5304 - acc: 0.1784\n",
      " 45/312 [===>..........................] - ETA: 1:09 - loss: 2.5205 - acc: 0.1795\n",
      " 46/312 [===>..........................] - ETA: 1:08 - loss: 2.5089 - acc: 0.1814\n",
      " 47/312 [===>..........................] - ETA: 1:06 - loss: 2.4992 - acc: 0.1825\u001b[0m\n",
      "\u001b[34m 48/312 [===>..........................] - ETA: 1:05 - loss: 2.4877 - acc: 0.1844\n",
      " 49/312 [===>..........................] - ETA: 1:04 - loss: 2.4785 - acc: 0.1854\n",
      " 50/312 [===>..........................] - ETA: 1:03 - loss: 2.4695 - acc: 0.1867\n",
      " 51/312 [===>..........................] - ETA: 1:02 - loss: 2.4607 - acc: 0.1875\n",
      " 52/312 [====>.........................] - ETA: 1:00 - loss: 2.4515 - acc: 0.1890\n",
      " 53/312 [====>.........................] - ETA: 59s - loss: 2.4440 - acc: 0.1897 \n",
      " 54/312 [====>.........................] - ETA: 58s - loss: 2.4361 - acc: 0.1910\n",
      " 55/312 [====>.........................] - ETA: 58s - loss: 2.4294 - acc: 0.1911\n",
      " 56/312 [====>.........................] - ETA: 57s - loss: 2.4195 - acc: 0.1936\n",
      " 57/312 [====>.........................] - ETA: 56s - loss: 2.4093 - acc: 0.1957\n",
      " 58/312 [====>.........................] - ETA: 55s - loss: 2.4013 - acc: 0.1972\n",
      " 59/312 [====>.........................] - ETA: 54s - loss: 2.3957 - acc: 0.1981\n",
      " 60/312 [====>.........................] - ETA: 53s - loss: 2.3885 - acc: 0.1990\n",
      " 61/312 [====>.........................] - ETA: 52s - loss: 2.3841 - acc: 0.1985\n",
      " 62/312 [====>.........................] - ETA: 51s - loss: 2.3759 - acc: 0.1998\u001b[0m\n",
      "\u001b[34m 63/312 [=====>........................] - ETA: 51s - loss: 2.3708 - acc: 0.2005\n",
      " 64/312 [=====>........................] - ETA: 50s - loss: 2.3657 - acc: 0.2009\n",
      " 65/312 [=====>........................] - ETA: 49s - loss: 2.3592 - acc: 0.2014\n",
      " 66/312 [=====>........................] - ETA: 48s - loss: 2.3528 - acc: 0.2024\n",
      " 67/312 [=====>........................] - ETA: 48s - loss: 2.3460 - acc: 0.2029\n",
      " 68/312 [=====>........................] - ETA: 47s - loss: 2.3384 - acc: 0.2046\n",
      " 69/312 [=====>........................] - ETA: 46s - loss: 2.3326 - acc: 0.2056\n",
      " 70/312 [=====>........................] - ETA: 46s - loss: 2.3278 - acc: 0.2062\n",
      " 71/312 [=====>........................] - ETA: 45s - loss: 2.3241 - acc: 0.2070\n",
      " 72/312 [=====>........................] - ETA: 45s - loss: 2.3208 - acc: 0.2068\n",
      " 73/312 [======>.......................] - ETA: 44s - loss: 2.3157 - acc: 0.2076\n",
      " 74/312 [======>.......................] - ETA: 43s - loss: 2.3110 - acc: 0.2080\n",
      " 75/312 [======>.......................] - ETA: 43s - loss: 2.3054 - acc: 0.2085\n",
      " 76/312 [======>.......................] - ETA: 42s - loss: 2.3001 - acc: 0.2094\n",
      " 77/312 [======>.......................] - ETA: 42s - loss: 2.2952 - acc: 0.2106\u001b[0m\n",
      "\u001b[34m 78/312 [======>.......................] - ETA: 41s - loss: 2.2902 - acc: 0.2116\n",
      " 79/312 [======>.......................] - ETA: 41s - loss: 2.2862 - acc: 0.2122\n",
      " 80/312 [======>.......................] - ETA: 40s - loss: 2.2822 - acc: 0.2125\n",
      " 81/312 [======>.......................] - ETA: 40s - loss: 2.2770 - acc: 0.2129\n",
      " 82/312 [======>.......................] - ETA: 39s - loss: 2.2717 - acc: 0.2137\n",
      " 83/312 [======>.......................] - ETA: 39s - loss: 2.2672 - acc: 0.2140\n",
      " 84/312 [=======>......................] - ETA: 38s - loss: 2.2646 - acc: 0.2138\n",
      " 85/312 [=======>......................] - ETA: 38s - loss: 2.2596 - acc: 0.2144\n",
      " 86/312 [=======>......................] - ETA: 37s - loss: 2.2549 - acc: 0.2155\n",
      " 87/312 [=======>......................] - ETA: 37s - loss: 2.2498 - acc: 0.2169\n",
      " 88/312 [=======>......................] - ETA: 37s - loss: 2.2460 - acc: 0.2180\n",
      " 89/312 [=======>......................] - ETA: 36s - loss: 2.2432 - acc: 0.2179\n",
      " 90/312 [=======>......................] - ETA: 36s - loss: 2.2399 - acc: 0.2182\n",
      " 91/312 [=======>......................] - ETA: 35s - loss: 2.2383 - acc: 0.2180\n",
      " 92/312 [=======>......................] - ETA: 35s - loss: 2.2357 - acc: 0.2182\u001b[0m\n",
      "\u001b[34m 93/312 [=======>......................] - ETA: 35s - loss: 2.2302 - acc: 0.2193\n",
      " 94/312 [========>.....................] - ETA: 34s - loss: 2.2256 - acc: 0.2206\n",
      " 95/312 [========>.....................] - ETA: 34s - loss: 2.2202 - acc: 0.2216\n",
      " 96/312 [========>.....................] - ETA: 33s - loss: 2.2171 - acc: 0.2219\n",
      " 97/312 [========>.....................] - ETA: 33s - loss: 2.2139 - acc: 0.2225\n",
      " 98/312 [========>.....................] - ETA: 33s - loss: 2.2109 - acc: 0.2231\n",
      " 99/312 [========>.....................] - ETA: 32s - loss: 2.2070 - acc: 0.2242\u001b[0m\n",
      "\u001b[34m100/312 [========>.....................] - ETA: 32s - loss: 2.2040 - acc: 0.2247\u001b[0m\n",
      "\u001b[34m101/312 [========>.....................] - ETA: 32s - loss: 2.2001 - acc: 0.2254\u001b[0m\n",
      "\u001b[34m102/312 [========>.....................] - ETA: 31s - loss: 2.1967 - acc: 0.2262\u001b[0m\n",
      "\u001b[34m103/312 [========>.....................] - ETA: 31s - loss: 2.1930 - acc: 0.2262\u001b[0m\n",
      "\u001b[34m104/312 [=========>....................] - ETA: 31s - loss: 2.1900 - acc: 0.2263\u001b[0m\n",
      "\u001b[34m105/312 [=========>....................] - ETA: 30s - loss: 2.1870 - acc: 0.2265\u001b[0m\n",
      "\u001b[34m106/312 [=========>....................] - ETA: 30s - loss: 2.1847 - acc: 0.2259\u001b[0m\n",
      "\u001b[34m107/312 [=========>....................] - ETA: 30s - loss: 2.1814 - acc: 0.2264\u001b[0m\n",
      "\u001b[34m108/312 [=========>....................] - ETA: 29s - loss: 2.1795 - acc: 0.2263\u001b[0m\n",
      "\u001b[34m109/312 [=========>....................] - ETA: 29s - loss: 2.1767 - acc: 0.2265\u001b[0m\n",
      "\u001b[34m110/312 [=========>....................] - ETA: 29s - loss: 2.1734 - acc: 0.2269\u001b[0m\n",
      "\u001b[34m111/312 [=========>....................] - ETA: 29s - loss: 2.1710 - acc: 0.2266\u001b[0m\n",
      "\u001b[34m112/312 [=========>....................] - ETA: 28s - loss: 2.1691 - acc: 0.2268\u001b[0m\n",
      "\u001b[34m113/312 [=========>....................] - ETA: 28s - loss: 2.1664 - acc: 0.2275\u001b[0m\n",
      "\u001b[34m114/312 [=========>....................] - ETA: 28s - loss: 2.1637 - acc: 0.2284\u001b[0m\n",
      "\u001b[34m115/312 [==========>...................] - ETA: 27s - loss: 2.1614 - acc: 0.2289\u001b[0m\n",
      "\u001b[34m116/312 [==========>...................] - ETA: 27s - loss: 2.1580 - acc: 0.2295\u001b[0m\n",
      "\u001b[34m117/312 [==========>...................] - ETA: 27s - loss: 2.1557 - acc: 0.2301\u001b[0m\n",
      "\u001b[34m118/312 [==========>...................] - ETA: 27s - loss: 2.1535 - acc: 0.2311\u001b[0m\n",
      "\u001b[34m119/312 [==========>...................] - ETA: 26s - loss: 2.1502 - acc: 0.2317\u001b[0m\n",
      "\u001b[34m120/312 [==========>...................] - ETA: 26s - loss: 2.1481 - acc: 0.2323\u001b[0m\n",
      "\u001b[34m121/312 [==========>...................] - ETA: 26s - loss: 2.1452 - acc: 0.2327\u001b[0m\n",
      "\u001b[34m122/312 [==========>...................] - ETA: 26s - loss: 2.1433 - acc: 0.2339\u001b[0m\n",
      "\u001b[34m123/312 [==========>...................] - ETA: 25s - loss: 2.1415 - acc: 0.2345\u001b[0m\n",
      "\u001b[34m124/312 [==========>...................] - ETA: 25s - loss: 2.1395 - acc: 0.2351\u001b[0m\n",
      "\u001b[34m125/312 [===========>..................] - ETA: 25s - loss: 2.1376 - acc: 0.2351\u001b[0m\n",
      "\u001b[34m126/312 [===========>..................] - ETA: 25s - loss: 2.1353 - acc: 0.2352\u001b[0m\n",
      "\u001b[34m127/312 [===========>..................] - ETA: 24s - loss: 2.1333 - acc: 0.2359\u001b[0m\n",
      "\u001b[34m128/312 [===========>..................] - ETA: 24s - loss: 2.1318 - acc: 0.2359\u001b[0m\n",
      "\u001b[34m129/312 [===========>..................] - ETA: 24s - loss: 2.1297 - acc: 0.2364\u001b[0m\n",
      "\u001b[34m130/312 [===========>..................] - ETA: 24s - loss: 2.1271 - acc: 0.2371\u001b[0m\n",
      "\u001b[34m131/312 [===========>..................] - ETA: 23s - loss: 2.1246 - acc: 0.2381\u001b[0m\n",
      "\u001b[34m132/312 [===========>..................] - ETA: 23s - loss: 2.1224 - acc: 0.2385\u001b[0m\n",
      "\u001b[34m133/312 [===========>..................] - ETA: 23s - loss: 2.1200 - acc: 0.2391\u001b[0m\n",
      "\u001b[34m134/312 [===========>..................] - ETA: 23s - loss: 2.1178 - acc: 0.2396\u001b[0m\n",
      "\u001b[34m135/312 [===========>..................] - ETA: 23s - loss: 2.1151 - acc: 0.2402\u001b[0m\n",
      "\u001b[34m136/312 [============>.................] - ETA: 22s - loss: 2.1131 - acc: 0.2404\u001b[0m\n",
      "\u001b[34m137/312 [============>.................] - ETA: 22s - loss: 2.1115 - acc: 0.2405\u001b[0m\n",
      "\u001b[34m138/312 [============>.................] - ETA: 22s - loss: 2.1088 - acc: 0.2410\u001b[0m\n",
      "\u001b[34m139/312 [============>.................] - ETA: 22s - loss: 2.1058 - acc: 0.2413\u001b[0m\n",
      "\u001b[34m140/312 [============>.................] - ETA: 21s - loss: 2.1035 - acc: 0.2416\u001b[0m\n",
      "\u001b[34m141/312 [============>.................] - ETA: 21s - loss: 2.1015 - acc: 0.2417\u001b[0m\n",
      "\u001b[34m142/312 [============>.................] - ETA: 21s - loss: 2.0995 - acc: 0.2427\u001b[0m\n",
      "\u001b[34m143/312 [============>.................] - ETA: 21s - loss: 2.0970 - acc: 0.2430\u001b[0m\n",
      "\u001b[34m144/312 [============>.................] - ETA: 21s - loss: 2.0959 - acc: 0.2431\u001b[0m\n",
      "\u001b[34m145/312 [============>.................] - ETA: 20s - loss: 2.0936 - acc: 0.2437\u001b[0m\n",
      "\u001b[34m146/312 [=============>................] - ETA: 20s - loss: 2.0913 - acc: 0.2445\u001b[0m\n",
      "\u001b[34m147/312 [=============>................] - ETA: 20s - loss: 2.0886 - acc: 0.2454\u001b[0m\n",
      "\u001b[34m148/312 [=============>................] - ETA: 20s - loss: 2.0871 - acc: 0.2457\u001b[0m\n",
      "\u001b[34m149/312 [=============>................] - ETA: 20s - loss: 2.0851 - acc: 0.2462\u001b[0m\n",
      "\u001b[34m150/312 [=============>................] - ETA: 20s - loss: 2.0835 - acc: 0.2465\u001b[0m\n",
      "\u001b[34m151/312 [=============>................] - ETA: 19s - loss: 2.0815 - acc: 0.2472\u001b[0m\n",
      "\u001b[34m152/312 [=============>................] - ETA: 19s - loss: 2.0794 - acc: 0.2479\u001b[0m\n",
      "\u001b[34m153/312 [=============>................] - ETA: 19s - loss: 2.0770 - acc: 0.2488\u001b[0m\n",
      "\u001b[34m155/312 [=============>................] - ETA: 19s - loss: 2.0742 - acc: 0.2492\u001b[0m\n",
      "\u001b[34m157/312 [==============>...............] - ETA: 18s - loss: 2.0704 - acc: 0.2505\u001b[0m\n",
      "\u001b[34m159/312 [==============>...............] - ETA: 18s - loss: 2.0666 - acc: 0.2515\u001b[0m\n",
      "\u001b[34m161/312 [==============>...............] - ETA: 17s - loss: 2.0633 - acc: 0.2519\u001b[0m\n",
      "\u001b[34m163/312 [==============>...............] - ETA: 17s - loss: 2.0608 - acc: 0.2524\u001b[0m\n",
      "\u001b[34m165/312 [==============>...............] - ETA: 17s - loss: 2.0568 - acc: 0.2535\u001b[0m\n",
      "\u001b[34m167/312 [===============>..............] - ETA: 16s - loss: 2.0525 - acc: 0.2547\u001b[0m\n",
      "\u001b[34m169/312 [===============>..............] - ETA: 16s - loss: 2.0494 - acc: 0.2552\u001b[0m\n",
      "\u001b[34m171/312 [===============>..............] - ETA: 16s - loss: 2.0467 - acc: 0.2562\u001b[0m\n",
      "\u001b[34m173/312 [===============>..............] - ETA: 15s - loss: 2.0430 - acc: 0.2566\u001b[0m\n",
      "\u001b[34m175/312 [===============>..............] - ETA: 15s - loss: 2.0404 - acc: 0.2566\u001b[0m\n",
      "\u001b[34m177/312 [================>.............] - ETA: 15s - loss: 2.0359 - acc: 0.2579\u001b[0m\n",
      "\u001b[34m179/312 [================>.............] - ETA: 14s - loss: 2.0335 - acc: 0.2585\u001b[0m\n",
      "\u001b[34m181/312 [================>.............] - ETA: 14s - loss: 2.0307 - acc: 0.2594\u001b[0m\n",
      "\u001b[34m183/312 [================>.............] - ETA: 14s - loss: 2.0264 - acc: 0.2607\u001b[0m\n",
      "\u001b[34m185/312 [================>.............] - ETA: 13s - loss: 2.0232 - acc: 0.2614\u001b[0m\n",
      "\u001b[34m187/312 [================>.............] - ETA: 13s - loss: 2.0195 - acc: 0.2621\u001b[0m\n",
      "\u001b[34m189/312 [=================>............] - ETA: 13s - loss: 2.0169 - acc: 0.2628\u001b[0m\n",
      "\u001b[34m191/312 [=================>............] - ETA: 12s - loss: 2.0138 - acc: 0.2638\u001b[0m\n",
      "\u001b[34m193/312 [=================>............] - ETA: 12s - loss: 2.0105 - acc: 0.2652\u001b[0m\n",
      "\u001b[34m195/312 [=================>............] - ETA: 12s - loss: 2.0066 - acc: 0.2666\u001b[0m\n",
      "\u001b[34m197/312 [=================>............] - ETA: 11s - loss: 2.0033 - acc: 0.2673\u001b[0m\n",
      "\u001b[34m199/312 [==================>...........] - ETA: 11s - loss: 2.0009 - acc: 0.2680\u001b[0m\n",
      "\u001b[34m201/312 [==================>...........] - ETA: 11s - loss: 1.9980 - acc: 0.2688\u001b[0m\n",
      "\u001b[34m203/312 [==================>...........] - ETA: 11s - loss: 1.9947 - acc: 0.2699\u001b[0m\n",
      "\u001b[34m205/312 [==================>...........] - ETA: 10s - loss: 1.9912 - acc: 0.2712\u001b[0m\n",
      "\u001b[34m207/312 [==================>...........] - ETA: 10s - loss: 1.9899 - acc: 0.2717\u001b[0m\n",
      "\u001b[34m209/312 [===================>..........] - ETA: 10s - loss: 1.9887 - acc: 0.2720\u001b[0m\n",
      "\u001b[34m211/312 [===================>..........] - ETA: 10s - loss: 1.9861 - acc: 0.2728\u001b[0m\n",
      "\u001b[34m213/312 [===================>..........] - ETA: 9s - loss: 1.9832 - acc: 0.2736 \u001b[0m\n",
      "\u001b[34m215/312 [===================>..........] - ETA: 9s - loss: 1.9807 - acc: 0.2743\u001b[0m\n",
      "\u001b[34m217/312 [===================>..........] - ETA: 9s - loss: 1.9783 - acc: 0.2747\u001b[0m\n",
      "\u001b[34m219/312 [====================>.........] - ETA: 9s - loss: 1.9761 - acc: 0.2754\u001b[0m\n",
      "\u001b[34m221/312 [====================>.........] - ETA: 8s - loss: 1.9737 - acc: 0.2760\u001b[0m\n",
      "\u001b[34m223/312 [====================>.........] - ETA: 8s - loss: 1.9705 - acc: 0.2771\u001b[0m\n",
      "\u001b[34m225/312 [====================>.........] - ETA: 8s - loss: 1.9673 - acc: 0.2779\u001b[0m\n",
      "\u001b[34m227/312 [====================>.........] - ETA: 8s - loss: 1.9651 - acc: 0.2786\u001b[0m\n",
      "\u001b[34m229/312 [=====================>........] - ETA: 7s - loss: 1.9633 - acc: 0.2792\u001b[0m\n",
      "\u001b[34m231/312 [=====================>........] - ETA: 7s - loss: 1.9603 - acc: 0.2801\u001b[0m\n",
      "\u001b[34m233/312 [=====================>........] - ETA: 7s - loss: 1.9585 - acc: 0.2806\u001b[0m\n",
      "\u001b[34m235/312 [=====================>........] - ETA: 7s - loss: 1.9554 - acc: 0.2817\u001b[0m\n",
      "\u001b[34m237/312 [=====================>........] - ETA: 6s - loss: 1.9540 - acc: 0.2820\u001b[0m\n",
      "\u001b[34m239/312 [=====================>........] - ETA: 6s - loss: 1.9512 - acc: 0.2829\u001b[0m\n",
      "\u001b[34m241/312 [======================>.......] - ETA: 6s - loss: 1.9491 - acc: 0.2836\u001b[0m\n",
      "\u001b[34m243/312 [======================>.......] - ETA: 6s - loss: 1.9473 - acc: 0.2843\u001b[0m\n",
      "\u001b[34m245/312 [======================>.......] - ETA: 6s - loss: 1.9452 - acc: 0.2850\u001b[0m\n",
      "\u001b[34m247/312 [======================>.......] - ETA: 5s - loss: 1.9439 - acc: 0.2856\u001b[0m\n",
      "\u001b[34m249/312 [======================>.......] - ETA: 5s - loss: 1.9425 - acc: 0.2859\u001b[0m\n",
      "\u001b[34m251/312 [=======================>......] - ETA: 5s - loss: 1.9406 - acc: 0.2863\u001b[0m\n",
      "\u001b[34m253/312 [=======================>......] - ETA: 5s - loss: 1.9388 - acc: 0.2868\u001b[0m\n",
      "\u001b[34m255/312 [=======================>......] - ETA: 5s - loss: 1.9373 - acc: 0.2872\u001b[0m\n",
      "\u001b[34m257/312 [=======================>......] - ETA: 4s - loss: 1.9344 - acc: 0.2881\u001b[0m\n",
      "\u001b[34m259/312 [=======================>......] - ETA: 4s - loss: 1.9323 - acc: 0.2887\u001b[0m\n",
      "\u001b[34m261/312 [========================>.....] - ETA: 4s - loss: 1.9302 - acc: 0.2893\u001b[0m\n",
      "\u001b[34m263/312 [========================>.....] - ETA: 4s - loss: 1.9280 - acc: 0.2899\u001b[0m\n",
      "\u001b[34m265/312 [========================>.....] - ETA: 4s - loss: 1.9258 - acc: 0.2906\u001b[0m\n",
      "\u001b[34m267/312 [========================>.....] - ETA: 3s - loss: 1.9237 - acc: 0.2913\u001b[0m\n",
      "\u001b[34m269/312 [========================>.....] - ETA: 3s - loss: 1.9211 - acc: 0.2924\u001b[0m\n",
      "\u001b[34m271/312 [=========================>....] - ETA: 3s - loss: 1.9191 - acc: 0.2931\u001b[0m\n",
      "\u001b[34m273/312 [=========================>....] - ETA: 3s - loss: 1.9168 - acc: 0.2940\u001b[0m\n",
      "\u001b[34m275/312 [=========================>....] - ETA: 3s - loss: 1.9147 - acc: 0.2947\u001b[0m\n",
      "\u001b[34m277/312 [=========================>....] - ETA: 2s - loss: 1.9121 - acc: 0.2954\u001b[0m\n",
      "\u001b[34m279/312 [=========================>....] - ETA: 2s - loss: 1.9096 - acc: 0.2962\u001b[0m\n",
      "\u001b[34m281/312 [==========================>...] - ETA: 2s - loss: 1.9072 - acc: 0.2967\u001b[0m\n",
      "\u001b[34m283/312 [==========================>...] - ETA: 2s - loss: 1.9049 - acc: 0.2976\u001b[0m\n",
      "\u001b[34m285/312 [==========================>...] - ETA: 2s - loss: 1.9032 - acc: 0.2984\u001b[0m\n",
      "\u001b[34m287/312 [==========================>...] - ETA: 2s - loss: 1.9013 - acc: 0.2990\u001b[0m\n",
      "\u001b[34m289/312 [==========================>...] - ETA: 1s - loss: 1.8998 - acc: 0.2996\u001b[0m\n",
      "\u001b[34m291/312 [==========================>...] - ETA: 1s - loss: 1.8976 - acc: 0.3006\u001b[0m\n",
      "\u001b[34m293/312 [===========================>..] - ETA: 1s - loss: 1.8953 - acc: 0.3010\u001b[0m\n",
      "\u001b[34m295/312 [===========================>..] - ETA: 1s - loss: 1.8928 - acc: 0.3018\u001b[0m\n",
      "\u001b[34m297/312 [===========================>..] - ETA: 1s - loss: 1.8918 - acc: 0.3022\u001b[0m\n",
      "\u001b[34m299/312 [===========================>..] - ETA: 1s - loss: 1.8888 - acc: 0.3033\u001b[0m\n",
      "\u001b[34m301/312 [===========================>..] - ETA: 0s - loss: 1.8870 - acc: 0.3040\u001b[0m\n",
      "\u001b[34m303/312 [============================>.] - ETA: 0s - loss: 1.8852 - acc: 0.3046\u001b[0m\n",
      "\u001b[34m305/312 [============================>.] - ETA: 0s - loss: 1.8833 - acc: 0.3054\u001b[0m\n",
      "\u001b[34m307/312 [============================>.] - ETA: 0s - loss: 1.8815 - acc: 0.3060\u001b[0m\n",
      "\u001b[34m309/312 [============================>.] - ETA: 0s - loss: 1.8799 - acc: 0.3066\u001b[0m\n",
      "\u001b[34m311/312 [============================>.] - ETA: 0s - loss: 1.8778 - acc: 0.3075\u001b[0m\n",
      "\u001b[34m312/312 [==============================] - 28s 89ms/step - loss: 1.8772 - acc: 0.3077 - val_loss: 1.7211 - val_acc: 0.3489\u001b[0m\n",
      "\u001b[34mI0226 14:58:35.726286 139953917273856 cifar10_keras_sm.py:219] Test loss:1.707933315863976\u001b[0m\n",
      "\u001b[34mI0226 14:58:35.726505 139953917273856 cifar10_keras_sm.py:220] Test accuracy:0.3430488782051282\u001b[0m\n",
      "\u001b[34m2021-02-26 14:58:36.126571: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1412] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.\u001b[0m\n",
      "\u001b[34mI0226 14:58:36.450210 139953917273856 cifar10_keras_sm.py:194] Model successfully saved at: /opt/ml/model\u001b[0m\n",
      "\n",
      "2021-02-26 14:58:46 Uploading - Uploading generated training model\n",
      "2021-02-26 14:58:46 Completed - Training job completed\n",
      "Training seconds: 127\n",
      "Billable seconds: 127\n",
      "CPU times: user 894 ms, sys: 15 ms, total: 909 ms\n",
      "Wall time: 5min 12s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "estimator.fit({'train':'{}/train'.format(dataset_location),\n",
    "              'validation':'{}/validation'.format(dataset_location),\n",
    "              'eval':'{}/eval'.format(dataset_location)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**잘 하셨습니다.** \n",
    "\n",
    "SageMaker에서 GPU 인스턴스를 사용해 5 epoch를 정상적으로 학습할 수 있었습니다.<br>\n",
    "다음 노트북으로 계속 진행하기 전에 SageMaker 콘솔의 Training jobs 섹션을 살펴보고 여러분이 수행한 job을 찾아 configuration을 확인하세요.\n",
    "\n",
    "스크립트 모드 학습에 대한 자세한 내용은 아래의 AWS 블로그를 참조해 주세요.<br>\n",
    "[Using TensorFlow eager execution with Amazon SageMaker script mode](https://aws.amazon.com/ko/blogs/machine-learning/using-tensorflow-eager-execution-with-amazon-sagemaker-script-mode/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
