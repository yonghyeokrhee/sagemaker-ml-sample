{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SageMaker Data Wrangler Job Notebook\n",
    "This notebook uses the Data Wrangler .flow file to submit a SageMaker Data Wrangler Job\n",
    "with the following steps:\n",
    "\n",
    "* Push Data Wrangler .flow file to S3\n",
    "* Parse the .flow file inputs, and create the argument dictionary to submit to a boto client\n",
    "* Submit the ProcessingJob arguments and wait for Job completion\n",
    "\n",
    "Optionally, the notebook also gives an example of starting a SageMaker XGBoost TrainingJob using\n",
    "the newly processed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SageMaker Python SDK version 2.x is required\n",
    "import pkg_resources\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "original_version = pkg_resources.get_distribution(\"sagemaker\").version\n",
    "_ = subprocess.check_call(\n",
    "    [sys.executable, \"-m\", \"pip\", \"install\", \"sagemaker==2.20.0\"]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import time\n",
    "import uuid\n",
    "\n",
    "import boto3\n",
    "import sagemaker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters\n",
    "The following lists configurable parameters that are used throughout this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# S3 bucket for saving processing job outputs\n",
    "# Feel free to specify a different bucket here if you wish.\n",
    "sess = sagemaker.Session()\n",
    "bucket = sess.default_bucket()\n",
    "prefix = \"data_wrangler_flows\"\n",
    "flow_id = f\"{time.strftime('%d-%H-%M-%S', time.gmtime())}-{str(uuid.uuid4())[:8]}\"\n",
    "flow_name = f\"flow-{flow_id}\"\n",
    "flow_uri = f\"s3://{bucket}/{prefix}/{flow_name}.flow\"\n",
    "\n",
    "flow_file_name = \"interactions.flow\"\n",
    "\n",
    "iam_role = sagemaker.get_execution_role()\n",
    "\n",
    "container_uri = \"131546521161.dkr.ecr.ap-northeast-2.amazonaws.com/sagemaker-data-wrangler-container:1.2.1\"\n",
    "\n",
    "# Processing Job Resources Configurations\n",
    "# Data wrangler processing job only supports 1 instance.\n",
    "instance_count = 1\n",
    "instance_type = \"ml.m5.4xlarge\"\n",
    "\n",
    "# Processing Job Path URI Information\n",
    "output_prefix = f\"export-{flow_name}/output\"\n",
    "output_path = f\"s3://{bucket}/{output_prefix}\"\n",
    "output_name = \"410bb997-de0d-4339-8039-53e1d78e46f7.default\"\n",
    "\n",
    "processing_job_name = f\"data-wrangler-flow-processing-{flow_id}\"\n",
    "\n",
    "processing_dir = \"/opt/ml/processing\"\n",
    "\n",
    "# Modify the variable below to specify the content type to be used for writing each output\n",
    "# Currently supported options are 'CSV' or 'PARQUET', and default to 'CSV'\n",
    "output_content_type = \"CSV\"\n",
    "\n",
    "# URL to use for sagemaker client.\n",
    "# If this is None, boto will automatically construct the appropriate URL to use\n",
    "# when communicating with sagemaker.\n",
    "sagemaker_endpoint_url = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Push Flow to S3\n",
    "Use the following cell to upload the Data Wrangler .flow file to Amazon S3 so that\n",
    "it can be used as an input to the processing job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Wrangler Flow notebook uploaded to s3://sagemaker-ap-northeast-2-029498593638/data_wrangler_flows/flow-02-05-22-00-ca04653e.flow\n"
     ]
    }
   ],
   "source": [
    "# Load .flow file\n",
    "with open(flow_file_name) as f:\n",
    "    flow = json.load(f)\n",
    "\n",
    "# Upload to S3\n",
    "s3_client = boto3.client(\"s3\")\n",
    "s3_client.upload_file(flow_file_name, bucket, f\"{prefix}/{flow_name}.flow\")\n",
    "\n",
    "print(f\"Data Wrangler Flow notebook uploaded to {flow_uri}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Processing Job arguments\n",
    "\n",
    "This notebook submits a Processing Job using the Sagmaker Python SDK. Below, utility methods are \n",
    "defined for creating Processing Job Inputs for the following sources: S3, Athena, and Redshift."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "from sagemaker.dataset_definition.inputs import AthenaDatasetDefinition, DatasetDefinition, RedshiftDatasetDefinition\n",
    "\n",
    "\n",
    "def create_flow_notebook_processing_input(base_dir, flow_s3_uri):\n",
    "    return ProcessingInput(\n",
    "        source=flow_s3_uri,\n",
    "        destination=f\"{base_dir}/flow\",\n",
    "        input_name=\"flow\",\n",
    "        s3_data_type=\"S3Prefix\",\n",
    "        s3_input_mode=\"File\",\n",
    "        s3_data_distribution_type=\"FullyReplicated\",\n",
    "    )\n",
    "\n",
    "\n",
    "def create_s3_processing_input(s3_dataset_definition, name, base_dir):\n",
    "    return ProcessingInput(\n",
    "        source=s3_dataset_definition['s3ExecutionContext']['s3Uri'],\n",
    "        destination=f\"{base_dir}/{name}\",\n",
    "        input_name=name,\n",
    "        s3_data_type=\"S3Prefix\",\n",
    "        s3_input_mode=\"File\",\n",
    "        s3_data_distribution_type=\"FullyReplicated\",\n",
    "    )\n",
    "\n",
    "\n",
    "def create_athena_processing_input(athena_dataset_defintion, name, base_dir):\n",
    "    return ProcessingInput(\n",
    "        input_name=name,\n",
    "        dataset_definition=DatasetDefinition(\n",
    "            local_path=f\"{base_dir}/{name}\",\n",
    "            athena_dataset_definition=AthenaDatasetDefinition(\n",
    "                catalog=athena_dataset_defintion[\"catalogName\"],\n",
    "                database=athena_dataset_defintion[\"databaseName\"],\n",
    "                query_string=athena_dataset_defintion[\"queryString\"],\n",
    "                output_s3_uri=athena_dataset_defintion[\"s3OutputLocation\"] + f\"{name}/\",\n",
    "                output_format=athena_dataset_defintion[\"outputFormat\"].upper()\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "def create_redshift_processing_input(redshift_dataset_defintion, name, base_dir):\n",
    "    return ProcessingInput(\n",
    "        input_name=name,\n",
    "        dataset_definition=DatasetDefinition(\n",
    "            local_path=f\"{base_dir}/{name}\",\n",
    "            redshift_dataset_definition=RedshiftDatasetDefinition(\n",
    "                cluster_id=redshift_dataset_defintion[\"clusterIdentifier\"],\n",
    "                database=redshift_dataset_defintion[\"database\"],\n",
    "                db_user=redshift_dataset_defintion[\"dbUser\"],\n",
    "                query_string=redshift_dataset_defintion[\"queryString\"],\n",
    "                cluster_role_arn=redshift_dataset_defintion[\"unloadIamRole\"],\n",
    "                output_s3_uri=redshift_dataset_defintion[\"s3OutputLocation\"] + f\"{name}/\",\n",
    "                output_format=redshift_dataset_defintion[\"outputFormat\"].upper()\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "def create_processing_inputs(processing_dir, flow, flow_uri):\n",
    "    \"\"\"Helper function for creating processing inputs\n",
    "    :param flow: loaded data wrangler flow notebook\n",
    "    :param flow_uri: S3 URI of the data wrangler flow notebook\n",
    "    \"\"\"\n",
    "    processing_inputs = []\n",
    "    flow_processing_input = create_flow_notebook_processing_input(processing_dir, flow_uri)\n",
    "    processing_inputs.append(flow_processing_input)\n",
    "\n",
    "    for node in flow[\"nodes\"]:\n",
    "        if \"dataset_definition\" in node[\"parameters\"]:\n",
    "            data_def = node[\"parameters\"][\"dataset_definition\"]\n",
    "            name = data_def[\"name\"]\n",
    "            source_type = data_def[\"datasetSourceType\"]\n",
    "\n",
    "            if source_type == \"S3\":\n",
    "                processing_inputs.append(create_s3_processing_input(data_def, name, processing_dir))\n",
    "            elif source_type == \"Athena\":\n",
    "                processing_inputs.append(create_athena_processing_input(data_def, name, processing_dir))\n",
    "            elif source_type == \"Redshift\":\n",
    "                processing_inputs.append(create_redshift_processing_input(data_def, name, processing_dir))\n",
    "            else:\n",
    "                raise ValueError(f\"{source_type} is not supported for Data Wrangler Processing.\")\n",
    "\n",
    "    return processing_inputs\n",
    "\n",
    "\n",
    "def create_processing_output(output_name, output_path, processing_dir):\n",
    "    return ProcessingOutput(\n",
    "        output_name=output_name,\n",
    "        source=os.path.join(processing_dir, \"output\"),\n",
    "        destination=output_path,\n",
    "        s3_upload_mode=\"EndOfJob\"\n",
    "    )\n",
    "\n",
    "\n",
    "def create_container_arguments(output_name, output_content_type):\n",
    "    output_config = {\n",
    "        output_name: {\n",
    "            \"content_type\": output_content_type\n",
    "        }\n",
    "    }\n",
    "    return [f\"--output-config '{json.dumps(output_config)}'\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start ProcessingJob\n",
    "\n",
    "Now, the Processing Job is submitted using the Processor from the Sagemaker SDK.\n",
    "Logs are turned off, but can be turned on for debugging purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Job Name:  data-wrangler-flow-processing-02-05-22-00-ca04653e\n",
      "Inputs:  [{'InputName': 'flow', 'AppManaged': False, 'S3Input': {'S3Uri': 's3://sagemaker-ap-northeast-2-029498593638/data_wrangler_flows/flow-02-05-22-00-ca04653e.flow', 'LocalPath': '/opt/ml/processing/flow', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'FullyReplicated', 'S3CompressionType': 'None'}}, {'InputName': 'mbr_no_map', 'AppManaged': False, 'DatasetDefinition': {'LocalPath': '/opt/ml/processing/mbr_no_map', 'AthenaDatasetDefinition': {'Catalog': 'AwsDataCatalog', 'Database': 'default', 'QueryString': \"select adid, mbr_no from event as evt join order_slim as ord on evt.order_id = ord.ord_no where event_name like 'abx:purchase'\", 'OutputS3Uri': 's3://sagemaker-ap-northeast-2-029498593638/athena/mbr_no_map/', 'OutputFormat': 'PARQUET'}}}, {'InputName': 'event_all', 'AppManaged': False, 'DatasetDefinition': {'LocalPath': '/opt/ml/processing/event_all', 'AthenaDatasetDefinition': {'Catalog': 'AwsDataCatalog', 'Database': 'default', 'QueryString': \"select * from event where product_id <> ''\", 'OutputS3Uri': 's3://sagemaker-ap-northeast-2-029498593638/athena/event_all/', 'OutputFormat': 'PARQUET'}}}, {'InputName': 'event_test', 'AppManaged': False, 'DatasetDefinition': {'LocalPath': '/opt/ml/processing/event_test', 'AthenaDatasetDefinition': {'Catalog': 'AwsDataCatalog', 'Database': 'default', 'QueryString': \"select * from event where product_id <> '' limit 10000\", 'OutputS3Uri': 's3://sagemaker-ap-northeast-2-029498593638/athena/event_test/', 'OutputFormat': 'PARQUET'}}}]\n",
      "Outputs:  [{'OutputName': '410bb997-de0d-4339-8039-53e1d78e46f7.default', 'AppManaged': False, 'S3Output': {'S3Uri': 's3://sagemaker-ap-northeast-2-029498593638/export-flow-02-05-22-00-ca04653e/output', 'LocalPath': '/opt/ml/processing/output', 'S3UploadMode': 'EndOfJob'}}]\n",
      "........................\u001b[34m{\"level\":\"INFO\",\"ts\":\"2021-03-02T05:25:55.603Z\",\"msg\":\"[sagemaker logs] [Input: mbr_no_map] Athena dataset definition specified. Starting athena query execution.\"}\u001b[0m\n",
      "\u001b[34m{\"level\":\"INFO\",\"ts\":\"2021-03-02T05:25:55.603Z\",\"msg\":\"[sagemaker logs] [Input: mbr_no_map] Creating database 'sagemaker_processing' in catalog 'awsdatacatalog' if doesn't exist already.\"}\u001b[0m\n",
      "\u001b[34m{\"level\":\"INFO\",\"ts\":\"2021-03-02T05:25:55.751Z\",\"msg\":\"[sagemaker logs] [Input: mbr_no_map] Database 'sagemaker_processing' already exists in catalog 'awsdatacatalog'.\"}\u001b[0m\n",
      "\u001b[34m{\"level\":\"INFO\",\"ts\":\"2021-03-02T05:25:55.751Z\",\"msg\":\"[sagemaker logs] [Input: mbr_no_map] Starting Athena query execution.\"}\u001b[0m\n",
      "\u001b[34m{\"level\":\"INFO\",\"ts\":\"2021-03-02T05:25:55.885Z\",\"msg\":\"[sagemaker logs] [Input: mbr_no_map] Waiting for query execution to complete, QueryExecutionId: b3360fef-1f91-4e08-a11a-de06924a2dc4\"}\u001b[0m\n",
      "\u001b[34m{\"level\":\"INFO\",\"ts\":\"2021-03-02T05:25:55.909Z\",\"msg\":\"[sagemaker logs] [Input: mbr_no_map] Waiting for query execution to complete, QueryExecutionId: b3360fef-1f91-4e08-a11a-de06924a2dc4\"}\u001b[0m\n",
      "\u001b[34m{\"level\":\"INFO\",\"ts\":\"2021-03-02T05:25:56.384Z\",\"msg\":\"[sagemaker logs] [Input: event_all] Athena dataset definition specified. Starting athena query execution.\"}\u001b[0m\n",
      "\u001b[34m{\"level\":\"INFO\",\"ts\":\"2021-03-02T05:25:56.384Z\",\"msg\":\"[sagemaker logs] [Input: event_all] Creating database 'sagemaker_processing' in catalog 'awsdatacatalog' if doesn't exist already.\"}\u001b[0m\n",
      "\u001b[34m{\"level\":\"INFO\",\"ts\":\"2021-03-02T05:25:56.500Z\",\"msg\":\"[sagemaker logs] [Input: event_all] Database 'sagemaker_processing' already exists in catalog 'awsdatacatalog'.\"}\u001b[0m\n",
      "\u001b[34m{\"level\":\"INFO\",\"ts\":\"2021-03-02T05:25:56.500Z\",\"msg\":\"[sagemaker logs] [Input: event_all] Starting Athena query execution.\"}\u001b[0m\n",
      "\u001b[34m{\"level\":\"INFO\",\"ts\":\"2021-03-02T05:25:56.574Z\",\"msg\":\"[sagemaker logs] [Input: event_all] Waiting for query execution to complete, QueryExecutionId: 7fe95274-44ef-47a8-a34c-dfc5ffeeae1c\"}\u001b[0m\n",
      "\u001b[34m{\"level\":\"INFO\",\"ts\":\"2021-03-02T05:25:56.593Z\",\"msg\":\"[sagemaker logs] [Input: event_all] Waiting for query execution to complete, QueryExecutionId: 7fe95274-44ef-47a8-a34c-dfc5ffeeae1c\"}\u001b[0m\n",
      "\u001b[34m{\"level\":\"INFO\",\"ts\":\"2021-03-02T05:25:57.105Z\",\"msg\":\"[sagemaker logs] [Input: event_test] Athena dataset definition specified. Starting athena query execution.\"}\u001b[0m\n",
      "\u001b[34m{\"level\":\"INFO\",\"ts\":\"2021-03-02T05:25:57.105Z\",\"msg\":\"[sagemaker logs] [Input: event_test] Creating database 'sagemaker_processing' in catalog 'awsdatacatalog' if doesn't exist already.\"}\u001b[0m\n",
      "\u001b[34m{\"level\":\"INFO\",\"ts\":\"2021-03-02T05:25:57.232Z\",\"msg\":\"[sagemaker logs] [Input: event_test] Database 'sagemaker_processing' already exists in catalog 'awsdatacatalog'.\"}\u001b[0m\n",
      "\u001b[34m{\"level\":\"INFO\",\"ts\":\"2021-03-02T05:25:57.232Z\",\"msg\":\"[sagemaker logs] [Input: event_test] Starting Athena query execution.\"}\u001b[0m\n",
      "\u001b[34m{\"level\":\"INFO\",\"ts\":\"2021-03-02T05:25:57.384Z\",\"msg\":\"[sagemaker logs] [Input: event_test] Waiting for query execution to complete, QueryExecutionId: 748bbc04-f4fd-45c5-a41c-610c16233a7a\"}\u001b[0m\n",
      "\u001b[34m{\"level\":\"INFO\",\"ts\":\"2021-03-02T05:25:57.405Z\",\"msg\":\"[sagemaker logs] [Input: event_test] Waiting for query execution to complete, QueryExecutionId: 748bbc04-f4fd-45c5-a41c-610c16233a7a\"}\u001b[0m\n",
      "\u001b[34m{\"level\":\"INFO\",\"ts\":\"2021-03-02T05:26:00.920Z\",\"msg\":\"[sagemaker logs] [Input: mbr_no_map] Query execution completed successfully.\"}\u001b[0m\n",
      "\u001b[34m{\"level\":\"INFO\",\"ts\":\"2021-03-02T05:26:00.920Z\",\"msg\":\"[sagemaker logs] [Input: mbr_no_map] Preparing to clean up resources.\"}\u001b[0m\n",
      "\u001b[34m{\"level\":\"INFO\",\"ts\":\"2021-03-02T05:26:01.199Z\",\"msg\":\"[sagemaker logs] [Input: mbr_no_map] Resource clean up complete. Exiting.\"}\u001b[0m\n",
      "\u001b[34m{\"level\":\"INFO\",\"ts\":\"2021-03-02T05:26:01.608Z\",\"msg\":\"[sagemaker logs] [Input: event_all] Waiting for query execution to complete, QueryExecutionId: 7fe95274-44ef-47a8-a34c-dfc5ffeeae1c\"}\u001b[0m\n",
      "\u001b[34m{\"level\":\"INFO\",\"ts\":\"2021-03-02T05:26:02.431Z\",\"msg\":\"[sagemaker logs] [Input: event_test] Query execution completed successfully.\"}\u001b[0m\n",
      "\u001b[34m{\"level\":\"INFO\",\"ts\":\"2021-03-02T05:26:02.431Z\",\"msg\":\"[sagemaker logs] [Input: event_test] Preparing to clean up resources.\"}\u001b[0m\n",
      "\u001b[34m{\"level\":\"INFO\",\"ts\":\"2021-03-02T05:26:02.659Z\",\"msg\":\"[sagemaker logs] [Input: event_test] Resource clean up complete. Exiting.\"}\u001b[0m\n",
      "\u001b[34m{\"level\":\"INFO\",\"ts\":\"2021-03-02T05:26:06.619Z\",\"msg\":\"[sagemaker logs] [Input: event_all] Query execution completed successfully.\"}\u001b[0m\n",
      "\u001b[34m{\"level\":\"INFO\",\"ts\":\"2021-03-02T05:26:06.620Z\",\"msg\":\"[sagemaker logs] [Input: event_all] Preparing to clean up resources.\"}\u001b[0m\n",
      "\u001b[34m{\"level\":\"INFO\",\"ts\":\"2021-03-02T05:26:06.891Z\",\"msg\":\"[sagemaker logs] [Input: event_all] Resource clean up complete. Exiting.\"}\u001b[0m\n",
      "\u001b[34mSPARK_HOME=/usr/lib/spark\u001b[0m\n",
      "\u001b[34mHOSTNAME=ip-10-0-189-145.ap-northeast-2.compute.internal\u001b[0m\n",
      "\u001b[34mNB_USER=sagemaker-user\u001b[0m\n",
      "\u001b[34mSHELL=/bin/bash\u001b[0m\n",
      "\u001b[34mHADOOP_HOME=/usr/lib/hadoop\u001b[0m\n",
      "\u001b[34mYARN_RESOURCEMANAGER_USER=root\u001b[0m\n",
      "\u001b[34mAWS_CONTAINER_CREDENTIALS_RELATIVE_URI=/v2/credentials/KVu08Y5fP6zLIaFxzdvznPTPGU1UFW6qcIPVgYl8O6E\u001b[0m\n",
      "\u001b[34mPYTHONUNBUFFERED=1\u001b[0m\n",
      "\u001b[34mLC_ALL=en_US.UTF-8\u001b[0m\n",
      "\u001b[34mPYTHONIOENCODING=UTF-8\u001b[0m\n",
      "\u001b[34mPYSPARK_PYTHON=/usr/bin/python3\u001b[0m\n",
      "\u001b[34mSPARK_NO_DAEMONIZE=TRUE\u001b[0m\n",
      "\u001b[34mYARN_NODEMANAGER_USER=root\u001b[0m\n",
      "\u001b[34mHDFS_NAMENODE_USER=root\u001b[0m\n",
      "\u001b[34mPYTHONHASHSEED=0\u001b[0m\n",
      "\u001b[34mPATH=/usr/bin:/opt/program:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\u001b[0m\n",
      "\u001b[34mHDFS_SECONDARYNAMENODE_USER=root\u001b[0m\n",
      "\u001b[34mPWD=/home/sagemaker-user\u001b[0m\n",
      "\u001b[34mLANG=en_US.UTF-8\u001b[0m\n",
      "\u001b[34mHADOOP_CONF_DIR=/usr/lib/hadoop/etc/hadoop\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG_FILE=/opt/ml/config/resourceconfig.json\u001b[0m\n",
      "\u001b[34mAWS_REGION=ap-northeast-2\u001b[0m\n",
      "\u001b[34mAWS_METADATA_SERVICE_TIMEOUT=3\u001b[0m\n",
      "\u001b[34mPYTHONDONTWRITEBYTECODE=1\u001b[0m\n",
      "\u001b[34mHDFS_DATANODE_USER=root\u001b[0m\n",
      "\u001b[34mSHLVL=1\u001b[0m\n",
      "\u001b[34mAWS_METADATA_SERVICE_NUM_ATTEMPTS=3\u001b[0m\n",
      "\u001b[34mHOME=/home/sagemaker-user\u001b[0m\n",
      "\u001b[34mLANGUAGE=en_US.UTF-8\u001b[0m\n",
      "\u001b[34mSM_PROCESSING_CONFIG_FILE=/opt/ml/config/processingjobconfig.json\u001b[0m\n",
      "\u001b[34mPIP_DISABLE_PIP_VERSION_CHECK=1\u001b[0m\n",
      "\u001b[34mNB_GID=100\u001b[0m\n",
      "\u001b[34mNB_UID=1000\u001b[0m\n",
      "\u001b[34m_=/usr/bin/printenv\u001b[0m\n",
      "\u001b[35mSPARK_HOME=/usr/lib/spark\u001b[0m\n",
      "\u001b[35mHOSTNAME=ip-10-0-189-145.ap-northeast-2.compute.internal\u001b[0m\n",
      "\u001b[35mNB_USER=sagemaker-user\u001b[0m\n",
      "\u001b[35mSHELL=/bin/bash\u001b[0m\n",
      "\u001b[35mHADOOP_HOME=/usr/lib/hadoop\u001b[0m\n",
      "\u001b[35mYARN_RESOURCEMANAGER_USER=root\u001b[0m\n",
      "\u001b[35mAWS_CONTAINER_CREDENTIALS_RELATIVE_URI=/v2/credentials/KVu08Y5fP6zLIaFxzdvznPTPGU1UFW6qcIPVgYl8O6E\u001b[0m\n",
      "\u001b[35mPYTHONUNBUFFERED=1\u001b[0m\n",
      "\u001b[35mLC_ALL=en_US.UTF-8\u001b[0m\n",
      "\u001b[35mPYTHONIOENCODING=UTF-8\u001b[0m\n",
      "\u001b[35mPYSPARK_PYTHON=/usr/bin/python3\u001b[0m\n",
      "\u001b[35mSPARK_NO_DAEMONIZE=TRUE\u001b[0m\n",
      "\u001b[35mYARN_NODEMANAGER_USER=root\u001b[0m\n",
      "\u001b[35mHDFS_NAMENODE_USER=root\u001b[0m\n",
      "\u001b[35mPYTHONHASHSEED=0\u001b[0m\n",
      "\u001b[35mPATH=/usr/bin:/opt/program:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\u001b[0m\n",
      "\u001b[35mHDFS_SECONDARYNAMENODE_USER=root\u001b[0m\n",
      "\u001b[35mPWD=/home/sagemaker-user\u001b[0m\n",
      "\u001b[35mLANG=en_US.UTF-8\u001b[0m\n",
      "\u001b[35mHADOOP_CONF_DIR=/usr/lib/hadoop/etc/hadoop\u001b[0m\n",
      "\u001b[35mSM_RESOURCE_CONFIG_FILE=/opt/ml/config/resourceconfig.json\u001b[0m\n",
      "\u001b[35mAWS_REGION=ap-northeast-2\u001b[0m\n",
      "\u001b[35mAWS_METADATA_SERVICE_TIMEOUT=3\u001b[0m\n",
      "\u001b[35mPYTHONDONTWRITEBYTECODE=1\u001b[0m\n",
      "\u001b[35mHDFS_DATANODE_USER=root\u001b[0m\n",
      "\u001b[35mSHLVL=1\u001b[0m\n",
      "\u001b[35mAWS_METADATA_SERVICE_NUM_ATTEMPTS=3\u001b[0m\n",
      "\u001b[35mHOME=/home/sagemaker-user\u001b[0m\n",
      "\u001b[35mLANGUAGE=en_US.UTF-8\u001b[0m\n",
      "\u001b[35mSM_PROCESSING_CONFIG_FILE=/opt/ml/config/processingjobconfig.json\u001b[0m\n",
      "\u001b[35mPIP_DISABLE_PIP_VERSION_CHECK=1\u001b[0m\n",
      "\u001b[35mNB_GID=100\u001b[0m\n",
      "\u001b[35mNB_UID=1000\u001b[0m\n",
      "\u001b[35m_=/usr/bin/printenv\u001b[0m\n",
      "\u001b[34mStarting DataPrep Container in Processing Job Mode\u001b[0m\n",
      "\u001b[34m03-02 05:26 smspark.cli  INFO     Parsing arguments. argv: ['/usr/local/bin/smspark-submit', '/entrypoint/processing_entrypoint.py', '--output-config', '{\"410bb997-de0d-4339-8039-53e1d78e46f7.default\": {\"content_type\": \"CSV\"}}']\u001b[0m\n",
      "\u001b[34m03-02 05:26 smspark.cli  INFO     Raw spark options before processing: {'class_': None, 'jars': None, 'py_files': None, 'files': None, 'verbose': False}\u001b[0m\n",
      "\u001b[34m03-02 05:26 smspark.cli  INFO     App and app arguments: ['/entrypoint/processing_entrypoint.py', '--output-config', '{\"410bb997-de0d-4339-8039-53e1d78e46f7.default\": {\"content_type\": \"CSV\"}}']\u001b[0m\n",
      "\u001b[34m03-02 05:26 smspark.cli  INFO     Rendered spark options: {'class_': None, 'jars': None, 'py_files': None, 'files': None, 'verbose': False}\u001b[0m\n",
      "\u001b[34m03-02 05:26 smspark.cli  INFO     Initializing processing job.\u001b[0m\n",
      "\u001b[34m03-02 05:26 smspark-submit INFO     {'current_host': 'algo-1', 'hosts': ['algo-1']}\u001b[0m\n",
      "\u001b[34m03-02 05:26 smspark-submit INFO     {'ProcessingJobArn': 'arn:aws:sagemaker:ap-northeast-2:029498593638:processing-job/data-wrangler-flow-processing-02-05-22-00-ca04653e', 'ProcessingJobName': 'data-wrangler-flow-processing-02-05-22-00-ca04653e', 'AppSpecification': {'ImageUri': '131546521161.dkr.ecr.ap-northeast-2.amazonaws.com/sagemaker-data-wrangler-container:1.2.1', 'ContainerEntrypoint': None, 'ContainerArguments': ['--output-config \\'{\"410bb997-de0d-4339-8039-53e1d78e46f7.default\": {\"content_type\": \"CSV\"}}\\'']}, 'ProcessingInputs': [{'InputName': 'flow', 'AppManaged': False, 'S3Input': {'LocalPath': '/opt/ml/processing/flow', 'S3Uri': 's3://sagemaker-ap-northeast-2-029498593638/data_wrangler_flows/flow-02-05-22-00-ca04653e.flow', 'S3DataDistributionType': 'FullyReplicated', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3CompressionType': 'None', 'S3DownloadMode': 'StartOfJob'}, 'DatasetDefinition': None}, {'InputName': 'mbr_no_map', 'AppManaged': False, 'S3Input': None, 'DatasetDefinition': {'AthenaDatasetDefinition': {'Catalog': 'AwsDataCatalog', 'Database': 'default', 'QueryString': \"select adid, mbr_no from event as evt join order_slim as ord on evt.order_id = ord.ord_no where event_name like 'abx:purchase'\", 'OutputS3Uri': 's3://sagemaker-ap-northeast-2-029498593638/athena/mbr_no_map/', 'KmsKeyId': None, 'OutputFormat': 'PARQUET', 'OutputCompression': None, 'WorkGroup': None}, 'RedshiftDatasetDefinition': None, 'LocalPath': '/opt/ml/processing/mbr_no_map', 'DataDistributionType': 'ShardedByS3Key', 'InputMode': 'File'}}, {'InputName': 'event_all', 'AppManaged': False, 'S3Input': None, 'DatasetDefinition': {'AthenaDatasetDefinition': {'Catalog': 'AwsDataCatalog', 'Database': 'default', 'QueryString': \"select * from event where product_id <> ''\", 'OutputS3Uri': 's3://sagemaker-ap-northeast-2-029498593638/athena/event_all/', 'KmsKeyId': None, 'OutputFormat': 'PARQUET', 'OutputCompression': None, 'WorkGroup': None}, 'RedshiftDatasetDefinition': None, 'LocalPath': '/opt/ml/processing/event_all', 'DataDistributionType': 'ShardedByS3Key', 'InputMode': 'File'}}, {'InputName': 'event_test', 'AppManaged': False, 'S3Input': None, 'DatasetDefinition': {'AthenaDatasetDefinition': {'Catalog': 'AwsDataCatalog', 'Database': 'default', 'QueryString': \"select * from event where product_id <> '' limit 10000\", 'OutputS3Uri': 's3://sagemaker-ap-northeast-2-029498593638/athena/event_test/', 'KmsKeyId': None, 'OutputFormat': 'PARQUET', 'OutputCompression': None, 'WorkGroup': None}, 'RedshiftDatasetDefinition': None, 'LocalPath': '/opt/ml/processing/event_test', 'DataDistributionType': 'ShardedByS3Key', 'InputMode': 'File'}}], 'ProcessingOutputConfig': {'Outputs': [{'OutputName': '410bb997-de0d-4339-8039-53e1d78e46f7.default', 'AppManaged': False, 'S3Output': {'LocalPath': '/opt/ml/processing/output', 'S3Uri': 's3://sagemaker-ap-northeast-2-029498593638/export-flow-02-05-22-00-ca04653e/output', 'S3UploadMode': 'EndOfJob'}, 'FeatureStoreOutput': None}], 'KmsKeyId': None}, 'ProcessingResources': {'ClusterConfig': {'InstanceCount': 1, 'InstanceType': 'ml.m5.4xlarge', 'VolumeSizeInGB': 30, 'VolumeKmsKeyId': None}}, 'RoleArn': 'arn:aws:iam::029498593638:role/service-role/AmazonSageMaker-ExecutionRole-20190920T225690', 'StoppingCondition': {'MaxRuntimeInSeconds': 86400}}\u001b[0m\n",
      "\u001b[34m03-02 05:26 smspark.cli  INFO     running spark submit command: spark-submit --master yarn --deploy-mode client /entrypoint/processing_entrypoint.py --output-config '{\"410bb997-de0d-4339-8039-53e1d78e46f7.default\": {\"content_type\": \"CSV\"}}'\u001b[0m\n",
      "\u001b[34m03-02 05:26 smspark-submit INFO     waiting for hosts\u001b[0m\n",
      "\u001b[34m03-02 05:26 smspark-submit INFO     starting status server\u001b[0m\n",
      "\u001b[34m03-02 05:26 smspark-submit INFO     Status server listening on algo-1:5555\u001b[0m\n",
      "\u001b[34m03-02 05:26 smspark-submit INFO     bootstrapping cluster\u001b[0m\n",
      "\u001b[34m03-02 05:26 smspark-submit INFO     transitioning from status INITIALIZING to BOOTSTRAPPING\u001b[0m\n",
      "\u001b[34m03-02 05:26 smspark-submit INFO     copying aws jars\u001b[0m\n",
      "\u001b[34mServing on http://algo-1:5555\u001b[0m\n",
      "\u001b[34m03-02 05:26 smspark-submit INFO     Found hadoop jar hadoop-aws-3.2.1-amzn-1.jar\u001b[0m\n",
      "\u001b[34m03-02 05:26 smspark-submit INFO     Optional jar jets3t-0.9.0.jar in /usr/lib/hadoop/lib does not exist\u001b[0m\n",
      "\u001b[34m03-02 05:26 smspark-submit INFO     copying cluster config\u001b[0m\n",
      "\u001b[34m03-02 05:26 smspark-submit INFO     copying /opt/hadoop-config/hdfs-site.xml to /usr/lib/hadoop/etc/hadoop/hdfs-site.xml\u001b[0m\n",
      "\u001b[34m03-02 05:26 smspark-submit INFO     copying /opt/hadoop-config/core-site.xml to /usr/lib/hadoop/etc/hadoop/core-site.xml\u001b[0m\n",
      "\u001b[34m03-02 05:26 smspark-submit INFO     copying /opt/hadoop-config/yarn-site.xml to /usr/lib/hadoop/etc/hadoop/yarn-site.xml\u001b[0m\n",
      "\u001b[34m03-02 05:26 smspark-submit INFO     copying /opt/hadoop-config/spark-defaults.conf to /usr/lib/spark/conf/spark-defaults.conf\u001b[0m\n",
      "\u001b[34m03-02 05:26 smspark-submit INFO     copying /opt/hadoop-config/spark-env.sh to /usr/lib/spark/conf/spark-env.sh\u001b[0m\n",
      "\u001b[34m03-02 05:26 root         INFO     Detected instance type: m5.4xlarge with total memory: 65536M and total cores: 16\u001b[0m\n",
      "\u001b[34m03-02 05:26 root         INFO     Writing default config to /usr/lib/hadoop/etc/hadoop/yarn-site.xml\u001b[0m\n",
      "\u001b[34m03-02 05:26 root         INFO     Configuration at /usr/lib/hadoop/etc/hadoop/yarn-site.xml is: \u001b[0m\n",
      "\u001b[34m<?xml version=\"1.0\"?>\u001b[0m\n",
      "\u001b[35mStarting DataPrep Container in Processing Job Mode\u001b[0m\n",
      "\u001b[35m03-02 05:26 smspark.cli  INFO     Parsing arguments. argv: ['/usr/local/bin/smspark-submit', '/entrypoint/processing_entrypoint.py', '--output-config', '{\"410bb997-de0d-4339-8039-53e1d78e46f7.default\": {\"content_type\": \"CSV\"}}']\u001b[0m\n",
      "\u001b[35m03-02 05:26 smspark.cli  INFO     Raw spark options before processing: {'class_': None, 'jars': None, 'py_files': None, 'files': None, 'verbose': False}\u001b[0m\n",
      "\u001b[35m03-02 05:26 smspark.cli  INFO     App and app arguments: ['/entrypoint/processing_entrypoint.py', '--output-config', '{\"410bb997-de0d-4339-8039-53e1d78e46f7.default\": {\"content_type\": \"CSV\"}}']\u001b[0m\n",
      "\u001b[35m03-02 05:26 smspark.cli  INFO     Rendered spark options: {'class_': None, 'jars': None, 'py_files': None, 'files': None, 'verbose': False}\u001b[0m\n",
      "\u001b[35m03-02 05:26 smspark.cli  INFO     Initializing processing job.\u001b[0m\n",
      "\u001b[35m03-02 05:26 smspark-submit INFO     {'current_host': 'algo-1', 'hosts': ['algo-1']}\u001b[0m\n",
      "\u001b[35m03-02 05:26 smspark-submit INFO     {'ProcessingJobArn': 'arn:aws:sagemaker:ap-northeast-2:029498593638:processing-job/data-wrangler-flow-processing-02-05-22-00-ca04653e', 'ProcessingJobName': 'data-wrangler-flow-processing-02-05-22-00-ca04653e', 'AppSpecification': {'ImageUri': '131546521161.dkr.ecr.ap-northeast-2.amazonaws.com/sagemaker-data-wrangler-container:1.2.1', 'ContainerEntrypoint': None, 'ContainerArguments': ['--output-config \\'{\"410bb997-de0d-4339-8039-53e1d78e46f7.default\": {\"content_type\": \"CSV\"}}\\'']}, 'ProcessingInputs': [{'InputName': 'flow', 'AppManaged': False, 'S3Input': {'LocalPath': '/opt/ml/processing/flow', 'S3Uri': 's3://sagemaker-ap-northeast-2-029498593638/data_wrangler_flows/flow-02-05-22-00-ca04653e.flow', 'S3DataDistributionType': 'FullyReplicated', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3CompressionType': 'None', 'S3DownloadMode': 'StartOfJob'}, 'DatasetDefinition': None}, {'InputName': 'mbr_no_map', 'AppManaged': False, 'S3Input': None, 'DatasetDefinition': {'AthenaDatasetDefinition': {'Catalog': 'AwsDataCatalog', 'Database': 'default', 'QueryString': \"select adid, mbr_no from event as evt join order_slim as ord on evt.order_id = ord.ord_no where event_name like 'abx:purchase'\", 'OutputS3Uri': 's3://sagemaker-ap-northeast-2-029498593638/athena/mbr_no_map/', 'KmsKeyId': None, 'OutputFormat': 'PARQUET', 'OutputCompression': None, 'WorkGroup': None}, 'RedshiftDatasetDefinition': None, 'LocalPath': '/opt/ml/processing/mbr_no_map', 'DataDistributionType': 'ShardedByS3Key', 'InputMode': 'File'}}, {'InputName': 'event_all', 'AppManaged': False, 'S3Input': None, 'DatasetDefinition': {'AthenaDatasetDefinition': {'Catalog': 'AwsDataCatalog', 'Database': 'default', 'QueryString': \"select * from event where product_id <> ''\", 'OutputS3Uri': 's3://sagemaker-ap-northeast-2-029498593638/athena/event_all/', 'KmsKeyId': None, 'OutputFormat': 'PARQUET', 'OutputCompression': None, 'WorkGroup': None}, 'RedshiftDatasetDefinition': None, 'LocalPath': '/opt/ml/processing/event_all', 'DataDistributionType': 'ShardedByS3Key', 'InputMode': 'File'}}, {'InputName': 'event_test', 'AppManaged': False, 'S3Input': None, 'DatasetDefinition': {'AthenaDatasetDefinition': {'Catalog': 'AwsDataCatalog', 'Database': 'default', 'QueryString': \"select * from event where product_id <> '' limit 10000\", 'OutputS3Uri': 's3://sagemaker-ap-northeast-2-029498593638/athena/event_test/', 'KmsKeyId': None, 'OutputFormat': 'PARQUET', 'OutputCompression': None, 'WorkGroup': None}, 'RedshiftDatasetDefinition': None, 'LocalPath': '/opt/ml/processing/event_test', 'DataDistributionType': 'ShardedByS3Key', 'InputMode': 'File'}}], 'ProcessingOutputConfig': {'Outputs': [{'OutputName': '410bb997-de0d-4339-8039-53e1d78e46f7.default', 'AppManaged': False, 'S3Output': {'LocalPath': '/opt/ml/processing/output', 'S3Uri': 's3://sagemaker-ap-northeast-2-029498593638/export-flow-02-05-22-00-ca04653e/output', 'S3UploadMode': 'EndOfJob'}, 'FeatureStoreOutput': None}], 'KmsKeyId': None}, 'ProcessingResources': {'ClusterConfig': {'InstanceCount': 1, 'InstanceType': 'ml.m5.4xlarge', 'VolumeSizeInGB': 30, 'VolumeKmsKeyId': None}}, 'RoleArn': 'arn:aws:iam::029498593638:role/service-role/AmazonSageMaker-ExecutionRole-20190920T225690', 'StoppingCondition': {'MaxRuntimeInSeconds': 86400}}\u001b[0m\n",
      "\u001b[35m03-02 05:26 smspark.cli  INFO     running spark submit command: spark-submit --master yarn --deploy-mode client /entrypoint/processing_entrypoint.py --output-config '{\"410bb997-de0d-4339-8039-53e1d78e46f7.default\": {\"content_type\": \"CSV\"}}'\u001b[0m\n",
      "\u001b[35m03-02 05:26 smspark-submit INFO     waiting for hosts\u001b[0m\n",
      "\u001b[35m03-02 05:26 smspark-submit INFO     starting status server\u001b[0m\n",
      "\u001b[35m03-02 05:26 smspark-submit INFO     Status server listening on algo-1:5555\u001b[0m\n",
      "\u001b[35m03-02 05:26 smspark-submit INFO     bootstrapping cluster\u001b[0m\n",
      "\u001b[35m03-02 05:26 smspark-submit INFO     transitioning from status INITIALIZING to BOOTSTRAPPING\u001b[0m\n",
      "\u001b[35m03-02 05:26 smspark-submit INFO     copying aws jars\u001b[0m\n",
      "\u001b[35mServing on http://algo-1:5555\u001b[0m\n",
      "\u001b[35m03-02 05:26 smspark-submit INFO     Found hadoop jar hadoop-aws-3.2.1-amzn-1.jar\u001b[0m\n",
      "\u001b[35m03-02 05:26 smspark-submit INFO     Optional jar jets3t-0.9.0.jar in /usr/lib/hadoop/lib does not exist\u001b[0m\n",
      "\u001b[35m03-02 05:26 smspark-submit INFO     copying cluster config\u001b[0m\n",
      "\u001b[35m03-02 05:26 smspark-submit INFO     copying /opt/hadoop-config/hdfs-site.xml to /usr/lib/hadoop/etc/hadoop/hdfs-site.xml\u001b[0m\n",
      "\u001b[35m03-02 05:26 smspark-submit INFO     copying /opt/hadoop-config/core-site.xml to /usr/lib/hadoop/etc/hadoop/core-site.xml\u001b[0m\n",
      "\u001b[35m03-02 05:26 smspark-submit INFO     copying /opt/hadoop-config/yarn-site.xml to /usr/lib/hadoop/etc/hadoop/yarn-site.xml\u001b[0m\n",
      "\u001b[35m03-02 05:26 smspark-submit INFO     copying /opt/hadoop-config/spark-defaults.conf to /usr/lib/spark/conf/spark-defaults.conf\u001b[0m\n",
      "\u001b[35m03-02 05:26 smspark-submit INFO     copying /opt/hadoop-config/spark-env.sh to /usr/lib/spark/conf/spark-env.sh\u001b[0m\n",
      "\u001b[35m03-02 05:26 root         INFO     Detected instance type: m5.4xlarge with total memory: 65536M and total cores: 16\u001b[0m\n",
      "\u001b[35m03-02 05:26 root         INFO     Writing default config to /usr/lib/hadoop/etc/hadoop/yarn-site.xml\u001b[0m\n",
      "\u001b[35m03-02 05:26 root         INFO     Configuration at /usr/lib/hadoop/etc/hadoop/yarn-site.xml is: \u001b[0m\n",
      "\u001b[35m<?xml version=\"1.0\"?>\u001b[0m\n",
      "\u001b[34m<!-- Site specific YARN configuration properties -->\n",
      " <configuration>\n",
      "     <property>\n",
      "         <name>yarn.resourcemanager.hostname</name>\n",
      "         <value>10.0.189.145</value>\n",
      "         <description>The hostname of the RM.</description>\n",
      "     </property>\n",
      "     <property>\n",
      "         <name>yarn.nodemanager.hostname</name>\n",
      "         <value>algo-1</value>\n",
      "         <description>The hostname of the NM.</description>\n",
      "     </property>\n",
      "     <property>\n",
      "         <name>yarn.nodemanager.webapp.address</name>\n",
      "         <value>algo-1:8042</value>\n",
      "     </property>\n",
      "     <property>\n",
      "         <name>yarn.nodemanager.vmem-pmem-ratio</name>\n",
      "         <value>5</value>\n",
      "         <description>Ratio between virtual memory to physical memory.</description>\n",
      "     </property>\n",
      "     <property>\n",
      "         <name>yarn.resourcemanager.am.max-attempts</name>\n",
      "         <value>1</value>\n",
      "         <description>The maximum number of application attempts.</description>\n",
      "     </property>\n",
      "     <property>\n",
      "         <name>yarn.nodemanager.env-whitelist</name>\n",
      "         <value>JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,YARN_HOME,AWS_CONTAINER_CREDENTIALS_RELATIVE_URI</value>\n",
      "         <description>Environment variable whitelist</description>\n",
      "     </property>\n",
      "\n",
      " \n",
      "  <property>\n",
      "    <name>yarn.scheduler.minimum-allocation-mb</name>\n",
      "    <value>1</value>\n",
      "  </property>\n",
      "  <property>\n",
      "    <name>yarn.scheduler.maximum-allocation-mb</name>\n",
      "    <value>63569</value>\n",
      "  </property>\n",
      "  <property>\n",
      "    <name>yarn.scheduler.minimum-allocation-vcores</name>\n",
      "    <value>1</value>\n",
      "  </property>\n",
      "  <property>\n",
      "    <name>yarn.scheduler.maximum-allocation-vcores</name>\n",
      "    <value>16</value>\n",
      "  </property>\n",
      "  <property>\n",
      "    <name>yarn.nodemanager.resource.memory-mb</name>\n",
      "    <value>63569</value>\n",
      "  </property>\n",
      "  <property>\n",
      "    <name>yarn.nodemanager.resource.cpu-vcores</name>\n",
      "    <value>16</value>\n",
      "  </property>\u001b[0m\n",
      "\u001b[34m</configuration>\n",
      "\u001b[0m\n",
      "\u001b[34m03-02 05:26 root         INFO     Writing default config to /usr/lib/spark/conf/spark-defaults.conf\u001b[0m\n",
      "\u001b[34m03-02 05:26 root         INFO     Configuration at /usr/lib/spark/conf/spark-defaults.conf is: \u001b[0m\n",
      "\u001b[34mspark.driver.extraClassPath      /usr/lib/hadoop-lzo/lib/*:/usr/lib/hadoop/hadoop-aws.jar:/usr/share/aws/aws-java-sdk/*:/usr/share/aws/emr/emrfs/conf:/usr/share/aws/emr/emrfs/lib/*:/usr/share/aws/emr/emrfs/auxlib/*:/usr/share/aws/emr/goodies/lib/emr-spark-goodies.jar:/usr/share/aws/emr/security/conf:/usr/share/aws/emr/security/lib/*:/usr/share/aws/hmclient/lib/aws-glue-datacatalog-spark-client.jar:/usr/share/java/Hive-JSON-Serde/hive-openx-serde.jar:/usr/share/aws/sagemaker-spark-sdk/lib/sagemaker-spark-sdk.jar:/usr/share/aws/emr/s3select/lib/emr-s3-select-spark-connector.jar\u001b[0m\n",
      "\u001b[34mspark.driver.extraLibraryPath    /usr/lib/hadoop/lib/native:/usr/lib/hadoop-lzo/lib/native\u001b[0m\n",
      "\u001b[34mspark.executor.extraClassPath    /usr/lib/hadoop-lzo/lib/*:/usr/lib/hadoop/hadoop-aws.jar:/usr/share/aws/aws-java-sdk/*:/usr/share/aws/emr/emrfs/conf:/usr/share/aws/emr/emrfs/lib/*:/usr/share/aws/emr/emrfs/auxlib/*:/usr/share/aws/emr/goodies/lib/emr-spark-goodies.jar:/usr/share/aws/emr/security/conf:/usr/share/aws/emr/security/lib/*:/usr/share/aws/hmclient/lib/aws-glue-datacatalog-spark-client.jar:/usr/share/java/Hive-JSON-Serde/hive-openx-serde.jar:/usr/share/aws/sagemaker-spark-sdk/lib/sagemaker-spark-sdk.jar:/usr/share/aws/emr/s3select/lib/emr-s3-select-spark-connector.jar\u001b[0m\n",
      "\u001b[34mspark.executor.extraLibraryPath  /usr/lib/hadoop/lib/native:/usr/lib/hadoop-lzo/lib/native\u001b[0m\n",
      "\u001b[34mspark.driver.host=10.0.189.145\u001b[0m\n",
      "\u001b[34mspark.hadoop.mapreduce.fileoutputcommitter.algorithm.version=2\u001b[0m\n",
      "\u001b[34mspark.driver.memory 2048m\u001b[0m\n",
      "\u001b[34mspark.driver.memoryOverhead 204m\u001b[0m\n",
      "\u001b[34mspark.driver.defaultJavaOptions -XX:OnOutOfMemoryError='kill -9 %p' -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:MaxHeapFreeRatio=70 -XX:+CMSClassUnloadingEnabled\u001b[0m\n",
      "\u001b[34mspark.executor.memory 55742m\u001b[0m\n",
      "\u001b[34mspark.executor.memoryOverhead 5574m\u001b[0m\n",
      "\u001b[34mspark.executor.cores 16\u001b[0m\n",
      "\u001b[34mspark.executor.defaultJavaOptions -verbose:gc -XX:OnOutOfMemoryError='kill -9 %p' -XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:+UseParallelGC -XX:InitiatingHeapOccupancyPercent=70 -XX:ConcGCThreads=4 -XX:ParallelGCThreads=12 \u001b[0m\n",
      "\u001b[34mspark.executor.instances 1\u001b[0m\n",
      "\u001b[34mspark.default.parallelism 32\n",
      "\u001b[0m\n",
      "\u001b[34m03-02 05:26 root         INFO     Finished Yarn configuration files setup.\u001b[0m\n",
      "\u001b[34m03-02 05:26 root         INFO     No file at /opt/ml/processing/input/conf/configuration.json exists, skipping user configuration\u001b[0m\n",
      "\u001b[34mWARNING: /usr/lib/hadoop/logs does not exist. Creating.\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:15,625 INFO namenode.NameNode: STARTUP_MSG: \u001b[0m\n",
      "\u001b[34m/************************************************************\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG: Starting NameNode\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   host = algo-1/10.0.189.145\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   args = [-format, -force]\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   version = 3.2.1-amzn-1\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   classpath = /usr/lib/hadoop/etc/hadoop:/usr/lib/hadoop/lib/jackson-xc-1.9.13.jar:/usr/lib/hadoop/lib/kerby-pkix-1.0.1.jar:/usr/lib/hadoop/lib/httpclient-4.5.9.jar:/usr/lib/hadoop/lib/zookeeper-3.4.14.jar:/usr/lib/hadoop/lib/jersey-core-1.19.jar:/usr/lib/hadoop/lib/re2j-1.1.jar:/usr/lib/hadoop/lib/jsp-api-2.1.jar:/usr/lib/hadoop/lib/kerby-config-1.0.1.jar:/usr/lib/hadoop/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop/lib/guava-11.0.2.jar:/usr/lib/hadoop/lib/jackson-core-2.10.0.jar:/usr/lib/hadoop/lib/commons-text-1.4.jar:/usr/lib/hadoop/lib/kerb-common-1.0.1.jar:/usr/lib/hadoop/lib/htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop/lib/kerb-admin-1.0.1.jar:/usr/lib/hadoop/lib/jetty-io-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/javax.servlet-api-3.1.0.jar:/usr/lib/hadoop/lib/commons-collections-3.2.2.jar:/usr/lib/hadoop/lib/kerb-util-1.0.1.jar:/usr/lib/hadoop/lib/paranamer-2.3.jar:/usr/lib/hadoop/lib/httpcore-4.4.11.jar:/usr/lib/hadoop/lib/jackson-databind-2.10.0.jar:/usr/lib/hadoop/lib/jsr305-3.0.0.jar:/usr/lib/hadoop/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop/lib/dnsjava-2.1.7.jar:/usr/lib/hadoop/lib/curator-recipes-2.13.0.jar:/usr/lib/hadoop/lib/stax2-api-3.1.4.jar:/usr/lib/hadoop/lib/slf4j-api-1.7.25.jar:/usr/lib/hadoop/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop/lib/kerb-client-1.0.1.jar:/usr/lib/hadoop/lib/commons-configuration2-2.1.1.jar:/usr/lib/hadoop/lib/asm-5.0.4.jar:/usr/lib/hadoop/lib/curator-client-2.13.0.jar:/usr/lib/hadoop/lib/commons-cli-1.2.jar:/usr/lib/hadoop/lib/gson-2.2.4.jar:/usr/lib/hadoop/lib/nimbus-jose-jwt-4.41.1.jar:/usr/lib/hadoop/lib/jetty-servlet-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/kerb-crypto-1.0.1.jar:/usr/lib/hadoop/lib/slf4j-log4j12-1.7.25.jar:/usr/lib/hadoop/lib/jetty-webapp-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/jetty-http-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/curator-framework-2.13.0.jar:/usr/lib/hadoop/lib/jersey-servlet-1.19.jar:/usr/lib/hadoop/lib/kerby-asn1-1.0.1.jar:/usr/lib/hadoop/lib/snappy-java-1.1.7.3.jar:/usr/lib/hadoop/lib/jsr311-api-1.1.1.jar:/usr/lib/hadoop/lib/commons-codec-1.11.jar:/usr/lib/hadoop/lib/jetty-xml-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/jetty-util-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop/lib/json-smart-2.3.jar:/usr/lib/hadoop/lib/commons-compress-1.18.jar:/usr/lib/hadoop/lib/jetty-server-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/accessors-smart-1.2.jar:/usr/lib/hadoop/lib/jersey-json-1.19.jar:/usr/lib/hadoop/lib/metrics-core-3.2.4.jar:/usr/lib/hadoop/lib/kerb-server-1.0.1.jar:/usr/lib/hadoop/lib/commons-io-2.5.jar:/usr/lib/hadoop/lib/jetty-security-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/jul-to-slf4j-1.7.25.jar:/usr/lib/hadoop/lib/netty-3.10.5.Final.jar:/usr/lib/hadoop/lib/token-provider-1.0.1.jar:/usr/lib/hadoop/lib/avro-1.7.7.jar:/usr/lib/hadoop/lib/log4j-1.2.17.jar:/usr/lib/hadoop/lib/commons-math3-3.1.1.jar:/usr/lib/hadoop/lib/jaxb-api-2.2.11.jar:/usr/lib/hadoop/lib/jackson-annotations-2.10.0.jar:/usr/lib/hadoop/lib/jettison-1.1.jar:/usr/lib/hadoop/lib/kerb-simplekdc-1.0.1.jar:/usr/lib/hadoop/lib/jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop/lib/kerby-util-1.0.1.jar:/usr/lib/hadoop/lib/commons-lang3-3.7.jar:/usr/lib/hadoop/lib/commons-beanutils-1.9.3.jar:/usr/lib/hadoop/lib/kerby-xdr-1.0.1.jar:/usr/lib/hadoop/lib/jersey-server-1.19.jar:/usr/lib/hadoop/lib/commons-net-3.6.jar:/usr/lib/hadoop/lib/kerb-identity-1.0.1.jar:/usr/lib/hadoop/lib/jsch-0.1.54.jar:/usr/lib/hadoop/lib/woodstox-core-5.0.3.jar:/usr/lib/hadoop/lib/jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop/lib/jcip-annotations-1.0-1.jar:/usr/lib/hadoop/lib/kerb-core-1.0.1.jar:/usr/lib/hadoop/.//hadoop-fs2img-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-extras.jar:/usr/lib/hadoop/.//hadoop-common-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop/.//hadoop-archives.jar:/usr/lib/hadoop/.//hadoop-sls-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-resourceestimator.jar:/usr/lib/hadoop/.//hadoop-sls.jar:/usr/lib/hadoop/.//hadoop-aws-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-aliyun-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-nfs-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-azure.jar:/usr/lib/hadoop/.//hadoop-nfs.jar:/usr/lib/hadoop/.//hadoop-auth.jar:/usr/lib/hadoop/.//hadoop-azure-datalake.jar:/usr/lib/hadoop/.//hadoop-annotations-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-openstack.jar:/usr/lib/hadoop/.//hadoop-streaming-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-distcp.jar:/usr/lib/hadoop/.//hadoop-archive-logs-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-azure-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-gridmix.jar:/usr/lib/hadoop/.//hadoop-auth-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-datajoin-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-openstack-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-rumen-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-datajoin.jar:/usr/lib/hadoop/.//hadoop-streaming.jar:/usr/lib/hadoop/.//hadoop-azure-datalake-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-extras-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-archive-logs.jar:/usr/lib/hadoop/.//hadoop-gridmix-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-resourceestimator-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-annotations.jar:/usr/lib/hadoop/.//hadoop-common-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-rumen.jar:/usr/lib/hadoop/.//hadoop-kms.jar:/usr/lib/hadoop/.//hadoop-kafka.jar:/usr/lib/hadoop/.//hadoop-archives-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-aliyun.jar:/usr/lib/hadoop/.//hadoop-aws.jar:/usr/lib/hadoop/.//hadoop-fs2img.jar:/usr/lib/hadoop/.//hadoop-kafka-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-kms-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-common.jar:/usr/lib/hadoop/.//hadoop-distcp-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/./:/usr/lib/hadoop-hdfs/lib/jackson-xc-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/kerby-pkix-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/httpclient-4.5.9.jar:/usr/lib/hadoop-hdfs/lib/zookeeper-3.4.14.jar:/usr/lib/hadoop-hdfs/lib/jersey-core-1.19.jar:/usr/lib/hadoop-hdfs/lib/re2j-1.1.jar:/usr/lib/hadoop-hdfs/lib/kerby-config-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop-hdfs/lib/guava-11.0.2.jar:/usr/lib/hadoop-hdfs/lib/jackson-core-2.10.0.jar:/usr/lib/hadoop-hdfs/lib/commons-text-1.4.jar:/usr/lib/hadoop-hdfs/lib/kerb-common-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop-hdfs/lib/kerb-admin-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jetty-io-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/javax.servlet-api-3.1.0.jar:/usr/lib/hadoop-hdfs/lib/commons-collections-3.2.2.jar:/usr/lib/hadoop-hdfs/lib/kerb-util-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/paranamer-2.3.jar:/usr/lib/hadoop-hdfs/lib/httpcore-4.4.11.jar:/usr/lib/hadoop-hdfs/lib/jackson-databind-2.10.0.jar:/usr/lib/hadoop-hdfs/lib/jsr305-3.0.0.jar:/usr/lib/hadoop-hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/dnsjava-2.1.7.jar:/usr/lib/hadoop-hdfs/lib/curator-recipes-2.13.0.jar:/usr/lib/hadoop-hdfs/lib/stax2-api-3.1.4.jar:/usr/lib/hadoop-hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/kerb-client-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/commons-configuration2-2.1.1.jar:/usr/lib/hadoop-hdfs/lib/asm-5.0.4.jar:/usr/lib/hadoop-hdfs/lib/curator-client-2.13.0.jar:/usr/lib/hadoop-hdfs/lib/commons-cli-1.2.jar:/usr/lib/hadoop-hdfs/lib/gson-2.2.4.jar:/usr/lib/hadoop-hdfs/lib/nimbus-jose-jwt-4.41.1.jar:/usr/lib/hadoop-hdfs/lib/jetty-servlet-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/kerb-crypto-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jetty-webapp-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/jetty-http-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/curator-framework-2.13.0.jar:/usr/lib/hadoop-hdfs/lib/jersey-servlet-1.19.jar:/usr/lib/hadoop-hdfs/lib/kerby-asn1-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/snappy-java-1.1.7.3.jar:/usr/lib/hadoop-hdfs/lib/jsr311-api-1.1.1.jar:/usr/lib/hadoop-hdfs/lib/json-simple-1.1.1.jar:/usr/lib/hadoop-hdfs/lib/commons-codec-1.11.jar:/usr/lib/hadoop-hdfs/lib/jetty-xml-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/jetty-util-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop-hdfs/lib/json-smart-2.3.jar:/usr/lib/hadoop-hdfs/lib/commons-compress-1.18.jar:/usr/lib/hadoop-hdfs/lib/jetty-server-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/netty-all-4.0.52.Final.jar:/usr/lib/hadoop-hdfs/lib/accessors-smart-1.2.jar:/usr/lib/hadoop-hdfs/lib/jersey-json-1.19.jar:/usr/lib/hadoop-hdfs/lib/kerb-server-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/commons-io-2.5.jar:/usr/lib/hadoop-hdfs/lib/jetty-security-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/netty-3.10.5.Final.jar:/usr/lib/hadoop-hdfs/lib/token-provider-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/avro-1.7.7.jar:/usr/lib/hadoop-hdfs/lib/jetty-util-ajax-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/log4j-1.2.17.jar:/usr/lib/hadoop-hdfs/lib/commons-math3-3.1.1.jar:/usr/lib/hadoop-hdfs/lib/jaxb-api-2.2.11.jar:/usr/lib/hadoop-hdfs/lib/jackson-annotations-2.10.0.jar:/usr/lib/hadoop-hdfs/lib/commons-httpclient-3.1.jar:/usr/lib/hadoop-hdfs/lib/jettison-1.1.jar:/usr/lib/hadoop-hdfs/lib/kerb-simplekdc-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop-hdfs/lib/kerby-util-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/okhttp-2.7.5.jar:/usr/lib/hadoop-hdfs/lib/commons-lang3-3.7.jar:/usr/lib/hadoop-hdfs/lib/commons-beanutils-1.9.3.jar:/usr/lib/hadoop-hdfs/lib/commons-daemon-1.0.13.jar:/usr/lib/hadoop-hdfs/lib/kerby-xdr-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/okio-1.6.0.jar:/usr/lib/hadoop-hdfs/lib/jersey-server-1.19.jar:/usr/lib/hadoop-hdfs/lib/commons-net-3.6.jar:/usr/lib/hadoop-hdfs/lib/kerb-identity-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jsch-0.1.54.jar:/usr/lib/hadoop-hdfs/lib/leveldbjni-all-1.8.jar:/usr/lib/hadoop-hdfs/lib/woodstox-core-5.0.3.jar:/usr/lib/hadoop-hdfs/lib/jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/jcip-annotations-1.0-1.jar:/usr/lib/hadoop-hdfs/lib/kerb-core-1.0.1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-httpfs-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-nfs-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-httpfs.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-nfs.jar:/usr/lib/hadoop-mapreduce/.//azure-data-lake-store-sdk-2.2.9.jar:/usr/lib/hadoop-mapreduce/.//kafka-clients-2.4.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-fs2img-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-extras.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-core.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archives.jar:/usr/lib/hadoop-mapreduce/.//hadoop-sls-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-resourceestimator.jar:/usr/lib/hadoop-mapreduce/.//azure-storage-7.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-sls.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aws-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aliyun-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-common-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-datalake.jar:/usr/lib/hadoop-mapreduce/.//hadoop-openstack.jar:/usr/lib/hadoop-mapreduce/.//hadoop-streaming-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-nativetask.jar:/usr/lib/hadoop-mapreduce/.//hadoop-distcp.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archive-logs-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//ojalgo-43.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-plugins.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-gridmix.jar:/usr/lib/hadoop-mapreduce/.//hadoop-datajoin-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//aws-java-sdk-bundle-1.11.828.jar:/usr/lib/hadoop-mapreduce/.//hadoop-openstack-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-common.jar:/usr/lib/hadoop-mapreduce/.//zstd-jni-1.4.3-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-rumen-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-datajoin.jar:/usr/lib/hadoop-mapreduce/.//hadoop-streaming.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-datalake-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-core-3.4.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-app.jar:/usr/lib/hadoop-mapreduce/.//hadoop-extras-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//aliyun-sdk-oss-3.4.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archive-logs.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-ecs-4.2.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-examples.jar:/usr/lib/hadoop-mapreduce/.//hadoop-gridmix-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-resourceestimator-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-uploader.jar:/usr/lib/hadoop-mapreduce/.//wildfly-openssl-1.0.7.Final.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-uploader-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-app-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-rumen.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-core-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-examples-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient.jar:/usr/lib/hadoop-mapreduce/.//hadoop-kafka.jar:/usr/lib/hadoop-mapreduce/.//azure-keyvault-core-1.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archives-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-shuffle-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aliyun.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aws.jar:/usr/lib/hadoop-mapreduce/.//hadoop-fs2img.jar:/usr/lib/hadoop-mapreduce/.//hadoop-kafka-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//lz4-java-1.6.0.jar:/usr/lib/hadoop-mapreduce/.//jdom-1.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-shuffle.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-nativetask-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-sts-3.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-plugins-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-ram-3.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-distcp-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop-yarn/lib/jackson-module-jaxb-annotations-2.10.0.jar:/usr/lib/hadoop-yarn/lib/jackson-jaxrs-json-provider-2.10.0.jar:/usr/lib/hadoop-yarn/lib/jackson-jaxrs-base-2.10.0.jar:/usr/lib/hadoop-yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/usr/lib/hadoop-yarn/lib/javax.inject-1.jar:/usr/lib/hadoop-yarn/lib/bcprov-jdk15on-1.60.jar:/usr/lib/hadoop-yarn/lib/bcpkix-jdk15on-1.60.jar:/usr/lib/hadoop-yarn/lib/swagger-annotations-1.5.4.jar:/usr/lib/hadoop-yarn/lib/jersey-client-1.19.jar:/usr/lib/hadoop-yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/usr/lib/hadoop-yarn/lib/jakarta.xml.bind-api-2.3.2.jar:/usr/lib/hadoop-yarn/lib/guice-servlet-4.0.jar:/usr/lib/hadoop-yarn/lib/objenesis-1.0.jar:/usr/lib/hadoop-yarn/lib/jersey-guice-1.19.jar:/usr/lib/hadoop-yarn/lib/jakarta.activation-api-1.2.1.jar:/usr/lib/hadoop-yarn/lib/ehcache-3.3.1.jar:/usr/lib/hadoop-yarn/lib/metrics-core-3.2.4.jar:/usr/lib/hadoop-yarn/lib/aopalliance-1.0.jar:/usr/lib/hadoop-yarn/lib/guice-4.0.jar:/usr/lib/hadoop-yarn/lib/java-util-1.9.0.jar:/usr/lib/hadoop-yarn/lib/fst-2.50.jar:/usr/lib/hadoop-yarn/lib/snakeyaml-1.16.jar:/usr/lib/hadoop-yarn/lib/HikariCP-java7-2.4.12.jar:/usr/lib/hadoop-yarn/lib/json-io-2.5.1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-unmanaged-am-launcher-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-tests.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-client.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-resourcemanager-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-\u001b[0m\n",
      "\u001b[34myarn-server-applicationhistoryservice-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-unmanaged-am-launcher.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-registry.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-api.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-timeline-pluginstorage-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-nodemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-common.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-core-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-tests-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-registry-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-common-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-timeline-pluginstorage.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-client-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-submarine.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-sharedcachemanager-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-common-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-web-proxy-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-sharedcachemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-applicationhistoryservice.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-api-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-router-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-common.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-core.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-resourcemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-submarine-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-api-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-distributedshell.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-router.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-api.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-nodemanager-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-distributedshell-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-web-proxy.jar\u001b[0m\n",
      "\u001b[35m<!-- Site specific YARN configuration properties -->\n",
      " <configuration>\n",
      "     <property>\n",
      "         <name>yarn.resourcemanager.hostname</name>\n",
      "         <value>10.0.189.145</value>\n",
      "         <description>The hostname of the RM.</description>\n",
      "     </property>\n",
      "     <property>\n",
      "         <name>yarn.nodemanager.hostname</name>\n",
      "         <value>algo-1</value>\n",
      "         <description>The hostname of the NM.</description>\n",
      "     </property>\n",
      "     <property>\n",
      "         <name>yarn.nodemanager.webapp.address</name>\n",
      "         <value>algo-1:8042</value>\n",
      "     </property>\n",
      "     <property>\n",
      "         <name>yarn.nodemanager.vmem-pmem-ratio</name>\n",
      "         <value>5</value>\n",
      "         <description>Ratio between virtual memory to physical memory.</description>\n",
      "     </property>\n",
      "     <property>\n",
      "         <name>yarn.resourcemanager.am.max-attempts</name>\n",
      "         <value>1</value>\n",
      "         <description>The maximum number of application attempts.</description>\n",
      "     </property>\n",
      "     <property>\n",
      "         <name>yarn.nodemanager.env-whitelist</name>\n",
      "         <value>JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,YARN_HOME,AWS_CONTAINER_CREDENTIALS_RELATIVE_URI</value>\n",
      "         <description>Environment variable whitelist</description>\n",
      "     </property>\n",
      "\n",
      " \n",
      "  <property>\n",
      "    <name>yarn.scheduler.minimum-allocation-mb</name>\n",
      "    <value>1</value>\n",
      "  </property>\n",
      "  <property>\n",
      "    <name>yarn.scheduler.maximum-allocation-mb</name>\n",
      "    <value>63569</value>\n",
      "  </property>\n",
      "  <property>\n",
      "    <name>yarn.scheduler.minimum-allocation-vcores</name>\n",
      "    <value>1</value>\n",
      "  </property>\n",
      "  <property>\n",
      "    <name>yarn.scheduler.maximum-allocation-vcores</name>\n",
      "    <value>16</value>\n",
      "  </property>\n",
      "  <property>\n",
      "    <name>yarn.nodemanager.resource.memory-mb</name>\n",
      "    <value>63569</value>\n",
      "  </property>\n",
      "  <property>\n",
      "    <name>yarn.nodemanager.resource.cpu-vcores</name>\n",
      "    <value>16</value>\n",
      "  </property>\u001b[0m\n",
      "\u001b[35m</configuration>\n",
      "\u001b[0m\n",
      "\u001b[35m03-02 05:26 root         INFO     Writing default config to /usr/lib/spark/conf/spark-defaults.conf\u001b[0m\n",
      "\u001b[35m03-02 05:26 root         INFO     Configuration at /usr/lib/spark/conf/spark-defaults.conf is: \u001b[0m\n",
      "\u001b[35mspark.driver.extraClassPath      /usr/lib/hadoop-lzo/lib/*:/usr/lib/hadoop/hadoop-aws.jar:/usr/share/aws/aws-java-sdk/*:/usr/share/aws/emr/emrfs/conf:/usr/share/aws/emr/emrfs/lib/*:/usr/share/aws/emr/emrfs/auxlib/*:/usr/share/aws/emr/goodies/lib/emr-spark-goodies.jar:/usr/share/aws/emr/security/conf:/usr/share/aws/emr/security/lib/*:/usr/share/aws/hmclient/lib/aws-glue-datacatalog-spark-client.jar:/usr/share/java/Hive-JSON-Serde/hive-openx-serde.jar:/usr/share/aws/sagemaker-spark-sdk/lib/sagemaker-spark-sdk.jar:/usr/share/aws/emr/s3select/lib/emr-s3-select-spark-connector.jar\u001b[0m\n",
      "\u001b[35mspark.driver.extraLibraryPath    /usr/lib/hadoop/lib/native:/usr/lib/hadoop-lzo/lib/native\u001b[0m\n",
      "\u001b[35mspark.executor.extraClassPath    /usr/lib/hadoop-lzo/lib/*:/usr/lib/hadoop/hadoop-aws.jar:/usr/share/aws/aws-java-sdk/*:/usr/share/aws/emr/emrfs/conf:/usr/share/aws/emr/emrfs/lib/*:/usr/share/aws/emr/emrfs/auxlib/*:/usr/share/aws/emr/goodies/lib/emr-spark-goodies.jar:/usr/share/aws/emr/security/conf:/usr/share/aws/emr/security/lib/*:/usr/share/aws/hmclient/lib/aws-glue-datacatalog-spark-client.jar:/usr/share/java/Hive-JSON-Serde/hive-openx-serde.jar:/usr/share/aws/sagemaker-spark-sdk/lib/sagemaker-spark-sdk.jar:/usr/share/aws/emr/s3select/lib/emr-s3-select-spark-connector.jar\u001b[0m\n",
      "\u001b[35mspark.executor.extraLibraryPath  /usr/lib/hadoop/lib/native:/usr/lib/hadoop-lzo/lib/native\u001b[0m\n",
      "\u001b[35mspark.driver.host=10.0.189.145\u001b[0m\n",
      "\u001b[35mspark.hadoop.mapreduce.fileoutputcommitter.algorithm.version=2\u001b[0m\n",
      "\u001b[35mspark.driver.memory 2048m\u001b[0m\n",
      "\u001b[35mspark.driver.memoryOverhead 204m\u001b[0m\n",
      "\u001b[35mspark.driver.defaultJavaOptions -XX:OnOutOfMemoryError='kill -9 %p' -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:MaxHeapFreeRatio=70 -XX:+CMSClassUnloadingEnabled\u001b[0m\n",
      "\u001b[35mspark.executor.memory 55742m\u001b[0m\n",
      "\u001b[35mspark.executor.memoryOverhead 5574m\u001b[0m\n",
      "\u001b[35mspark.executor.cores 16\u001b[0m\n",
      "\u001b[35mspark.executor.defaultJavaOptions -verbose:gc -XX:OnOutOfMemoryError='kill -9 %p' -XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:+UseParallelGC -XX:InitiatingHeapOccupancyPercent=70 -XX:ConcGCThreads=4 -XX:ParallelGCThreads=12 \u001b[0m\n",
      "\u001b[35mspark.executor.instances 1\u001b[0m\n",
      "\u001b[35mspark.default.parallelism 32\n",
      "\u001b[0m\n",
      "\u001b[35m03-02 05:26 root         INFO     Finished Yarn configuration files setup.\u001b[0m\n",
      "\u001b[35m03-02 05:26 root         INFO     No file at /opt/ml/processing/input/conf/configuration.json exists, skipping user configuration\u001b[0m\n",
      "\u001b[35mWARNING: /usr/lib/hadoop/logs does not exist. Creating.\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:15,625 INFO namenode.NameNode: STARTUP_MSG: \u001b[0m\n",
      "\u001b[35m/************************************************************\u001b[0m\n",
      "\u001b[35mSTARTUP_MSG: Starting NameNode\u001b[0m\n",
      "\u001b[35mSTARTUP_MSG:   host = algo-1/10.0.189.145\u001b[0m\n",
      "\u001b[35mSTARTUP_MSG:   args = [-format, -force]\u001b[0m\n",
      "\u001b[35mSTARTUP_MSG:   version = 3.2.1-amzn-1\u001b[0m\n",
      "\u001b[35mSTARTUP_MSG:   classpath = /usr/lib/hadoop/etc/hadoop:/usr/lib/hadoop/lib/jackson-xc-1.9.13.jar:/usr/lib/hadoop/lib/kerby-pkix-1.0.1.jar:/usr/lib/hadoop/lib/httpclient-4.5.9.jar:/usr/lib/hadoop/lib/zookeeper-3.4.14.jar:/usr/lib/hadoop/lib/jersey-core-1.19.jar:/usr/lib/hadoop/lib/re2j-1.1.jar:/usr/lib/hadoop/lib/jsp-api-2.1.jar:/usr/lib/hadoop/lib/kerby-config-1.0.1.jar:/usr/lib/hadoop/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop/lib/guava-11.0.2.jar:/usr/lib/hadoop/lib/jackson-core-2.10.0.jar:/usr/lib/hadoop/lib/commons-text-1.4.jar:/usr/lib/hadoop/lib/kerb-common-1.0.1.jar:/usr/lib/hadoop/lib/htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop/lib/kerb-admin-1.0.1.jar:/usr/lib/hadoop/lib/jetty-io-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/javax.servlet-api-3.1.0.jar:/usr/lib/hadoop/lib/commons-collections-3.2.2.jar:/usr/lib/hadoop/lib/kerb-util-1.0.1.jar:/usr/lib/hadoop/lib/paranamer-2.3.jar:/usr/lib/hadoop/lib/httpcore-4.4.11.jar:/usr/lib/hadoop/lib/jackson-databind-2.10.0.jar:/usr/lib/hadoop/lib/jsr305-3.0.0.jar:/usr/lib/hadoop/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop/lib/dnsjava-2.1.7.jar:/usr/lib/hadoop/lib/curator-recipes-2.13.0.jar:/usr/lib/hadoop/lib/stax2-api-3.1.4.jar:/usr/lib/hadoop/lib/slf4j-api-1.7.25.jar:/usr/lib/hadoop/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop/lib/kerb-client-1.0.1.jar:/usr/lib/hadoop/lib/commons-configuration2-2.1.1.jar:/usr/lib/hadoop/lib/asm-5.0.4.jar:/usr/lib/hadoop/lib/curator-client-2.13.0.jar:/usr/lib/hadoop/lib/commons-cli-1.2.jar:/usr/lib/hadoop/lib/gson-2.2.4.jar:/usr/lib/hadoop/lib/nimbus-jose-jwt-4.41.1.jar:/usr/lib/hadoop/lib/jetty-servlet-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/kerb-crypto-1.0.1.jar:/usr/lib/hadoop/lib/slf4j-log4j12-1.7.25.jar:/usr/lib/hadoop/lib/jetty-webapp-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/jetty-http-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/curator-framework-2.13.0.jar:/usr/lib/hadoop/lib/jersey-servlet-1.19.jar:/usr/lib/hadoop/lib/kerby-asn1-1.0.1.jar:/usr/lib/hadoop/lib/snappy-java-1.1.7.3.jar:/usr/lib/hadoop/lib/jsr311-api-1.1.1.jar:/usr/lib/hadoop/lib/commons-codec-1.11.jar:/usr/lib/hadoop/lib/jetty-xml-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/jetty-util-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop/lib/json-smart-2.3.jar:/usr/lib/hadoop/lib/commons-compress-1.18.jar:/usr/lib/hadoop/lib/jetty-server-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/accessors-smart-1.2.jar:/usr/lib/hadoop/lib/jersey-json-1.19.jar:/usr/lib/hadoop/lib/metrics-core-3.2.4.jar:/usr/lib/hadoop/lib/kerb-server-1.0.1.jar:/usr/lib/hadoop/lib/commons-io-2.5.jar:/usr/lib/hadoop/lib/jetty-security-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/jul-to-slf4j-1.7.25.jar:/usr/lib/hadoop/lib/netty-3.10.5.Final.jar:/usr/lib/hadoop/lib/token-provider-1.0.1.jar:/usr/lib/hadoop/lib/avro-1.7.7.jar:/usr/lib/hadoop/lib/log4j-1.2.17.jar:/usr/lib/hadoop/lib/commons-math3-3.1.1.jar:/usr/lib/hadoop/lib/jaxb-api-2.2.11.jar:/usr/lib/hadoop/lib/jackson-annotations-2.10.0.jar:/usr/lib/hadoop/lib/jettison-1.1.jar:/usr/lib/hadoop/lib/kerb-simplekdc-1.0.1.jar:/usr/lib/hadoop/lib/jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop/lib/kerby-util-1.0.1.jar:/usr/lib/hadoop/lib/commons-lang3-3.7.jar:/usr/lib/hadoop/lib/commons-beanutils-1.9.3.jar:/usr/lib/hadoop/lib/kerby-xdr-1.0.1.jar:/usr/lib/hadoop/lib/jersey-server-1.19.jar:/usr/lib/hadoop/lib/commons-net-3.6.jar:/usr/lib/hadoop/lib/kerb-identity-1.0.1.jar:/usr/lib/hadoop/lib/jsch-0.1.54.jar:/usr/lib/hadoop/lib/woodstox-core-5.0.3.jar:/usr/lib/hadoop/lib/jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop/lib/jcip-annotations-1.0-1.jar:/usr/lib/hadoop/lib/kerb-core-1.0.1.jar:/usr/lib/hadoop/.//hadoop-fs2img-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-extras.jar:/usr/lib/hadoop/.//hadoop-common-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop/.//hadoop-archives.jar:/usr/lib/hadoop/.//hadoop-sls-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-resourceestimator.jar:/usr/lib/hadoop/.//hadoop-sls.jar:/usr/lib/hadoop/.//hadoop-aws-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-aliyun-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-nfs-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-azure.jar:/usr/lib/hadoop/.//hadoop-nfs.jar:/usr/lib/hadoop/.//hadoop-auth.jar:/usr/lib/hadoop/.//hadoop-azure-datalake.jar:/usr/lib/hadoop/.//hadoop-annotations-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-openstack.jar:/usr/lib/hadoop/.//hadoop-streaming-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-distcp.jar:/usr/lib/hadoop/.//hadoop-archive-logs-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-azure-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-gridmix.jar:/usr/lib/hadoop/.//hadoop-auth-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-datajoin-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-openstack-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-rumen-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-datajoin.jar:/usr/lib/hadoop/.//hadoop-streaming.jar:/usr/lib/hadoop/.//hadoop-azure-datalake-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-extras-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-archive-logs.jar:/usr/lib/hadoop/.//hadoop-gridmix-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-resourceestimator-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-annotations.jar:/usr/lib/hadoop/.//hadoop-common-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-rumen.jar:/usr/lib/hadoop/.//hadoop-kms.jar:/usr/lib/hadoop/.//hadoop-kafka.jar:/usr/lib/hadoop/.//hadoop-archives-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-aliyun.jar:/usr/lib/hadoop/.//hadoop-aws.jar:/usr/lib/hadoop/.//hadoop-fs2img.jar:/usr/lib/hadoop/.//hadoop-kafka-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-kms-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-common.jar:/usr/lib/hadoop/.//hadoop-distcp-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/./:/usr/lib/hadoop-hdfs/lib/jackson-xc-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/kerby-pkix-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/httpclient-4.5.9.jar:/usr/lib/hadoop-hdfs/lib/zookeeper-3.4.14.jar:/usr/lib/hadoop-hdfs/lib/jersey-core-1.19.jar:/usr/lib/hadoop-hdfs/lib/re2j-1.1.jar:/usr/lib/hadoop-hdfs/lib/kerby-config-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop-hdfs/lib/guava-11.0.2.jar:/usr/lib/hadoop-hdfs/lib/jackson-core-2.10.0.jar:/usr/lib/hadoop-hdfs/lib/commons-text-1.4.jar:/usr/lib/hadoop-hdfs/lib/kerb-common-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop-hdfs/lib/kerb-admin-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jetty-io-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/javax.servlet-api-3.1.0.jar:/usr/lib/hadoop-hdfs/lib/commons-collections-3.2.2.jar:/usr/lib/hadoop-hdfs/lib/kerb-util-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/paranamer-2.3.jar:/usr/lib/hadoop-hdfs/lib/httpcore-4.4.11.jar:/usr/lib/hadoop-hdfs/lib/jackson-databind-2.10.0.jar:/usr/lib/hadoop-hdfs/lib/jsr305-3.0.0.jar:/usr/lib/hadoop-hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/dnsjava-2.1.7.jar:/usr/lib/hadoop-hdfs/lib/curator-recipes-2.13.0.jar:/usr/lib/hadoop-hdfs/lib/stax2-api-3.1.4.jar:/usr/lib/hadoop-hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/kerb-client-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/commons-configuration2-2.1.1.jar:/usr/lib/hadoop-hdfs/lib/asm-5.0.4.jar:/usr/lib/hadoop-hdfs/lib/curator-client-2.13.0.jar:/usr/lib/hadoop-hdfs/lib/commons-cli-1.2.jar:/usr/lib/hadoop-hdfs/lib/gson-2.2.4.jar:/usr/lib/hadoop-hdfs/lib/nimbus-jose-jwt-4.41.1.jar:/usr/lib/hadoop-hdfs/lib/jetty-servlet-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/kerb-crypto-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jetty-webapp-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/jetty-http-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/curator-framework-2.13.0.jar:/usr/lib/hadoop-hdfs/lib/jersey-servlet-1.19.jar:/usr/lib/hadoop-hdfs/lib/kerby-asn1-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/snappy-java-1.1.7.3.jar:/usr/lib/hadoop-hdfs/lib/jsr311-api-1.1.1.jar:/usr/lib/hadoop-hdfs/lib/json-simple-1.1.1.jar:/usr/lib/hadoop-hdfs/lib/commons-codec-1.11.jar:/usr/lib/hadoop-hdfs/lib/jetty-xml-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/jetty-util-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop-hdfs/lib/json-smart-2.3.jar:/usr/lib/hadoop-hdfs/lib/commons-compress-1.18.jar:/usr/lib/hadoop-hdfs/lib/jetty-server-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/netty-all-4.0.52.Final.jar:/usr/lib/hadoop-hdfs/lib/accessors-smart-1.2.jar:/usr/lib/hadoop-hdfs/lib/jersey-json-1.19.jar:/usr/lib/hadoop-hdfs/lib/kerb-server-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/commons-io-2.5.jar:/usr/lib/hadoop-hdfs/lib/jetty-security-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/netty-3.10.5.Final.jar:/usr/lib/hadoop-hdfs/lib/token-provider-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/avro-1.7.7.jar:/usr/lib/hadoop-hdfs/lib/jetty-util-ajax-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/log4j-1.2.17.jar:/usr/lib/hadoop-hdfs/lib/commons-math3-3.1.1.jar:/usr/lib/hadoop-hdfs/lib/jaxb-api-2.2.11.jar:/usr/lib/hadoop-hdfs/lib/jackson-annotations-2.10.0.jar:/usr/lib/hadoop-hdfs/lib/commons-httpclient-3.1.jar:/usr/lib/hadoop-hdfs/lib/jettison-1.1.jar:/usr/lib/hadoop-hdfs/lib/kerb-simplekdc-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop-hdfs/lib/kerby-util-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/okhttp-2.7.5.jar:/usr/lib/hadoop-hdfs/lib/commons-lang3-3.7.jar:/usr/lib/hadoop-hdfs/lib/commons-beanutils-1.9.3.jar:/usr/lib/hadoop-hdfs/lib/commons-daemon-1.0.13.jar:/usr/lib/hadoop-hdfs/lib/kerby-xdr-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/okio-1.6.0.jar:/usr/lib/hadoop-hdfs/lib/jersey-server-1.19.jar:/usr/lib/hadoop-hdfs/lib/commons-net-3.6.jar:/usr/lib/hadoop-hdfs/lib/kerb-identity-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jsch-0.1.54.jar:/usr/lib/hadoop-hdfs/lib/leveldbjni-all-1.8.jar:/usr/lib/hadoop-hdfs/lib/woodstox-core-5.0.3.jar:/usr/lib/hadoop-hdfs/lib/jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/jcip-annotations-1.0-1.jar:/usr/lib/hadoop-hdfs/lib/kerb-core-1.0.1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-httpfs-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-nfs-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-httpfs.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-nfs.jar:/usr/lib/hadoop-mapreduce/.//azure-data-lake-store-sdk-2.2.9.jar:/usr/lib/hadoop-mapreduce/.//kafka-clients-2.4.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-fs2img-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-extras.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-core.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archives.jar:/usr/lib/hadoop-mapreduce/.//hadoop-sls-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-resourceestimator.jar:/usr/lib/hadoop-mapreduce/.//azure-storage-7.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-sls.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aws-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aliyun-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-common-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-datalake.jar:/usr/lib/hadoop-mapreduce/.//hadoop-openstack.jar:/usr/lib/hadoop-mapreduce/.//hadoop-streaming-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-nativetask.jar:/usr/lib/hadoop-mapreduce/.//hadoop-distcp.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archive-logs-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//ojalgo-43.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-plugins.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-gridmix.jar:/usr/lib/hadoop-mapreduce/.//hadoop-datajoin-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//aws-java-sdk-bundle-1.11.828.jar:/usr/lib/hadoop-mapreduce/.//hadoop-openstack-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-common.jar:/usr/lib/hadoop-mapreduce/.//zstd-jni-1.4.3-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-rumen-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-datajoin.jar:/usr/lib/hadoop-mapreduce/.//hadoop-streaming.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-datalake-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-core-3.4.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-app.jar:/usr/lib/hadoop-mapreduce/.//hadoop-extras-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//aliyun-sdk-oss-3.4.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archive-logs.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-ecs-4.2.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-examples.jar:/usr/lib/hadoop-mapreduce/.//hadoop-gridmix-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-resourceestimator-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-uploader.jar:/usr/lib/hadoop-mapreduce/.//wildfly-openssl-1.0.7.Final.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-uploader-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-app-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-rumen.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-core-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-examples-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient.jar:/usr/lib/hadoop-mapreduce/.//hadoop-kafka.jar:/usr/lib/hadoop-mapreduce/.//azure-keyvault-core-1.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archives-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-shuffle-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aliyun.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aws.jar:/usr/lib/hadoop-mapreduce/.//hadoop-fs2img.jar:/usr/lib/hadoop-mapreduce/.//hadoop-kafka-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//lz4-java-1.6.0.jar:/usr/lib/hadoop-mapreduce/.//jdom-1.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-shuffle.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-nativetask-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-sts-3.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-plugins-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-ram-3.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-distcp-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop-yarn/lib/jackson-module-jaxb-annotations-2.10.0.jar:/usr/lib/hadoop-yarn/lib/jackson-jaxrs-json-provider-2.10.0.jar:/usr/lib/hadoop-yarn/lib/jackson-jaxrs-base-2.10.0.jar:/usr/lib/hadoop-yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/usr/lib/hadoop-yarn/lib/javax.inject-1.jar:/usr/lib/hadoop-yarn/lib/bcprov-jdk15on-1.60.jar:/usr/lib/hadoop-yarn/lib/bcpkix-jdk15on-1.60.jar:/usr/lib/hadoop-yarn/lib/swagger-annotations-1.5.4.jar:/usr/lib/hadoop-yarn/lib/jersey-client-1.19.jar:/usr/lib/hadoop-yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/usr/lib/hadoop-yarn/lib/jakarta.xml.bind-api-2.3.2.jar:/usr/lib/hadoop-yarn/lib/guice-servlet-4.0.jar:/usr/lib/hadoop-yarn/lib/objenesis-1.0.jar:/usr/lib/hadoop-yarn/lib/jersey-guice-1.19.jar:/usr/lib/hadoop-yarn/lib/jakarta.activation-api-1.2.1.jar:/usr/lib/hadoop-yarn/lib/ehcache-3.3.1.jar:/usr/lib/hadoop-yarn/lib/metrics-core-3.2.4.jar:/usr/lib/hadoop-yarn/lib/aopalliance-1.0.jar:/usr/lib/hadoop-yarn/lib/guice-4.0.jar:/usr/lib/hadoop-yarn/lib/java-util-1.9.0.jar:/usr/lib/hadoop-yarn/lib/fst-2.50.jar:/usr/lib/hadoop-yarn/lib/snakeyaml-1.16.jar:/usr/lib/hadoop-yarn/lib/HikariCP-java7-2.4.12.jar:/usr/lib/hadoop-yarn/lib/json-io-2.5.1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-unmanaged-am-launcher-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-tests.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-client.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-resourcemanager-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-\u001b[0m\n",
      "\u001b[35myarn-server-applicationhistoryservice-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-unmanaged-am-launcher.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-registry.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-api.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-timeline-pluginstorage-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-nodemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-common.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-core-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-tests-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-registry-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-common-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-timeline-pluginstorage.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-client-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-submarine.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-sharedcachemanager-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-common-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-web-proxy-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-sharedcachemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-applicationhistoryservice.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-api-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-router-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-common.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-core.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-resourcemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-submarine-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-api-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-distributedshell.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-router.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-api.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-nodemanager-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-distributedshell-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-web-proxy.jar\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   build = git@aws157git.com:/pkg/Aws157BigTop -r 520b97ed429ab8eb7a8d30068b0f34a8036c51a7; compiled by 'ec2-user' on 2020-07-30T08:02Z\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   java = 1.8.0_272\u001b[0m\n",
      "\u001b[34m************************************************************/\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:15,632 INFO namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:15,682 INFO namenode.NameNode: createNameNode [-format, -force]\u001b[0m\n",
      "\u001b[34mFormatting using clusterid: CID-82b1b13a-dc26-466f-a555-0a30873ad261\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:15,992 INFO namenode.FSEditLog: Edit logging is async:true\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:16,003 INFO namenode.FSNamesystem: KeyProvider: null\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:16,004 INFO namenode.FSNamesystem: fsLock is fair: true\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:16,005 INFO namenode.FSNamesystem: Detailed lock hold time metrics enabled: false\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:16,009 INFO namenode.FSNamesystem: fsOwner             = root (auth:SIMPLE)\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:16,009 INFO namenode.FSNamesystem: supergroup          = supergroup\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:16,009 INFO namenode.FSNamesystem: isPermissionEnabled = true\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:16,009 INFO namenode.FSNamesystem: HA Enabled: false\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:16,049 INFO common.Util: dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:16,059 INFO blockmanagement.DatanodeManager: dfs.block.invalidate.limit: configured=1000, counted=60, effected=1000\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:16,059 INFO blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:16,062 INFO blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:16,063 INFO blockmanagement.BlockManager: The block deletion will start around 2021 Mar 02 05:26:16\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:16,064 INFO util.GSet: Computing capacity for map BlocksMap\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:16,064 INFO util.GSet: VM type       = 64-bit\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:16,065 INFO util.GSet: 2.0% max memory 13.5 GB = 277.2 MB\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:16,065 INFO util.GSet: capacity      = 2^25 = 33554432 entries\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:16,144 INFO blockmanagement.BlockManager: Storage policy satisfier is disabled\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:16,144 INFO blockmanagement.BlockManager: dfs.block.access.token.enable = false\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:16,150 INFO Configuration.deprecation: No unit for dfs.namenode.safemode.extension(30000) assuming MILLISECONDS\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:16,150 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.threshold-pct = 0.9990000128746033\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:16,150 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.min.datanodes = 0\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:16,150 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.extension = 30000\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:16,151 INFO blockmanagement.BlockManager: defaultReplication         = 3\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:16,151 INFO blockmanagement.BlockManager: maxReplication             = 512\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:16,151 INFO blockmanagement.BlockManager: minReplication             = 1\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:16,151 INFO blockmanagement.BlockManager: maxReplicationStreams      = 2\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:16,151 INFO blockmanagement.BlockManager: redundancyRecheckInterval  = 3000ms\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:16,151 INFO blockmanagement.BlockManager: encryptDataTransfer        = false\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:16,151 INFO blockmanagement.BlockManager: maxNumBlocksToLog          = 1000\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:16,172 INFO namenode.FSDirectory: GLOBAL serial map: bits=29 maxEntries=536870911\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:16,172 INFO namenode.FSDirectory: USER serial map: bits=24 maxEntries=16777215\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:16,172 INFO namenode.FSDirectory: GROUP serial map: bits=24 maxEntries=16777215\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:16,172 INFO namenode.FSDirectory: XATTR serial map: bits=24 maxEntries=16777215\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:16,183 INFO util.GSet: Computing capacity for map INodeMap\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:16,183 INFO util.GSet: VM type       = 64-bit\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:16,184 INFO util.GSet: 1.0% max memory 13.5 GB = 138.6 MB\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:16,184 INFO util.GSet: capacity      = 2^24 = 16777216 entries\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:16,220 INFO namenode.FSDirectory: ACLs enabled? false\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:16,220 INFO namenode.FSDirectory: POSIX ACL inheritance enabled? true\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:16,220 INFO namenode.FSDirectory: XAttrs enabled? true\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:16,220 INFO namenode.NameNode: Caching file names occurring more than 10 times\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:16,225 INFO snapshot.SnapshotManager: Loaded config captureOpenFiles: false, skipCaptureAccessTimeOnlyChange: false, snapshotDiffAllowSnapRootDescendant: true, maxSnapshotLimit: 65536\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:16,226 INFO snapshot.SnapshotManager: SkipList is disabled\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:16,230 INFO util.GSet: Computing capacity for map cachedBlocks\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:16,230 INFO util.GSet: VM type       = 64-bit\u001b[0m\n",
      "\u001b[35mSTARTUP_MSG:   build = git@aws157git.com:/pkg/Aws157BigTop -r 520b97ed429ab8eb7a8d30068b0f34a8036c51a7; compiled by 'ec2-user' on 2020-07-30T08:02Z\u001b[0m\n",
      "\u001b[35mSTARTUP_MSG:   java = 1.8.0_272\u001b[0m\n",
      "\u001b[35m************************************************************/\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:15,632 INFO namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:15,682 INFO namenode.NameNode: createNameNode [-format, -force]\u001b[0m\n",
      "\u001b[35mFormatting using clusterid: CID-82b1b13a-dc26-466f-a555-0a30873ad261\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:15,992 INFO namenode.FSEditLog: Edit logging is async:true\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:16,003 INFO namenode.FSNamesystem: KeyProvider: null\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:16,004 INFO namenode.FSNamesystem: fsLock is fair: true\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:16,005 INFO namenode.FSNamesystem: Detailed lock hold time metrics enabled: false\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:16,009 INFO namenode.FSNamesystem: fsOwner             = root (auth:SIMPLE)\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:16,009 INFO namenode.FSNamesystem: supergroup          = supergroup\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:16,009 INFO namenode.FSNamesystem: isPermissionEnabled = true\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:16,009 INFO namenode.FSNamesystem: HA Enabled: false\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:16,049 INFO common.Util: dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:16,059 INFO blockmanagement.DatanodeManager: dfs.block.invalidate.limit: configured=1000, counted=60, effected=1000\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:16,059 INFO blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:16,062 INFO blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:16,063 INFO blockmanagement.BlockManager: The block deletion will start around 2021 Mar 02 05:26:16\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:16,064 INFO util.GSet: Computing capacity for map BlocksMap\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:16,064 INFO util.GSet: VM type       = 64-bit\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:16,065 INFO util.GSet: 2.0% max memory 13.5 GB = 277.2 MB\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:16,065 INFO util.GSet: capacity      = 2^25 = 33554432 entries\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:16,144 INFO blockmanagement.BlockManager: Storage policy satisfier is disabled\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:16,144 INFO blockmanagement.BlockManager: dfs.block.access.token.enable = false\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:16,150 INFO Configuration.deprecation: No unit for dfs.namenode.safemode.extension(30000) assuming MILLISECONDS\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:16,150 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.threshold-pct = 0.9990000128746033\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:16,150 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.min.datanodes = 0\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:16,150 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.extension = 30000\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:16,151 INFO blockmanagement.BlockManager: defaultReplication         = 3\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:16,151 INFO blockmanagement.BlockManager: maxReplication             = 512\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:16,151 INFO blockmanagement.BlockManager: minReplication             = 1\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:16,151 INFO blockmanagement.BlockManager: maxReplicationStreams      = 2\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:16,151 INFO blockmanagement.BlockManager: redundancyRecheckInterval  = 3000ms\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:16,151 INFO blockmanagement.BlockManager: encryptDataTransfer        = false\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:16,151 INFO blockmanagement.BlockManager: maxNumBlocksToLog          = 1000\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:16,172 INFO namenode.FSDirectory: GLOBAL serial map: bits=29 maxEntries=536870911\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:16,172 INFO namenode.FSDirectory: USER serial map: bits=24 maxEntries=16777215\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:16,172 INFO namenode.FSDirectory: GROUP serial map: bits=24 maxEntries=16777215\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:16,172 INFO namenode.FSDirectory: XATTR serial map: bits=24 maxEntries=16777215\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:16,183 INFO util.GSet: Computing capacity for map INodeMap\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:16,183 INFO util.GSet: VM type       = 64-bit\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:16,184 INFO util.GSet: 1.0% max memory 13.5 GB = 138.6 MB\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:16,184 INFO util.GSet: capacity      = 2^24 = 16777216 entries\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:16,220 INFO namenode.FSDirectory: ACLs enabled? false\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:16,220 INFO namenode.FSDirectory: POSIX ACL inheritance enabled? true\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:16,220 INFO namenode.FSDirectory: XAttrs enabled? true\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:16,220 INFO namenode.NameNode: Caching file names occurring more than 10 times\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:16,225 INFO snapshot.SnapshotManager: Loaded config captureOpenFiles: false, skipCaptureAccessTimeOnlyChange: false, snapshotDiffAllowSnapRootDescendant: true, maxSnapshotLimit: 65536\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:16,226 INFO snapshot.SnapshotManager: SkipList is disabled\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:16,230 INFO util.GSet: Computing capacity for map cachedBlocks\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:16,230 INFO util.GSet: VM type       = 64-bit\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:16,230 INFO util.GSet: 0.25% max memory 13.5 GB = 34.6 MB\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:16,230 INFO util.GSet: capacity      = 2^22 = 4194304 entries\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:16,246 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:16,246 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:16,246 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:16,249 INFO namenode.FSNamesystem: Retry cache on namenode is enabled\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:16,249 INFO namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:16,251 INFO util.GSet: Computing capacity for map NameNodeRetryCache\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:16,251 INFO util.GSet: VM type       = 64-bit\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:16,251 INFO util.GSet: 0.029999999329447746% max memory 13.5 GB = 4.2 MB\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:16,251 INFO util.GSet: capacity      = 2^19 = 524288 entries\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:16,277 INFO namenode.FSImage: Allocated new BlockPoolId: BP-2073241018-10.0.189.145-1614662776271\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:16,290 INFO common.Storage: Storage directory /opt/amazon/hadoop/hdfs/namenode has been successfully formatted.\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:16,312 INFO namenode.FSImageFormatProtobuf: Saving image file /opt/amazon/hadoop/hdfs/namenode/current/fsimage.ckpt_0000000000000000000 using no compression\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:16,409 INFO namenode.FSImageFormatProtobuf: Image file /opt/amazon/hadoop/hdfs/namenode/current/fsimage.ckpt_0000000000000000000 of size 399 bytes saved in 0 seconds .\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:16,417 INFO namenode.NNStorageRetentionManager: Going to retain 1 images with txid >= 0\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:16,532 INFO namenode.FSImage: FSImageSaver clean checkpoint: txid=0 when meet shutdown.\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:16,532 INFO namenode.NameNode: SHUTDOWN_MSG: \u001b[0m\n",
      "\u001b[34m/************************************************************\u001b[0m\n",
      "\u001b[34mSHUTDOWN_MSG: Shutting down NameNode at algo-1/10.0.189.145\u001b[0m\n",
      "\u001b[34m************************************************************/\u001b[0m\n",
      "\u001b[34m03-02 05:26 smspark-submit INFO     waiting for cluster to be up\u001b[0m\n",
      "\u001b[34mWARNING: YARN_LOG_DIR has been replaced by HADOOP_LOG_DIR. Using value of YARN_LOG_DIR.\u001b[0m\n",
      "\u001b[34mWARNING: YARN_LOG_DIR has been replaced by HADOOP_LOG_DIR. Using value of YARN_LOG_DIR.\u001b[0m\n",
      "\u001b[34mWARNING: /var/log/yarn/ does not exist. Creating.\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:16,230 INFO util.GSet: 0.25% max memory 13.5 GB = 34.6 MB\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:16,230 INFO util.GSet: capacity      = 2^22 = 4194304 entries\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:16,246 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:16,246 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:16,246 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:16,249 INFO namenode.FSNamesystem: Retry cache on namenode is enabled\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:16,249 INFO namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:16,251 INFO util.GSet: Computing capacity for map NameNodeRetryCache\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:16,251 INFO util.GSet: VM type       = 64-bit\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:16,251 INFO util.GSet: 0.029999999329447746% max memory 13.5 GB = 4.2 MB\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:16,251 INFO util.GSet: capacity      = 2^19 = 524288 entries\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:16,277 INFO namenode.FSImage: Allocated new BlockPoolId: BP-2073241018-10.0.189.145-1614662776271\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:16,290 INFO common.Storage: Storage directory /opt/amazon/hadoop/hdfs/namenode has been successfully formatted.\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:16,312 INFO namenode.FSImageFormatProtobuf: Saving image file /opt/amazon/hadoop/hdfs/namenode/current/fsimage.ckpt_0000000000000000000 using no compression\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:16,409 INFO namenode.FSImageFormatProtobuf: Image file /opt/amazon/hadoop/hdfs/namenode/current/fsimage.ckpt_0000000000000000000 of size 399 bytes saved in 0 seconds .\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:16,417 INFO namenode.NNStorageRetentionManager: Going to retain 1 images with txid >= 0\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:16,532 INFO namenode.FSImage: FSImageSaver clean checkpoint: txid=0 when meet shutdown.\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:16,532 INFO namenode.NameNode: SHUTDOWN_MSG: \u001b[0m\n",
      "\u001b[35m/************************************************************\u001b[0m\n",
      "\u001b[35mSHUTDOWN_MSG: Shutting down NameNode at algo-1/10.0.189.145\u001b[0m\n",
      "\u001b[35m************************************************************/\u001b[0m\n",
      "\u001b[35m03-02 05:26 smspark-submit INFO     waiting for cluster to be up\u001b[0m\n",
      "\u001b[35mWARNING: YARN_LOG_DIR has been replaced by HADOOP_LOG_DIR. Using value of YARN_LOG_DIR.\u001b[0m\n",
      "\u001b[35mWARNING: YARN_LOG_DIR has been replaced by HADOOP_LOG_DIR. Using value of YARN_LOG_DIR.\u001b[0m\n",
      "\u001b[35mWARNING: /var/log/yarn/ does not exist. Creating.\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:17,059 INFO nodemanager.NodeManager: STARTUP_MSG: \u001b[0m\n",
      "\u001b[34m/************************************************************\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG: Starting NodeManager\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   host = algo-1/10.0.189.145\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   args = []\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:17,059 INFO nodemanager.NodeManager: STARTUP_MSG: \u001b[0m\n",
      "\u001b[35m/************************************************************\u001b[0m\n",
      "\u001b[35mSTARTUP_MSG: Starting NodeManager\u001b[0m\n",
      "\u001b[35mSTARTUP_MSG:   host = algo-1/10.0.189.145\u001b[0m\n",
      "\u001b[35mSTARTUP_MSG:   args = []\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   version = 3.2.1-amzn-1\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   classpath = /usr/lib/hadoop/etc/hadoop:/usr/lib/hadoop/lib/jackson-xc-1.9.13.jar:/usr/lib/hadoop/lib/kerby-pkix-1.0.1.jar:/usr/lib/hadoop/lib/httpclient-4.5.9.jar:/usr/lib/hadoop/lib/zookeeper-3.4.14.jar:/usr/lib/hadoop/lib/jersey-core-1.19.jar:/usr/lib/hadoop/lib/re2j-1.1.jar:/usr/lib/hadoop/lib/jsp-api-2.1.jar:/usr/lib/hadoop/lib/kerby-config-1.0.1.jar:/usr/lib/hadoop/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop/lib/guava-11.0.2.jar:/usr/lib/hadoop/lib/jackson-core-2.10.0.jar:/usr/lib/hadoop/lib/commons-text-1.4.jar:/usr/lib/hadoop/lib/kerb-common-1.0.1.jar:/usr/lib/hadoop/lib/htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop/lib/kerb-admin-1.0.1.jar:/usr/lib/hadoop/lib/jetty-io-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/javax.servlet-api-3.1.0.jar:/usr/lib/hadoop/lib/commons-collections-3.2.2.jar:/usr/lib/hadoop/lib/kerb-util-1.0.1.jar:/usr/lib/hadoop/lib/paranamer-2.3.jar:/usr/lib/hadoop/lib/httpcore-4.4.11.jar:/usr/lib/hadoop/lib/jackson-databind-2.10.0.jar:/usr/lib/hadoop/lib/jsr305-3.0.0.jar:/usr/lib/hadoop/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop/lib/dnsjava-2.1.7.jar:/usr/lib/hadoop/lib/curator-recipes-2.13.0.jar:/usr/lib/hadoop/lib/stax2-api-3.1.4.jar:/usr/lib/hadoop/lib/slf4j-api-1.7.25.jar:/usr/lib/hadoop/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop/lib/kerb-client-1.0.1.jar:/usr/lib/hadoop/lib/commons-configuration2-2.1.1.jar:/usr/lib/hadoop/lib/asm-5.0.4.jar:/usr/lib/hadoop/lib/curator-client-2.13.0.jar:/usr/lib/hadoop/lib/commons-cli-1.2.jar:/usr/lib/hadoop/lib/gson-2.2.4.jar:/usr/lib/hadoop/lib/nimbus-jose-jwt-4.41.1.jar:/usr/lib/hadoop/lib/jetty-servlet-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/kerb-crypto-1.0.1.jar:/usr/lib/hadoop/lib/slf4j-log4j12-1.7.25.jar:/usr/lib/hadoop/lib/jetty-webapp-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/jetty-http-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/curator-framework-2.13.0.jar:/usr/lib/hadoop/lib/jersey-servlet-1.19.jar:/usr/lib/hadoop/lib/kerby-asn1-1.0.1.jar:/usr/lib/hadoop/lib/snappy-java-1.1.7.3.jar:/usr/lib/hadoop/lib/jsr311-api-1.1.1.jar:/usr/lib/hadoop/lib/commons-codec-1.11.jar:/usr/lib/hadoop/lib/jetty-xml-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/jetty-util-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop/lib/json-smart-2.3.jar:/usr/lib/hadoop/lib/commons-compress-1.18.jar:/usr/lib/hadoop/lib/jetty-server-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/accessors-smart-1.2.jar:/usr/lib/hadoop/lib/jersey-json-1.19.jar:/usr/lib/hadoop/lib/metrics-core-3.2.4.jar:/usr/lib/hadoop/lib/kerb-server-1.0.1.jar:/usr/lib/hadoop/lib/commons-io-2.5.jar:/usr/lib/hadoop/lib/jetty-security-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/jul-to-slf4j-1.7.25.jar:/usr/lib/hadoop/lib/netty-3.10.5.Final.jar:/usr/lib/hadoop/lib/token-provider-1.0.1.jar:/usr/lib/hadoop/lib/avro-1.7.7.jar:/usr/lib/hadoop/lib/log4j-1.2.17.jar:/usr/lib/hadoop/lib/commons-math3-3.1.1.jar:/usr/lib/hadoop/lib/jaxb-api-2.2.11.jar:/usr/lib/hadoop/lib/jackson-annotations-2.10.0.jar:/usr/lib/hadoop/lib/jettison-1.1.jar:/usr/lib/hadoop/lib/kerb-simplekdc-1.0.1.jar:/usr/lib/hadoop/lib/jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop/lib/kerby-util-1.0.1.jar:/usr/lib/hadoop/lib/commons-lang3-3.7.jar:/usr/lib/hadoop/lib/commons-beanutils-1.9.3.jar:/usr/lib/hadoop/lib/kerby-xdr-1.0.1.jar:/usr/lib/hadoop/lib/jersey-server-1.19.jar:/usr/lib/hadoop/lib/commons-net-3.6.jar:/usr/lib/hadoop/lib/kerb-identity-1.0.1.jar:/usr/lib/hadoop/lib/jsch-0.1.54.jar:/usr/lib/hadoop/lib/woodstox-core-5.0.3.jar:/usr/lib/hadoop/lib/jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop/lib/jcip-annotations-1.0-1.jar:/usr/lib/hadoop/lib/kerb-core-1.0.1.jar:/usr/lib/hadoop/.//hadoop-fs2img-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-extras.jar:/usr/lib/hadoop/.//hadoop-common-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop/.//hadoop-archives.jar:/usr/lib/hadoop/.//hadoop-sls-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-resourceestimator.jar:/usr/lib/hadoop/.//hadoop-sls.jar:/usr/lib/hadoop/.//hadoop-aws-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-aliyun-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-nfs-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-azure.jar:/usr/lib/hadoop/.//hadoop-nfs.jar:/usr/lib/hadoop/.//hadoop-auth.jar:/usr/lib/hadoop/.//hadoop-azure-datalake.jar:/usr/lib/hadoop/.//hadoop-annotations-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-openstack.jar:/usr/lib/hadoop/.//hadoop-streaming-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-distcp.jar:/usr/lib/hadoop/.//hadoop-archive-logs-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-azure-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-gridmix.jar:/usr/lib/hadoop/.//hadoop-auth-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-datajoin-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-openstack-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-rumen-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-datajoin.jar:/usr/lib/hadoop/.//hadoop-streaming.jar:/usr/lib/hadoop/.//hadoop-azure-datalake-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-extras-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-archive-logs.jar:/usr/lib/hadoop/.//hadoop-gridmix-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-resourceestimator-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-annotations.jar:/usr/lib/hadoop/.//hadoop-common-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-rumen.jar:/usr/lib/hadoop/.//hadoop-kms.jar:/usr/lib/hadoop/.//hadoop-kafka.jar:/usr/lib/hadoop/.//hadoop-archives-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-aliyun.jar:/usr/lib/hadoop/.//hadoop-aws.jar:/usr/lib/hadoop/.//hadoop-fs2img.jar:/usr/lib/hadoop/.//hadoop-kafka-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-kms-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-common.jar:/usr/lib/hadoop/.//hadoop-distcp-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/./:/usr/lib/hadoop-hdfs/lib/jackson-xc-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/kerby-pkix-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/httpclient-4.5.9.jar:/usr/lib/hadoop-hdfs/lib/zookeeper-3.4.14.jar:/usr/lib/hadoop-hdfs/lib/jersey-core-1.19.jar:/usr/lib/hadoop-hdfs/lib/re2j-1.1.jar:/usr/lib/hadoop-hdfs/lib/kerby-config-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop-hdfs/lib/guava-11.0.2.jar:/usr/lib/hadoop-hdfs/lib/jackson-core-2.10.0.jar:/usr/lib/hadoop-hdfs/lib/commons-text-1.4.jar:/usr/lib/hadoop-hdfs/lib/kerb-common-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop-hdfs/lib/kerb-admin-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jetty-io-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/javax.servlet-api-3.1.0.jar:/usr/lib/hadoop-hdfs/lib/commons-collections-3.2.2.jar:/usr/lib/hadoop-hdfs/lib/kerb-util-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/paranamer-2.3.jar:/usr/lib/hadoop-hdfs/lib/httpcore-4.4.11.jar:/usr/lib/hadoop-hdfs/lib/jackson-databind-2.10.0.jar:/usr/lib/hadoop-hdfs/lib/jsr305-3.0.0.jar:/usr/lib/hadoop-hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/dnsjava-2.1.7.jar:/usr/lib/hadoop-hdfs/lib/curator-recipes-2.13.0.jar:/usr/lib/hadoop-hdfs/lib/stax2-api-3.1.4.jar:/usr/lib/hadoop-hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/kerb-client-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/commons-configuration2-2.1.1.jar:/usr/lib/hadoop-hdfs/lib/asm-5.0.4.jar:/usr/lib/hadoop-hdfs/lib/curator-client-2.13.0.jar:/usr/lib/hadoop-hdfs/lib/commons-cli-1.2.jar:/usr/lib/hadoop-hdfs/lib/gson-2.2.4.jar:/usr/lib/hadoop-hdfs/lib/nimbus-jose-jwt-4.41.1.jar:/usr/lib/hadoop-hdfs/lib/jetty-servlet-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/kerb-crypto-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jetty-webapp-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/jetty-http-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/curator-framework-2.13.0.jar:/usr/lib/hadoop-hdfs/lib/jersey-servlet-1.19.jar:/usr/lib/hadoop-hdfs/lib/kerby-asn1-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/snappy-java-1.1.7.3.jar:/usr/lib/hadoop-hdfs/lib/jsr311-api-1.1.1.jar:/usr/lib/hadoop-hdfs/lib/json-simple-1.1.1.jar:/usr/lib/hadoop-hdfs/lib/commons-codec-1.11.jar:/usr/lib/hadoop-hdfs/lib/jetty-xml-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/jetty-util-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop-hdfs/lib/json-smart-2.3.jar:/usr/lib/hadoop-hdfs/lib/commons-compress-1.18.jar:/usr/lib/hadoop-hdfs/lib/jetty-server-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/netty-all-4.0.52.Final.jar:/usr/lib/hadoop-hdfs/lib/accessors-smart-1.2.jar:/usr/lib/hadoop-hdfs/lib/jersey-json-1.19.jar:/usr/lib/hadoop-hdfs/lib/kerb-server-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/commons-io-2.5.jar:/usr/lib/hadoop-hdfs/lib/jetty-security-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/netty-3.10.5.Final.jar:/usr/lib/hadoop-hdfs/lib/token-provider-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/avro-1.7.7.jar:/usr/lib/hadoop-hdfs/lib/jetty-util-ajax-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/log4j-1.2.17.jar:/usr/lib/hadoop-hdfs/lib/commons-math3-3.1.1.jar:/usr/lib/hadoop-hdfs/lib/jaxb-api-2.2.11.jar:/usr/lib/hadoop-hdfs/lib/jackson-annotations-2.10.0.jar:/usr/lib/hadoop-hdfs/lib/commons-httpclient-3.1.jar:/usr/lib/hadoop-hdfs/lib/jettison-1.1.jar:/usr/lib/hadoop-hdfs/lib/kerb-simplekdc-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop-hdfs/lib/kerby-util-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/okhttp-2.7.5.jar:/usr/lib/hadoop-hdfs/lib/commons-lang3-3.7.jar:/usr/lib/hadoop-hdfs/lib/commons-beanutils-1.9.3.jar:/usr/lib/hadoop-hdfs/lib/commons-daemon-1.0.13.jar:/usr/lib/hadoop-hdfs/lib/kerby-xdr-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/okio-1.6.0.jar:/usr/lib/hadoop-hdfs/lib/jersey-server-1.19.jar:/usr/lib/hadoop-hdfs/lib/commons-net-3.6.jar:/usr/lib/hadoop-hdfs/lib/kerb-identity-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jsch-0.1.54.jar:/usr/lib/hadoop-hdfs/lib/leveldbjni-all-1.8.jar:/usr/lib/hadoop-hdfs/lib/woodstox-core-5.0.3.jar:/usr/lib/hadoop-hdfs/lib/jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/jcip-annotations-1.0-1.jar:/usr/lib/hadoop-hdfs/lib/kerb-core-1.0.1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-httpfs-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-nfs-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-httpfs.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-nfs.jar:/usr/lib/hadoop-mapreduce/.//azure-data-lake-store-sdk-2.2.9.jar:/usr/lib/hadoop-mapreduce/.//kafka-clients-2.4.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-fs2img-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-extras.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-core.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archives.jar:/usr/lib/hadoop-mapreduce/.//hadoop-sls-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-resourceestimator.jar:/usr/lib/hadoop-mapreduce/.//azure-storage-7.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-sls.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aws-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aliyun-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-common-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-datalake.jar:/usr/lib/hadoop-mapreduce/.//hadoop-openstack.jar:/usr/lib/hadoop-mapreduce/.//hadoop-streaming-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-nativetask.jar:/usr/lib/hadoop-mapreduce/.//hadoop-distcp.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archive-logs-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//ojalgo-43.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-plugins.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-gridmix.jar:/usr/lib/hadoop-mapreduce/.//hadoop-datajoin-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//aws-java-sdk-bundle-1.11.828.jar:/usr/lib/hadoop-mapreduce/.//hadoop-openstack-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-common.jar:/usr/lib/hadoop-mapreduce/.//zstd-jni-1.4.3-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-rumen-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-datajoin.jar:/usr/lib/hadoop-mapreduce/.//hadoop-streaming.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-datalake-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-core-3.4.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-app.jar:/usr/lib/hadoop-mapreduce/.//hadoop-extras-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//aliyun-sdk-oss-3.4.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archive-logs.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-ecs-4.2.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-examples.jar:/usr/lib/hadoop-mapreduce/.//hadoop-gridmix-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-resourceestimator-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-uploader.jar:/usr/lib/hadoop-mapreduce/.//wildfly-openssl-1.0.7.Final.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-uploader-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-app-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-rumen.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-core-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-examples-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient.jar:/usr/lib/hadoop-mapreduce/.//hadoop-kafka.jar:/usr/lib/hadoop-mapreduce/.//azure-keyvault-core-1.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archives-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-shuffle-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aliyun.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aws.jar:/usr/lib/hadoop-mapreduce/.//hadoop-fs2img.jar:/usr/lib/hadoop-mapreduce/.//hadoop-kafka-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//lz4-java-1.6.0.jar:/usr/lib/hadoop-mapreduce/.//jdom-1.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-shuffle.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-nativetask-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-sts-3.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-plugins-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-ram-3.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-distcp-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop-yarn/lib/jackson-module-jaxb-annotations-2.10.0.jar:/usr/lib/hadoop-yarn/lib/jackson-jaxrs-json-provider-2.10.0.jar:/usr/lib/hadoop-yarn/lib/jackson-jaxrs-base-2.10.0.jar:/usr/lib/hadoop-yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/usr/lib/hadoop-yarn/lib/javax.inject-1.jar:/usr/lib/hadoop-yarn/lib/bcprov-jdk15on-1.60.jar:/usr/lib/hadoop-yarn/lib/bcpkix-jdk15on-1.60.jar:/usr/lib/hadoop-yarn/lib/swagger-annotations-1.5.4.jar:/usr/lib/hadoop-yarn/lib/jersey-client-1.19.jar:/usr/lib/hadoop-yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/usr/lib/hadoop-yarn/lib/jakarta.xml.bind-api-2.3.2.jar:/usr/lib/hadoop-yarn/lib/guice-servlet-4.0.jar:/usr/lib/hadoop-yarn/lib/objenesis-1.0.jar:/usr/lib/hadoop-yarn/lib/jersey-guice-1.19.jar:/usr/lib/hadoop-yarn/lib/jakarta.activation-api-1.2.1.jar:/usr/lib/hadoop-yarn/lib/ehcache-3.3.1.jar:/usr/lib/hadoop-yarn/lib/metrics-core-3.2.4.jar:/usr/lib/hadoop-yarn/lib/aopalliance-1.0.jar:/usr/lib/hadoop-yarn/lib/guice-4.0.jar:/usr/lib/hadoop-yarn/lib/java-util-1.9.0.jar:/usr/lib/hadoop-yarn/lib/fst-2.50.jar:/usr/lib/hadoop-yarn/lib/snakeyaml-1.16.jar:/usr/lib/hadoop-yarn/lib/HikariCP-java7-2.4.12.jar:/usr/lib/hadoop-yarn/lib/json-io-2.5.1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-unmanaged-am-launcher-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-tests.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-client.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-resourcemanager-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-\u001b[0m\n",
      "\u001b[34myarn-server-applicationhistoryservice-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-unmanaged-am-launcher.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-registry.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-api.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-timeline-pluginstorage-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-nodemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-common.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-core-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-tests-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-registry-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-common-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-timeline-pluginstorage.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-client-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-submarine.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-sharedcachemanager-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-common-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-web-proxy-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-sharedcachemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-applicationhistoryservice.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-api-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-router-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-common.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-core.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-resourcemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-submarine-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-api-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-distributedshell.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-router.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-api.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-nodemanager-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-distributedshell-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-web-proxy.jar:/usr/lib/hadoop-yarn/.//timelineservice/hadoop-yarn-server-timelineservice-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//timelineservice/hadoop-yarn-server-timelineservice-hbase-common-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//timelineservice/hadoop-yarn-server-timelineservice-hbase-client-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//timelineservice/hadoop-yarn-server-timelineservice-hbase-coprocessor-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/commons-lang-2.6.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/commons-csv-1.0.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/jcodings-1.0.13.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/hbase-client-1.2.6.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/metrics-core-2.2.0.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/hbase-common-1.2.6.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/htrace-core-3.1.0-incubating.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/joni-2.1.2.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/hbase-annotations-1.2.6.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/hbase-protocol-1.2.6.jar\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   build = git@aws157git.com:/pkg/Aws157BigTop -r 520b97ed429ab8eb7a8d30068b0f34a8036c51a7; compiled by 'ec2-user' on 2020-07-30T08:02Z\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   java = 1.8.0_272\u001b[0m\n",
      "\u001b[34m************************************************************/\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:17,066 INFO nodemanager.NodeManager: registered UNIX signal handlers for [TERM, HUP, INT]\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:17,083 INFO resourcemanager.ResourceManager: STARTUP_MSG: \u001b[0m\n",
      "\u001b[34m/************************************************************\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG: Starting ResourceManager\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   host = algo-1/10.0.189.145\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   args = []\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   version = 3.2.1-amzn-1\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   classpath = /usr/lib/hadoop/etc/hadoop:/usr/lib/hadoop/lib/jackson-xc-1.9.13.jar:/usr/lib/hadoop/lib/kerby-pkix-1.0.1.jar:/usr/lib/hadoop/lib/httpclient-4.5.9.jar:/usr/lib/hadoop/lib/zookeeper-3.4.14.jar:/usr/lib/hadoop/lib/jersey-core-1.19.jar:/usr/lib/hadoop/lib/re2j-1.1.jar:/usr/lib/hadoop/lib/jsp-api-2.1.jar:/usr/lib/hadoop/lib/kerby-config-1.0.1.jar:/usr/lib/hadoop/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop/lib/guava-11.0.2.jar:/usr/lib/hadoop/lib/jackson-core-2.10.0.jar:/usr/lib/hadoop/lib/commons-text-1.4.jar:/usr/lib/hadoop/lib/kerb-common-1.0.1.jar:/usr/lib/hadoop/lib/htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop/lib/kerb-admin-1.0.1.jar:/usr/lib/hadoop/lib/jetty-io-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/javax.servlet-api-3.1.0.jar:/usr/lib/hadoop/lib/commons-collections-3.2.2.jar:/usr/lib/hadoop/lib/kerb-util-1.0.1.jar:/usr/lib/hadoop/lib/paranamer-2.3.jar:/usr/lib/hadoop/lib/httpcore-4.4.11.jar:/usr/lib/hadoop/lib/jackson-databind-2.10.0.jar:/usr/lib/hadoop/lib/jsr305-3.0.0.jar:/usr/lib/hadoop/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop/lib/dnsjava-2.1.7.jar:/usr/lib/hadoop/lib/curator-recipes-2.13.0.jar:/usr/lib/hadoop/lib/stax2-api-3.1.4.jar:/usr/lib/hadoop/lib/slf4j-api-1.7.25.jar:/usr/lib/hadoop/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop/lib/kerb-client-1.0.1.jar:/usr/lib/hadoop/lib/commons-configuration2-2.1.1.jar:/usr/lib/hadoop/lib/asm-5.0.4.jar:/usr/lib/hadoop/lib/curator-client-2.13.0.jar:/usr/lib/hadoop/lib/commons-cli-1.2.jar:/usr/lib/hadoop/lib/gson-2.2.4.jar:/usr/lib/hadoop/lib/nimbus-jose-jwt-4.41.1.jar:/usr/lib/hadoop/lib/jetty-servlet-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/kerb-crypto-1.0.1.jar:/usr/lib/hadoop/lib/slf4j-log4j12-1.7.25.jar:/usr/lib/hadoop/lib/jetty-webapp-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/jetty-http-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/curator-framework-2.13.0.jar:/usr/lib/hadoop/lib/jersey-servlet-1.19.jar:/usr/lib/hadoop/lib/kerby-asn1-1.0.1.jar:/usr/lib/hadoop/lib/snappy-java-1.1.7.3.jar:/usr/lib/hadoop/lib/jsr311-api-1.1.1.jar:/usr/lib/hadoop/lib/commons-codec-1.11.jar:/usr/lib/hadoop/lib/jetty-xml-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/jetty-util-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop/lib/json-smart-2.3.jar:/usr/lib/hadoop/lib/commons-compress-1.18.jar:/usr/lib/hadoop/lib/jetty-server-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/accessors-smart-1.2.jar:/usr/lib/hadoop/lib/jersey-json-1.19.jar:/usr/lib/hadoop/lib/metrics-core-3.2.4.jar:/usr/lib/hadoop/lib/kerb-server-1.0.1.jar:/usr/lib/hadoop/lib/commons-io-2.5.jar:/usr/lib/hadoop/lib/jetty-security-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/jul-to-slf4j-1.7.25.jar:/usr/lib/hadoop/lib/netty-3.10.5.Final.jar:/usr/lib/hadoop/lib/token-provider-1.0.1.jar:/usr/lib/hadoop/lib/avro-1.7.7.jar:/usr/lib/hadoop/lib/log4j-1.2.17.jar:/usr/lib/hadoop/lib/commons-math3-3.1.1.jar:/usr/lib/hadoop/lib/jaxb-api-2.2.11.jar:/usr/lib/hadoop/lib/jackson-annotations-2.10.0.jar:/usr/lib/hadoop/lib/jettison-1.1.jar:/usr/lib/hadoop/lib/kerb-simplekdc-1.0.1.jar:/usr/lib/hadoop/lib/jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop/lib/kerby-util-1.0.1.jar:/usr/lib/hadoop/lib/commons-lang3-3.7.jar:/usr/lib/hadoop/lib/commons-beanutils-1.9.3.jar:/usr/lib/hadoop/lib/kerby-xdr-1.0.1.jar:/usr/lib/hadoop/lib/jersey-server-1.19.jar:/usr/lib/hadoop/lib/commons-net-3.6.jar:/usr/lib/hadoop/lib/kerb-identity-1.0.1.jar:/usr/lib/hadoop/lib/jsch-0.1.54.jar:/usr/lib/hadoop/lib/woodstox-core-5.0.3.jar:/usr/lib/hadoop/lib/jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop/lib/jcip-annotations-1.0-1.jar:/usr/lib/hadoop/lib/kerb-core-1.0.1.jar:/usr/lib/hadoop/.//hadoop-fs2img-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-extras.jar:/usr/lib/hadoop/.//hadoop-common-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop/.//hadoop-archives.jar:/usr/lib/hadoop/.//hadoop-sls-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-resourceestimator.jar:/usr/lib/hadoop/.//hadoop-sls.jar:/usr/lib/hadoop/.//hadoop-aws-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-aliyun-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-nfs-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-azure.jar:/usr/lib/hadoop/.//hadoop-nfs.jar:/usr/lib/hadoop/.//hadoop-auth.jar:/usr/lib/hadoop/.//hadoop-azure-datalake.jar:/usr/lib/hadoop/.//hadoop-annotations-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-openstack.jar:/usr/lib/hadoop/.//hadoop-streaming-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-distcp.jar:/usr/lib/hadoop/.//hadoop-archive-logs-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-azure-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-gridmix.jar:/usr/lib/hadoop/.//hadoop-auth-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-datajoin-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-openstack-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-rumen-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-datajoin.jar:/usr/lib/hadoop/.//hadoop-streaming.jar:/usr/lib/hadoop/.//hadoop-azure-datalake-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-extras-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-archive-logs.jar:/usr/lib/hadoop/.//hadoop-gridmix-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-resourceestimator-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-annotations.jar:/usr/lib/hadoop/.//hadoop-common-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-rumen.jar:/usr/lib/hadoop/.//hadoop-kms.jar:/usr/lib/hadoop/.//hadoop-kafka.jar:/usr/lib/hadoop/.//hadoop-archives-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-aliyun.jar:/usr/lib/hadoop/.//hadoop-aws.jar:/usr/lib/hadoop/.//hadoop-fs2img.jar:/usr/lib/hadoop/.//hadoop-kafka-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-kms-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-common.jar:/usr/lib/hadoop/.//hadoop-distcp-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/./:/usr/lib/hadoop-hdfs/lib/jackson-xc-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/kerby-pkix-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/httpclient-4.5.9.jar:/usr/lib/hadoop-hdfs/lib/zookeeper-3.4.14.jar:/usr/lib/hadoop-hdfs/lib/jersey-core-1.19.jar:/usr/lib/hadoop-hdfs/lib/re2j-1.1.jar:/usr/lib/hadoop-hdfs/lib/kerby-config-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop-hdfs/lib/guava-11.0.2.jar:/usr/lib/hadoop-hdfs/lib/jackson-core-2.10.0.jar:/usr/lib/hadoop-hdfs/lib/commons-text-1.4.jar:/usr/lib/hadoop-hdfs/lib/kerb-common-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop-hdfs/lib/kerb-admin-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jetty-io-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/javax.servlet-api-3.1.0.jar:/usr/lib/hadoop-hdfs/lib/commons-collections-3.2.2.jar:/usr/lib/hadoop-hdfs/lib/kerb-util-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/paranamer-2.3.jar:/usr/lib/hadoop-hdfs/lib/httpcore-4.4.11.jar:/usr/lib/hadoop-hdfs/lib/jackson-databind-2.10.0.jar:/usr/lib/hadoop-hdfs/lib/jsr305-3.0.0.jar:/usr/lib/hadoop-hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/dnsjava-2.1.7.jar:/usr/lib/hadoop-hdfs/lib/curator-recipes-2.13.0.jar:/usr/lib/hadoop-hdfs/lib/stax2-api-3.1.4.jar:/usr/lib/hadoop-hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/kerb-client-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/commons-configuration2-2.1.1.jar:/usr/lib/hadoop-hdfs/lib/asm-5.0.4.jar:/usr/lib/hadoop-hdfs/lib/curator-client-2.13.0.jar:/usr/lib/hadoop-hdfs/lib/commons-cli-1.2.jar:/usr/lib/hadoop-hdfs/lib/gson-2.2.4.jar:/usr/lib/hadoop-hdfs/lib/nimbus-jose-jwt-4.41.1.jar:/usr/lib/hadoop-hdfs/lib/jetty-servlet-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/kerb-crypto-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jetty-webapp-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/jetty-http-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/curator-framework-2.13.0.jar:/usr/lib/hadoop-hdfs/lib/jersey-servlet-1.19.jar:/usr/lib/hadoop-hdfs/lib/kerby-asn1-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/snappy-java-1.1.7.3.jar:/usr/lib/hadoop-hdfs/lib/jsr311-api-1.1.1.jar:/usr/lib/hadoop-hdfs/lib/json-simple-1.1.1.jar:/usr/lib/hadoop-hdfs/lib/commons-codec-1.11.jar:/usr/lib/hadoop-hdfs/lib/jetty-xml-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/jetty-util-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop-hdfs/lib/json-smart-2.3.jar:/usr/lib/hadoop-hdfs/lib/commons-compress-1.18.jar:/usr/lib/hadoop-hdfs/lib/jetty-server-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/netty-all-4.0.52.Final.jar:/usr/lib/hadoop-hdfs/lib/accessors-smart-1.2.jar:/usr/lib/hadoop-hdfs/lib/jersey-json-1.19.jar:/usr/lib/hadoop-hdfs/lib/kerb-server-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/commons-io-2.5.jar:/usr/lib/hadoop-hdfs/lib/jetty-security-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/netty-3.10.5.Final.jar:/usr/lib/hadoop-hdfs/lib/token-provider-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/avro-1.7.7.jar:/usr/lib/hadoop-hdfs/lib/jetty-util-ajax-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/log4j-1.2.17.jar:/usr/lib/hadoop-hdfs/lib/commons-math3-3.1.1.jar:/usr/lib/hadoop-hdfs/lib/jaxb-api-2.2.11.jar:/usr/lib/hadoop-hdfs/lib/jackson-annotations-2.10.0.jar:/usr/lib/hadoop-hdfs/lib/commons-httpclient-3.1.jar:/usr/lib/hadoop-hdfs/lib/jettison-1.1.jar:/usr/lib/hadoop-hdfs/lib/kerb-simplekdc-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop-hdfs/lib/kerby-util-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/okhttp-2.7.5.jar:/usr/lib/hadoop-hdfs/lib/commons-lang3-3.7.jar:/usr/lib/hadoop-hdfs/lib/commons-beanutils-1.9.3.jar:/usr/lib/hadoop-hdfs/lib/commons-daemon-1.0.13.jar:/usr/lib/hadoop-hdfs/lib/kerby-xdr-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/okio-1.6.0.jar:/usr/lib/hadoop-hdfs/lib/jersey-server-1.19.jar:/usr/lib/hadoop-hdfs/lib/commons-net-3.6.jar:/usr/lib/hadoop-hdfs/lib/kerb-identity-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jsch-0.1.54.jar:/usr/lib/hadoop-hdfs/lib/leveldbjni-all-1.8.jar:/usr/lib/hadoop-hdfs/lib/woodstox-core-5.0.3.jar:/usr/lib/hadoop-hdfs/lib/jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/jcip-annotations-1.0-1.jar:/usr/lib/hadoop-hdfs/lib/kerb-core-1.0.1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-httpfs-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-nfs-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-httpfs.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-nfs.jar:/usr/lib/hadoop-mapreduce/.//azure-data-lake-store-sdk-2.2.9.jar:/usr/lib/hadoop-mapreduce/.//kafka-clients-2.4.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-fs2img-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-extras.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-core.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archives.jar:/usr/lib/hadoop-mapreduce/.//hadoop-sls-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-resourceestimator.jar:/usr/lib/hadoop-mapreduce/.//azure-storage-7.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-sls.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aws-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aliyun-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-common-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-datalake.jar:/usr/lib/hadoop-mapreduce/.//hadoop-openstack.jar:/usr/lib/hadoop-mapreduce/.//hadoop-streaming-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-nativetask.jar:/usr/lib/hadoop-mapreduce/.//hadoop-distcp.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archive-logs-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//ojalgo-43.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-plugins.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-gridmix.jar:/usr/lib/hadoop-mapreduce/.//hadoop-datajoin-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//aws-java-sdk-bundle-1.11.828.jar:/usr/lib/hadoop-mapreduce/.//hadoop-openstack-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-common.jar:/usr/lib/hadoop-mapreduce/.//zstd-jni-1.4.3-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-rumen-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-datajoin.jar:/usr/lib/hadoop-mapreduce/.//hadoop-streaming.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-datalake-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-core-3.4.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-app.jar:/usr/lib/hadoop-mapreduce/.//hadoop-extras-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//aliyun-sdk-oss-3.4.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archive-logs.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-ecs-4.2.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-examples.jar:/usr/lib/hadoop-mapreduce/.//hadoop-gridmix-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-resourceestimator-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-uploader.jar:/usr/lib/hadoop-mapreduce/.//wildfly-openssl-1.0.7.Final.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-uploader-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-app-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-rumen.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-core-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-examples-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient.jar:/usr/lib/hadoop-mapreduce/.//hadoop-kafka.jar:/usr/lib/hadoop-mapreduce/.//azure-keyvault-core-1.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archives-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-shuffle-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aliyun.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aws.jar:/usr/lib/hadoop-mapreduce/.//hadoop-fs2img.jar:/usr/lib/hadoop-mapreduce/.//hadoop-kafka-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//lz4-java-1.6.0.jar:/usr/lib/hadoop-mapreduce/.//jdom-1.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-shuffle.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-nativetask-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-sts-3.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-plugins-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-ram-3.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-distcp-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop-yarn/lib/jackson-module-jaxb-annotations-2.10.0.jar:/usr/lib/hadoop-yarn/lib/jackson-jaxrs-json-provider-2.10.0.jar:/usr/lib/hadoop-yarn/lib/jackson-jaxrs-base-2.10.0.jar:/usr/lib/hadoop-yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/usr/lib/hadoop-yarn/lib/javax.inject-1.jar:/usr/lib/hadoop-yarn/lib/bcprov-jdk15on-1.60.jar:/usr/lib/hadoop-yarn/lib/bcpkix-jdk15on-1.60.jar:/usr/lib/hadoop-yarn/lib/swagger-annotations-1.5.4.jar:/usr/lib/hadoop-yarn/lib/jersey-client-1.19.jar:/usr/lib/hadoop-yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/usr/lib/hadoop-yarn/lib/jakarta.xml.bind-api-2.3.2.jar:/usr/lib/hadoop-yarn/lib/guice-servlet-4.0.jar:/usr/lib/hadoop-yarn/lib/objenesis-1.0.jar:/usr/lib/hadoop-yarn/lib/jersey-guice-1.19.jar:/usr/lib/hadoop-yarn/lib/jakarta.activation-api-1.2.1.jar:/usr/lib/hadoop-yarn/lib/ehcache-3.3.1.jar:/usr/lib/hadoop-yarn/lib/metrics-core-3.2.4.jar:/usr/lib/hadoop-yarn/lib/aopalliance-1.0.jar:/usr/lib/hadoop-yarn/lib/guice-4.0.jar:/usr/lib/hadoop-yarn/lib/java-util-1.9.0.jar:/usr/lib/hadoop-yarn/lib/fst-2.50.jar:/usr/lib/hadoop-yarn/lib/snakeyaml-1.16.jar:/usr/lib/hadoop-yarn/lib/HikariCP-java7-2.4.12.jar:/usr/lib/hadoop-yarn/lib/json-io-2.5.1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-unmanaged-am-launcher-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-tests.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-client.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-resourcemanager-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-\u001b[0m\n",
      "\u001b[34myarn-server-applicationhistoryservice-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-unmanaged-am-launcher.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-registry.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-api.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-timeline-pluginstorage-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-nodemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-common.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-core-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-tests-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-registry-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-common-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-timeline-pluginstorage.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-client-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-submarine.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-sharedcachemanager-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-common-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-web-proxy-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-sharedcachemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-applicationhistoryservice.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-api-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-router-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-common.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-core.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-resourcemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-submarine-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-api-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-distributedshell.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-router.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-api.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-nodemanager-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-distributedshell-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-web-proxy.jar:/usr/lib/hadoop-yarn/.//timelineservice/hadoop-yarn-server-timelineservice-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//timelineservice/hadoop-yarn-server-timelineservice-hbase-common-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//timelineservice/hadoop-yarn-server-timelineservice-hbase-client-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//timelineservice/hadoop-yarn-server-timelineservice-hbase-coprocessor-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/commons-lang-2.6.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/commons-csv-1.0.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/jcodings-1.0.13.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/hbase-client-1.2.6.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/metrics-core-2.2.0.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/hbase-common-1.2.6.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/htrace-core-3.1.0-incubating.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/joni-2.1.2.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/hbase-annotations-1.2.6.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/hbase-protocol-1.2.6.jar\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   build = git@aws157git.com:/pkg/Aws157BigTop -r 520b97ed429ab8eb7a8d30068b0f34a8036c51a7; compiled by 'ec2-user' on 2020-07-30T08:02Z\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   java = 1.8.0_272\u001b[0m\n",
      "\u001b[34m************************************************************/\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:17,091 INFO resourcemanager.ResourceManager: registered UNIX signal handlers for [TERM, HUP, INT]\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:17,136 INFO namenode.NameNode: STARTUP_MSG: \u001b[0m\n",
      "\u001b[34m/************************************************************\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG: Starting NameNode\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   host = algo-1/10.0.189.145\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   args = []\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   version = 3.2.1-amzn-1\u001b[0m\n",
      "\u001b[35mSTARTUP_MSG:   version = 3.2.1-amzn-1\u001b[0m\n",
      "\u001b[35mSTARTUP_MSG:   classpath = /usr/lib/hadoop/etc/hadoop:/usr/lib/hadoop/lib/jackson-xc-1.9.13.jar:/usr/lib/hadoop/lib/kerby-pkix-1.0.1.jar:/usr/lib/hadoop/lib/httpclient-4.5.9.jar:/usr/lib/hadoop/lib/zookeeper-3.4.14.jar:/usr/lib/hadoop/lib/jersey-core-1.19.jar:/usr/lib/hadoop/lib/re2j-1.1.jar:/usr/lib/hadoop/lib/jsp-api-2.1.jar:/usr/lib/hadoop/lib/kerby-config-1.0.1.jar:/usr/lib/hadoop/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop/lib/guava-11.0.2.jar:/usr/lib/hadoop/lib/jackson-core-2.10.0.jar:/usr/lib/hadoop/lib/commons-text-1.4.jar:/usr/lib/hadoop/lib/kerb-common-1.0.1.jar:/usr/lib/hadoop/lib/htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop/lib/kerb-admin-1.0.1.jar:/usr/lib/hadoop/lib/jetty-io-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/javax.servlet-api-3.1.0.jar:/usr/lib/hadoop/lib/commons-collections-3.2.2.jar:/usr/lib/hadoop/lib/kerb-util-1.0.1.jar:/usr/lib/hadoop/lib/paranamer-2.3.jar:/usr/lib/hadoop/lib/httpcore-4.4.11.jar:/usr/lib/hadoop/lib/jackson-databind-2.10.0.jar:/usr/lib/hadoop/lib/jsr305-3.0.0.jar:/usr/lib/hadoop/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop/lib/dnsjava-2.1.7.jar:/usr/lib/hadoop/lib/curator-recipes-2.13.0.jar:/usr/lib/hadoop/lib/stax2-api-3.1.4.jar:/usr/lib/hadoop/lib/slf4j-api-1.7.25.jar:/usr/lib/hadoop/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop/lib/kerb-client-1.0.1.jar:/usr/lib/hadoop/lib/commons-configuration2-2.1.1.jar:/usr/lib/hadoop/lib/asm-5.0.4.jar:/usr/lib/hadoop/lib/curator-client-2.13.0.jar:/usr/lib/hadoop/lib/commons-cli-1.2.jar:/usr/lib/hadoop/lib/gson-2.2.4.jar:/usr/lib/hadoop/lib/nimbus-jose-jwt-4.41.1.jar:/usr/lib/hadoop/lib/jetty-servlet-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/kerb-crypto-1.0.1.jar:/usr/lib/hadoop/lib/slf4j-log4j12-1.7.25.jar:/usr/lib/hadoop/lib/jetty-webapp-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/jetty-http-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/curator-framework-2.13.0.jar:/usr/lib/hadoop/lib/jersey-servlet-1.19.jar:/usr/lib/hadoop/lib/kerby-asn1-1.0.1.jar:/usr/lib/hadoop/lib/snappy-java-1.1.7.3.jar:/usr/lib/hadoop/lib/jsr311-api-1.1.1.jar:/usr/lib/hadoop/lib/commons-codec-1.11.jar:/usr/lib/hadoop/lib/jetty-xml-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/jetty-util-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop/lib/json-smart-2.3.jar:/usr/lib/hadoop/lib/commons-compress-1.18.jar:/usr/lib/hadoop/lib/jetty-server-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/accessors-smart-1.2.jar:/usr/lib/hadoop/lib/jersey-json-1.19.jar:/usr/lib/hadoop/lib/metrics-core-3.2.4.jar:/usr/lib/hadoop/lib/kerb-server-1.0.1.jar:/usr/lib/hadoop/lib/commons-io-2.5.jar:/usr/lib/hadoop/lib/jetty-security-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/jul-to-slf4j-1.7.25.jar:/usr/lib/hadoop/lib/netty-3.10.5.Final.jar:/usr/lib/hadoop/lib/token-provider-1.0.1.jar:/usr/lib/hadoop/lib/avro-1.7.7.jar:/usr/lib/hadoop/lib/log4j-1.2.17.jar:/usr/lib/hadoop/lib/commons-math3-3.1.1.jar:/usr/lib/hadoop/lib/jaxb-api-2.2.11.jar:/usr/lib/hadoop/lib/jackson-annotations-2.10.0.jar:/usr/lib/hadoop/lib/jettison-1.1.jar:/usr/lib/hadoop/lib/kerb-simplekdc-1.0.1.jar:/usr/lib/hadoop/lib/jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop/lib/kerby-util-1.0.1.jar:/usr/lib/hadoop/lib/commons-lang3-3.7.jar:/usr/lib/hadoop/lib/commons-beanutils-1.9.3.jar:/usr/lib/hadoop/lib/kerby-xdr-1.0.1.jar:/usr/lib/hadoop/lib/jersey-server-1.19.jar:/usr/lib/hadoop/lib/commons-net-3.6.jar:/usr/lib/hadoop/lib/kerb-identity-1.0.1.jar:/usr/lib/hadoop/lib/jsch-0.1.54.jar:/usr/lib/hadoop/lib/woodstox-core-5.0.3.jar:/usr/lib/hadoop/lib/jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop/lib/jcip-annotations-1.0-1.jar:/usr/lib/hadoop/lib/kerb-core-1.0.1.jar:/usr/lib/hadoop/.//hadoop-fs2img-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-extras.jar:/usr/lib/hadoop/.//hadoop-common-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop/.//hadoop-archives.jar:/usr/lib/hadoop/.//hadoop-sls-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-resourceestimator.jar:/usr/lib/hadoop/.//hadoop-sls.jar:/usr/lib/hadoop/.//hadoop-aws-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-aliyun-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-nfs-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-azure.jar:/usr/lib/hadoop/.//hadoop-nfs.jar:/usr/lib/hadoop/.//hadoop-auth.jar:/usr/lib/hadoop/.//hadoop-azure-datalake.jar:/usr/lib/hadoop/.//hadoop-annotations-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-openstack.jar:/usr/lib/hadoop/.//hadoop-streaming-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-distcp.jar:/usr/lib/hadoop/.//hadoop-archive-logs-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-azure-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-gridmix.jar:/usr/lib/hadoop/.//hadoop-auth-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-datajoin-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-openstack-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-rumen-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-datajoin.jar:/usr/lib/hadoop/.//hadoop-streaming.jar:/usr/lib/hadoop/.//hadoop-azure-datalake-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-extras-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-archive-logs.jar:/usr/lib/hadoop/.//hadoop-gridmix-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-resourceestimator-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-annotations.jar:/usr/lib/hadoop/.//hadoop-common-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-rumen.jar:/usr/lib/hadoop/.//hadoop-kms.jar:/usr/lib/hadoop/.//hadoop-kafka.jar:/usr/lib/hadoop/.//hadoop-archives-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-aliyun.jar:/usr/lib/hadoop/.//hadoop-aws.jar:/usr/lib/hadoop/.//hadoop-fs2img.jar:/usr/lib/hadoop/.//hadoop-kafka-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-kms-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-common.jar:/usr/lib/hadoop/.//hadoop-distcp-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/./:/usr/lib/hadoop-hdfs/lib/jackson-xc-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/kerby-pkix-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/httpclient-4.5.9.jar:/usr/lib/hadoop-hdfs/lib/zookeeper-3.4.14.jar:/usr/lib/hadoop-hdfs/lib/jersey-core-1.19.jar:/usr/lib/hadoop-hdfs/lib/re2j-1.1.jar:/usr/lib/hadoop-hdfs/lib/kerby-config-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop-hdfs/lib/guava-11.0.2.jar:/usr/lib/hadoop-hdfs/lib/jackson-core-2.10.0.jar:/usr/lib/hadoop-hdfs/lib/commons-text-1.4.jar:/usr/lib/hadoop-hdfs/lib/kerb-common-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop-hdfs/lib/kerb-admin-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jetty-io-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/javax.servlet-api-3.1.0.jar:/usr/lib/hadoop-hdfs/lib/commons-collections-3.2.2.jar:/usr/lib/hadoop-hdfs/lib/kerb-util-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/paranamer-2.3.jar:/usr/lib/hadoop-hdfs/lib/httpcore-4.4.11.jar:/usr/lib/hadoop-hdfs/lib/jackson-databind-2.10.0.jar:/usr/lib/hadoop-hdfs/lib/jsr305-3.0.0.jar:/usr/lib/hadoop-hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/dnsjava-2.1.7.jar:/usr/lib/hadoop-hdfs/lib/curator-recipes-2.13.0.jar:/usr/lib/hadoop-hdfs/lib/stax2-api-3.1.4.jar:/usr/lib/hadoop-hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/kerb-client-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/commons-configuration2-2.1.1.jar:/usr/lib/hadoop-hdfs/lib/asm-5.0.4.jar:/usr/lib/hadoop-hdfs/lib/curator-client-2.13.0.jar:/usr/lib/hadoop-hdfs/lib/commons-cli-1.2.jar:/usr/lib/hadoop-hdfs/lib/gson-2.2.4.jar:/usr/lib/hadoop-hdfs/lib/nimbus-jose-jwt-4.41.1.jar:/usr/lib/hadoop-hdfs/lib/jetty-servlet-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/kerb-crypto-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jetty-webapp-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/jetty-http-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/curator-framework-2.13.0.jar:/usr/lib/hadoop-hdfs/lib/jersey-servlet-1.19.jar:/usr/lib/hadoop-hdfs/lib/kerby-asn1-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/snappy-java-1.1.7.3.jar:/usr/lib/hadoop-hdfs/lib/jsr311-api-1.1.1.jar:/usr/lib/hadoop-hdfs/lib/json-simple-1.1.1.jar:/usr/lib/hadoop-hdfs/lib/commons-codec-1.11.jar:/usr/lib/hadoop-hdfs/lib/jetty-xml-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/jetty-util-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop-hdfs/lib/json-smart-2.3.jar:/usr/lib/hadoop-hdfs/lib/commons-compress-1.18.jar:/usr/lib/hadoop-hdfs/lib/jetty-server-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/netty-all-4.0.52.Final.jar:/usr/lib/hadoop-hdfs/lib/accessors-smart-1.2.jar:/usr/lib/hadoop-hdfs/lib/jersey-json-1.19.jar:/usr/lib/hadoop-hdfs/lib/kerb-server-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/commons-io-2.5.jar:/usr/lib/hadoop-hdfs/lib/jetty-security-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/netty-3.10.5.Final.jar:/usr/lib/hadoop-hdfs/lib/token-provider-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/avro-1.7.7.jar:/usr/lib/hadoop-hdfs/lib/jetty-util-ajax-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/log4j-1.2.17.jar:/usr/lib/hadoop-hdfs/lib/commons-math3-3.1.1.jar:/usr/lib/hadoop-hdfs/lib/jaxb-api-2.2.11.jar:/usr/lib/hadoop-hdfs/lib/jackson-annotations-2.10.0.jar:/usr/lib/hadoop-hdfs/lib/commons-httpclient-3.1.jar:/usr/lib/hadoop-hdfs/lib/jettison-1.1.jar:/usr/lib/hadoop-hdfs/lib/kerb-simplekdc-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop-hdfs/lib/kerby-util-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/okhttp-2.7.5.jar:/usr/lib/hadoop-hdfs/lib/commons-lang3-3.7.jar:/usr/lib/hadoop-hdfs/lib/commons-beanutils-1.9.3.jar:/usr/lib/hadoop-hdfs/lib/commons-daemon-1.0.13.jar:/usr/lib/hadoop-hdfs/lib/kerby-xdr-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/okio-1.6.0.jar:/usr/lib/hadoop-hdfs/lib/jersey-server-1.19.jar:/usr/lib/hadoop-hdfs/lib/commons-net-3.6.jar:/usr/lib/hadoop-hdfs/lib/kerb-identity-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jsch-0.1.54.jar:/usr/lib/hadoop-hdfs/lib/leveldbjni-all-1.8.jar:/usr/lib/hadoop-hdfs/lib/woodstox-core-5.0.3.jar:/usr/lib/hadoop-hdfs/lib/jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/jcip-annotations-1.0-1.jar:/usr/lib/hadoop-hdfs/lib/kerb-core-1.0.1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-httpfs-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-nfs-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-httpfs.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-nfs.jar:/usr/lib/hadoop-mapreduce/.//azure-data-lake-store-sdk-2.2.9.jar:/usr/lib/hadoop-mapreduce/.//kafka-clients-2.4.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-fs2img-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-extras.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-core.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archives.jar:/usr/lib/hadoop-mapreduce/.//hadoop-sls-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-resourceestimator.jar:/usr/lib/hadoop-mapreduce/.//azure-storage-7.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-sls.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aws-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aliyun-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-common-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-datalake.jar:/usr/lib/hadoop-mapreduce/.//hadoop-openstack.jar:/usr/lib/hadoop-mapreduce/.//hadoop-streaming-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-nativetask.jar:/usr/lib/hadoop-mapreduce/.//hadoop-distcp.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archive-logs-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//ojalgo-43.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-plugins.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-gridmix.jar:/usr/lib/hadoop-mapreduce/.//hadoop-datajoin-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//aws-java-sdk-bundle-1.11.828.jar:/usr/lib/hadoop-mapreduce/.//hadoop-openstack-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-common.jar:/usr/lib/hadoop-mapreduce/.//zstd-jni-1.4.3-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-rumen-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-datajoin.jar:/usr/lib/hadoop-mapreduce/.//hadoop-streaming.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-datalake-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-core-3.4.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-app.jar:/usr/lib/hadoop-mapreduce/.//hadoop-extras-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//aliyun-sdk-oss-3.4.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archive-logs.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-ecs-4.2.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-examples.jar:/usr/lib/hadoop-mapreduce/.//hadoop-gridmix-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-resourceestimator-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-uploader.jar:/usr/lib/hadoop-mapreduce/.//wildfly-openssl-1.0.7.Final.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-uploader-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-app-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-rumen.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-core-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-examples-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient.jar:/usr/lib/hadoop-mapreduce/.//hadoop-kafka.jar:/usr/lib/hadoop-mapreduce/.//azure-keyvault-core-1.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archives-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-shuffle-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aliyun.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aws.jar:/usr/lib/hadoop-mapreduce/.//hadoop-fs2img.jar:/usr/lib/hadoop-mapreduce/.//hadoop-kafka-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//lz4-java-1.6.0.jar:/usr/lib/hadoop-mapreduce/.//jdom-1.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-shuffle.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-nativetask-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-sts-3.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-plugins-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-ram-3.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-distcp-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop-yarn/lib/jackson-module-jaxb-annotations-2.10.0.jar:/usr/lib/hadoop-yarn/lib/jackson-jaxrs-json-provider-2.10.0.jar:/usr/lib/hadoop-yarn/lib/jackson-jaxrs-base-2.10.0.jar:/usr/lib/hadoop-yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/usr/lib/hadoop-yarn/lib/javax.inject-1.jar:/usr/lib/hadoop-yarn/lib/bcprov-jdk15on-1.60.jar:/usr/lib/hadoop-yarn/lib/bcpkix-jdk15on-1.60.jar:/usr/lib/hadoop-yarn/lib/swagger-annotations-1.5.4.jar:/usr/lib/hadoop-yarn/lib/jersey-client-1.19.jar:/usr/lib/hadoop-yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/usr/lib/hadoop-yarn/lib/jakarta.xml.bind-api-2.3.2.jar:/usr/lib/hadoop-yarn/lib/guice-servlet-4.0.jar:/usr/lib/hadoop-yarn/lib/objenesis-1.0.jar:/usr/lib/hadoop-yarn/lib/jersey-guice-1.19.jar:/usr/lib/hadoop-yarn/lib/jakarta.activation-api-1.2.1.jar:/usr/lib/hadoop-yarn/lib/ehcache-3.3.1.jar:/usr/lib/hadoop-yarn/lib/metrics-core-3.2.4.jar:/usr/lib/hadoop-yarn/lib/aopalliance-1.0.jar:/usr/lib/hadoop-yarn/lib/guice-4.0.jar:/usr/lib/hadoop-yarn/lib/java-util-1.9.0.jar:/usr/lib/hadoop-yarn/lib/fst-2.50.jar:/usr/lib/hadoop-yarn/lib/snakeyaml-1.16.jar:/usr/lib/hadoop-yarn/lib/HikariCP-java7-2.4.12.jar:/usr/lib/hadoop-yarn/lib/json-io-2.5.1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-unmanaged-am-launcher-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-tests.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-client.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-resourcemanager-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-\u001b[0m\n",
      "\u001b[35myarn-server-applicationhistoryservice-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-unmanaged-am-launcher.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-registry.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-api.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-timeline-pluginstorage-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-nodemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-common.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-core-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-tests-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-registry-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-common-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-timeline-pluginstorage.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-client-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-submarine.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-sharedcachemanager-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-common-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-web-proxy-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-sharedcachemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-applicationhistoryservice.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-api-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-router-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-common.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-core.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-resourcemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-submarine-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-api-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-distributedshell.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-router.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-api.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-nodemanager-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-distributedshell-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-web-proxy.jar:/usr/lib/hadoop-yarn/.//timelineservice/hadoop-yarn-server-timelineservice-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//timelineservice/hadoop-yarn-server-timelineservice-hbase-common-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//timelineservice/hadoop-yarn-server-timelineservice-hbase-client-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//timelineservice/hadoop-yarn-server-timelineservice-hbase-coprocessor-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/commons-lang-2.6.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/commons-csv-1.0.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/jcodings-1.0.13.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/hbase-client-1.2.6.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/metrics-core-2.2.0.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/hbase-common-1.2.6.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/htrace-core-3.1.0-incubating.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/joni-2.1.2.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/hbase-annotations-1.2.6.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/hbase-protocol-1.2.6.jar\u001b[0m\n",
      "\u001b[35mSTARTUP_MSG:   build = git@aws157git.com:/pkg/Aws157BigTop -r 520b97ed429ab8eb7a8d30068b0f34a8036c51a7; compiled by 'ec2-user' on 2020-07-30T08:02Z\u001b[0m\n",
      "\u001b[35mSTARTUP_MSG:   java = 1.8.0_272\u001b[0m\n",
      "\u001b[35m************************************************************/\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:17,066 INFO nodemanager.NodeManager: registered UNIX signal handlers for [TERM, HUP, INT]\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:17,083 INFO resourcemanager.ResourceManager: STARTUP_MSG: \u001b[0m\n",
      "\u001b[35m/************************************************************\u001b[0m\n",
      "\u001b[35mSTARTUP_MSG: Starting ResourceManager\u001b[0m\n",
      "\u001b[35mSTARTUP_MSG:   host = algo-1/10.0.189.145\u001b[0m\n",
      "\u001b[35mSTARTUP_MSG:   args = []\u001b[0m\n",
      "\u001b[35mSTARTUP_MSG:   version = 3.2.1-amzn-1\u001b[0m\n",
      "\u001b[35mSTARTUP_MSG:   classpath = /usr/lib/hadoop/etc/hadoop:/usr/lib/hadoop/lib/jackson-xc-1.9.13.jar:/usr/lib/hadoop/lib/kerby-pkix-1.0.1.jar:/usr/lib/hadoop/lib/httpclient-4.5.9.jar:/usr/lib/hadoop/lib/zookeeper-3.4.14.jar:/usr/lib/hadoop/lib/jersey-core-1.19.jar:/usr/lib/hadoop/lib/re2j-1.1.jar:/usr/lib/hadoop/lib/jsp-api-2.1.jar:/usr/lib/hadoop/lib/kerby-config-1.0.1.jar:/usr/lib/hadoop/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop/lib/guava-11.0.2.jar:/usr/lib/hadoop/lib/jackson-core-2.10.0.jar:/usr/lib/hadoop/lib/commons-text-1.4.jar:/usr/lib/hadoop/lib/kerb-common-1.0.1.jar:/usr/lib/hadoop/lib/htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop/lib/kerb-admin-1.0.1.jar:/usr/lib/hadoop/lib/jetty-io-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/javax.servlet-api-3.1.0.jar:/usr/lib/hadoop/lib/commons-collections-3.2.2.jar:/usr/lib/hadoop/lib/kerb-util-1.0.1.jar:/usr/lib/hadoop/lib/paranamer-2.3.jar:/usr/lib/hadoop/lib/httpcore-4.4.11.jar:/usr/lib/hadoop/lib/jackson-databind-2.10.0.jar:/usr/lib/hadoop/lib/jsr305-3.0.0.jar:/usr/lib/hadoop/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop/lib/dnsjava-2.1.7.jar:/usr/lib/hadoop/lib/curator-recipes-2.13.0.jar:/usr/lib/hadoop/lib/stax2-api-3.1.4.jar:/usr/lib/hadoop/lib/slf4j-api-1.7.25.jar:/usr/lib/hadoop/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop/lib/kerb-client-1.0.1.jar:/usr/lib/hadoop/lib/commons-configuration2-2.1.1.jar:/usr/lib/hadoop/lib/asm-5.0.4.jar:/usr/lib/hadoop/lib/curator-client-2.13.0.jar:/usr/lib/hadoop/lib/commons-cli-1.2.jar:/usr/lib/hadoop/lib/gson-2.2.4.jar:/usr/lib/hadoop/lib/nimbus-jose-jwt-4.41.1.jar:/usr/lib/hadoop/lib/jetty-servlet-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/kerb-crypto-1.0.1.jar:/usr/lib/hadoop/lib/slf4j-log4j12-1.7.25.jar:/usr/lib/hadoop/lib/jetty-webapp-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/jetty-http-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/curator-framework-2.13.0.jar:/usr/lib/hadoop/lib/jersey-servlet-1.19.jar:/usr/lib/hadoop/lib/kerby-asn1-1.0.1.jar:/usr/lib/hadoop/lib/snappy-java-1.1.7.3.jar:/usr/lib/hadoop/lib/jsr311-api-1.1.1.jar:/usr/lib/hadoop/lib/commons-codec-1.11.jar:/usr/lib/hadoop/lib/jetty-xml-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/jetty-util-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop/lib/json-smart-2.3.jar:/usr/lib/hadoop/lib/commons-compress-1.18.jar:/usr/lib/hadoop/lib/jetty-server-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/accessors-smart-1.2.jar:/usr/lib/hadoop/lib/jersey-json-1.19.jar:/usr/lib/hadoop/lib/metrics-core-3.2.4.jar:/usr/lib/hadoop/lib/kerb-server-1.0.1.jar:/usr/lib/hadoop/lib/commons-io-2.5.jar:/usr/lib/hadoop/lib/jetty-security-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/jul-to-slf4j-1.7.25.jar:/usr/lib/hadoop/lib/netty-3.10.5.Final.jar:/usr/lib/hadoop/lib/token-provider-1.0.1.jar:/usr/lib/hadoop/lib/avro-1.7.7.jar:/usr/lib/hadoop/lib/log4j-1.2.17.jar:/usr/lib/hadoop/lib/commons-math3-3.1.1.jar:/usr/lib/hadoop/lib/jaxb-api-2.2.11.jar:/usr/lib/hadoop/lib/jackson-annotations-2.10.0.jar:/usr/lib/hadoop/lib/jettison-1.1.jar:/usr/lib/hadoop/lib/kerb-simplekdc-1.0.1.jar:/usr/lib/hadoop/lib/jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop/lib/kerby-util-1.0.1.jar:/usr/lib/hadoop/lib/commons-lang3-3.7.jar:/usr/lib/hadoop/lib/commons-beanutils-1.9.3.jar:/usr/lib/hadoop/lib/kerby-xdr-1.0.1.jar:/usr/lib/hadoop/lib/jersey-server-1.19.jar:/usr/lib/hadoop/lib/commons-net-3.6.jar:/usr/lib/hadoop/lib/kerb-identity-1.0.1.jar:/usr/lib/hadoop/lib/jsch-0.1.54.jar:/usr/lib/hadoop/lib/woodstox-core-5.0.3.jar:/usr/lib/hadoop/lib/jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop/lib/jcip-annotations-1.0-1.jar:/usr/lib/hadoop/lib/kerb-core-1.0.1.jar:/usr/lib/hadoop/.//hadoop-fs2img-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-extras.jar:/usr/lib/hadoop/.//hadoop-common-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop/.//hadoop-archives.jar:/usr/lib/hadoop/.//hadoop-sls-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-resourceestimator.jar:/usr/lib/hadoop/.//hadoop-sls.jar:/usr/lib/hadoop/.//hadoop-aws-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-aliyun-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-nfs-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-azure.jar:/usr/lib/hadoop/.//hadoop-nfs.jar:/usr/lib/hadoop/.//hadoop-auth.jar:/usr/lib/hadoop/.//hadoop-azure-datalake.jar:/usr/lib/hadoop/.//hadoop-annotations-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-openstack.jar:/usr/lib/hadoop/.//hadoop-streaming-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-distcp.jar:/usr/lib/hadoop/.//hadoop-archive-logs-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-azure-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-gridmix.jar:/usr/lib/hadoop/.//hadoop-auth-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-datajoin-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-openstack-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-rumen-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-datajoin.jar:/usr/lib/hadoop/.//hadoop-streaming.jar:/usr/lib/hadoop/.//hadoop-azure-datalake-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-extras-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-archive-logs.jar:/usr/lib/hadoop/.//hadoop-gridmix-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-resourceestimator-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-annotations.jar:/usr/lib/hadoop/.//hadoop-common-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-rumen.jar:/usr/lib/hadoop/.//hadoop-kms.jar:/usr/lib/hadoop/.//hadoop-kafka.jar:/usr/lib/hadoop/.//hadoop-archives-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-aliyun.jar:/usr/lib/hadoop/.//hadoop-aws.jar:/usr/lib/hadoop/.//hadoop-fs2img.jar:/usr/lib/hadoop/.//hadoop-kafka-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-kms-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-common.jar:/usr/lib/hadoop/.//hadoop-distcp-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/./:/usr/lib/hadoop-hdfs/lib/jackson-xc-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/kerby-pkix-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/httpclient-4.5.9.jar:/usr/lib/hadoop-hdfs/lib/zookeeper-3.4.14.jar:/usr/lib/hadoop-hdfs/lib/jersey-core-1.19.jar:/usr/lib/hadoop-hdfs/lib/re2j-1.1.jar:/usr/lib/hadoop-hdfs/lib/kerby-config-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop-hdfs/lib/guava-11.0.2.jar:/usr/lib/hadoop-hdfs/lib/jackson-core-2.10.0.jar:/usr/lib/hadoop-hdfs/lib/commons-text-1.4.jar:/usr/lib/hadoop-hdfs/lib/kerb-common-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop-hdfs/lib/kerb-admin-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jetty-io-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/javax.servlet-api-3.1.0.jar:/usr/lib/hadoop-hdfs/lib/commons-collections-3.2.2.jar:/usr/lib/hadoop-hdfs/lib/kerb-util-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/paranamer-2.3.jar:/usr/lib/hadoop-hdfs/lib/httpcore-4.4.11.jar:/usr/lib/hadoop-hdfs/lib/jackson-databind-2.10.0.jar:/usr/lib/hadoop-hdfs/lib/jsr305-3.0.0.jar:/usr/lib/hadoop-hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/dnsjava-2.1.7.jar:/usr/lib/hadoop-hdfs/lib/curator-recipes-2.13.0.jar:/usr/lib/hadoop-hdfs/lib/stax2-api-3.1.4.jar:/usr/lib/hadoop-hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/kerb-client-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/commons-configuration2-2.1.1.jar:/usr/lib/hadoop-hdfs/lib/asm-5.0.4.jar:/usr/lib/hadoop-hdfs/lib/curator-client-2.13.0.jar:/usr/lib/hadoop-hdfs/lib/commons-cli-1.2.jar:/usr/lib/hadoop-hdfs/lib/gson-2.2.4.jar:/usr/lib/hadoop-hdfs/lib/nimbus-jose-jwt-4.41.1.jar:/usr/lib/hadoop-hdfs/lib/jetty-servlet-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/kerb-crypto-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jetty-webapp-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/jetty-http-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/curator-framework-2.13.0.jar:/usr/lib/hadoop-hdfs/lib/jersey-servlet-1.19.jar:/usr/lib/hadoop-hdfs/lib/kerby-asn1-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/snappy-java-1.1.7.3.jar:/usr/lib/hadoop-hdfs/lib/jsr311-api-1.1.1.jar:/usr/lib/hadoop-hdfs/lib/json-simple-1.1.1.jar:/usr/lib/hadoop-hdfs/lib/commons-codec-1.11.jar:/usr/lib/hadoop-hdfs/lib/jetty-xml-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/jetty-util-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop-hdfs/lib/json-smart-2.3.jar:/usr/lib/hadoop-hdfs/lib/commons-compress-1.18.jar:/usr/lib/hadoop-hdfs/lib/jetty-server-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/netty-all-4.0.52.Final.jar:/usr/lib/hadoop-hdfs/lib/accessors-smart-1.2.jar:/usr/lib/hadoop-hdfs/lib/jersey-json-1.19.jar:/usr/lib/hadoop-hdfs/lib/kerb-server-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/commons-io-2.5.jar:/usr/lib/hadoop-hdfs/lib/jetty-security-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/netty-3.10.5.Final.jar:/usr/lib/hadoop-hdfs/lib/token-provider-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/avro-1.7.7.jar:/usr/lib/hadoop-hdfs/lib/jetty-util-ajax-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/log4j-1.2.17.jar:/usr/lib/hadoop-hdfs/lib/commons-math3-3.1.1.jar:/usr/lib/hadoop-hdfs/lib/jaxb-api-2.2.11.jar:/usr/lib/hadoop-hdfs/lib/jackson-annotations-2.10.0.jar:/usr/lib/hadoop-hdfs/lib/commons-httpclient-3.1.jar:/usr/lib/hadoop-hdfs/lib/jettison-1.1.jar:/usr/lib/hadoop-hdfs/lib/kerb-simplekdc-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop-hdfs/lib/kerby-util-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/okhttp-2.7.5.jar:/usr/lib/hadoop-hdfs/lib/commons-lang3-3.7.jar:/usr/lib/hadoop-hdfs/lib/commons-beanutils-1.9.3.jar:/usr/lib/hadoop-hdfs/lib/commons-daemon-1.0.13.jar:/usr/lib/hadoop-hdfs/lib/kerby-xdr-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/okio-1.6.0.jar:/usr/lib/hadoop-hdfs/lib/jersey-server-1.19.jar:/usr/lib/hadoop-hdfs/lib/commons-net-3.6.jar:/usr/lib/hadoop-hdfs/lib/kerb-identity-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jsch-0.1.54.jar:/usr/lib/hadoop-hdfs/lib/leveldbjni-all-1.8.jar:/usr/lib/hadoop-hdfs/lib/woodstox-core-5.0.3.jar:/usr/lib/hadoop-hdfs/lib/jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/jcip-annotations-1.0-1.jar:/usr/lib/hadoop-hdfs/lib/kerb-core-1.0.1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-httpfs-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-nfs-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-httpfs.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-nfs.jar:/usr/lib/hadoop-mapreduce/.//azure-data-lake-store-sdk-2.2.9.jar:/usr/lib/hadoop-mapreduce/.//kafka-clients-2.4.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-fs2img-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-extras.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-core.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archives.jar:/usr/lib/hadoop-mapreduce/.//hadoop-sls-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-resourceestimator.jar:/usr/lib/hadoop-mapreduce/.//azure-storage-7.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-sls.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aws-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aliyun-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-common-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-datalake.jar:/usr/lib/hadoop-mapreduce/.//hadoop-openstack.jar:/usr/lib/hadoop-mapreduce/.//hadoop-streaming-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-nativetask.jar:/usr/lib/hadoop-mapreduce/.//hadoop-distcp.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archive-logs-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//ojalgo-43.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-plugins.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-gridmix.jar:/usr/lib/hadoop-mapreduce/.//hadoop-datajoin-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//aws-java-sdk-bundle-1.11.828.jar:/usr/lib/hadoop-mapreduce/.//hadoop-openstack-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-common.jar:/usr/lib/hadoop-mapreduce/.//zstd-jni-1.4.3-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-rumen-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-datajoin.jar:/usr/lib/hadoop-mapreduce/.//hadoop-streaming.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-datalake-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-core-3.4.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-app.jar:/usr/lib/hadoop-mapreduce/.//hadoop-extras-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//aliyun-sdk-oss-3.4.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archive-logs.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-ecs-4.2.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-examples.jar:/usr/lib/hadoop-mapreduce/.//hadoop-gridmix-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-resourceestimator-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-uploader.jar:/usr/lib/hadoop-mapreduce/.//wildfly-openssl-1.0.7.Final.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-uploader-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-app-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-rumen.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-core-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-examples-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient.jar:/usr/lib/hadoop-mapreduce/.//hadoop-kafka.jar:/usr/lib/hadoop-mapreduce/.//azure-keyvault-core-1.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archives-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-shuffle-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aliyun.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aws.jar:/usr/lib/hadoop-mapreduce/.//hadoop-fs2img.jar:/usr/lib/hadoop-mapreduce/.//hadoop-kafka-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//lz4-java-1.6.0.jar:/usr/lib/hadoop-mapreduce/.//jdom-1.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-shuffle.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-nativetask-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-sts-3.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-plugins-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-ram-3.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-distcp-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop-yarn/lib/jackson-module-jaxb-annotations-2.10.0.jar:/usr/lib/hadoop-yarn/lib/jackson-jaxrs-json-provider-2.10.0.jar:/usr/lib/hadoop-yarn/lib/jackson-jaxrs-base-2.10.0.jar:/usr/lib/hadoop-yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/usr/lib/hadoop-yarn/lib/javax.inject-1.jar:/usr/lib/hadoop-yarn/lib/bcprov-jdk15on-1.60.jar:/usr/lib/hadoop-yarn/lib/bcpkix-jdk15on-1.60.jar:/usr/lib/hadoop-yarn/lib/swagger-annotations-1.5.4.jar:/usr/lib/hadoop-yarn/lib/jersey-client-1.19.jar:/usr/lib/hadoop-yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/usr/lib/hadoop-yarn/lib/jakarta.xml.bind-api-2.3.2.jar:/usr/lib/hadoop-yarn/lib/guice-servlet-4.0.jar:/usr/lib/hadoop-yarn/lib/objenesis-1.0.jar:/usr/lib/hadoop-yarn/lib/jersey-guice-1.19.jar:/usr/lib/hadoop-yarn/lib/jakarta.activation-api-1.2.1.jar:/usr/lib/hadoop-yarn/lib/ehcache-3.3.1.jar:/usr/lib/hadoop-yarn/lib/metrics-core-3.2.4.jar:/usr/lib/hadoop-yarn/lib/aopalliance-1.0.jar:/usr/lib/hadoop-yarn/lib/guice-4.0.jar:/usr/lib/hadoop-yarn/lib/java-util-1.9.0.jar:/usr/lib/hadoop-yarn/lib/fst-2.50.jar:/usr/lib/hadoop-yarn/lib/snakeyaml-1.16.jar:/usr/lib/hadoop-yarn/lib/HikariCP-java7-2.4.12.jar:/usr/lib/hadoop-yarn/lib/json-io-2.5.1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-unmanaged-am-launcher-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-tests.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-client.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-resourcemanager-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-\u001b[0m\n",
      "\u001b[35myarn-server-applicationhistoryservice-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-unmanaged-am-launcher.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-registry.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-api.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-timeline-pluginstorage-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-nodemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-common.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-core-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-tests-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-registry-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-common-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-timeline-pluginstorage.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-client-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-submarine.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-sharedcachemanager-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-common-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-web-proxy-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-sharedcachemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-applicationhistoryservice.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-api-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-router-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-common.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-core.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-resourcemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-submarine-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-api-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-distributedshell.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-router.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-api.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-nodemanager-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-distributedshell-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-web-proxy.jar:/usr/lib/hadoop-yarn/.//timelineservice/hadoop-yarn-server-timelineservice-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//timelineservice/hadoop-yarn-server-timelineservice-hbase-common-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//timelineservice/hadoop-yarn-server-timelineservice-hbase-client-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//timelineservice/hadoop-yarn-server-timelineservice-hbase-coprocessor-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/commons-lang-2.6.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/commons-csv-1.0.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/jcodings-1.0.13.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/hbase-client-1.2.6.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/metrics-core-2.2.0.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/hbase-common-1.2.6.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/htrace-core-3.1.0-incubating.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/joni-2.1.2.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/hbase-annotations-1.2.6.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/hbase-protocol-1.2.6.jar\u001b[0m\n",
      "\u001b[35mSTARTUP_MSG:   build = git@aws157git.com:/pkg/Aws157BigTop -r 520b97ed429ab8eb7a8d30068b0f34a8036c51a7; compiled by 'ec2-user' on 2020-07-30T08:02Z\u001b[0m\n",
      "\u001b[35mSTARTUP_MSG:   java = 1.8.0_272\u001b[0m\n",
      "\u001b[35m************************************************************/\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:17,091 INFO resourcemanager.ResourceManager: registered UNIX signal handlers for [TERM, HUP, INT]\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:17,136 INFO namenode.NameNode: STARTUP_MSG: \u001b[0m\n",
      "\u001b[35m/************************************************************\u001b[0m\n",
      "\u001b[35mSTARTUP_MSG: Starting NameNode\u001b[0m\n",
      "\u001b[35mSTARTUP_MSG:   host = algo-1/10.0.189.145\u001b[0m\n",
      "\u001b[35mSTARTUP_MSG:   args = []\u001b[0m\n",
      "\u001b[35mSTARTUP_MSG:   version = 3.2.1-amzn-1\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   classpath = /usr/lib/hadoop/etc/hadoop:/usr/lib/hadoop/lib/jackson-xc-1.9.13.jar:/usr/lib/hadoop/lib/kerby-pkix-1.0.1.jar:/usr/lib/hadoop/lib/httpclient-4.5.9.jar:/usr/lib/hadoop/lib/zookeeper-3.4.14.jar:/usr/lib/hadoop/lib/jersey-core-1.19.jar:/usr/lib/hadoop/lib/re2j-1.1.jar:/usr/lib/hadoop/lib/jsp-api-2.1.jar:/usr/lib/hadoop/lib/kerby-config-1.0.1.jar:/usr/lib/hadoop/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop/lib/guava-11.0.2.jar:/usr/lib/hadoop/lib/jackson-core-2.10.0.jar:/usr/lib/hadoop/lib/commons-text-1.4.jar:/usr/lib/hadoop/lib/kerb-common-1.0.1.jar:/usr/lib/hadoop/lib/htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop/lib/kerb-admin-1.0.1.jar:/usr/lib/hadoop/lib/jetty-io-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/javax.servlet-api-3.1.0.jar:/usr/lib/hadoop/lib/commons-collections-3.2.2.jar:/usr/lib/hadoop/lib/kerb-util-1.0.1.jar:/usr/lib/hadoop/lib/paranamer-2.3.jar:/usr/lib/hadoop/lib/httpcore-4.4.11.jar:/usr/lib/hadoop/lib/jackson-databind-2.10.0.jar:/usr/lib/hadoop/lib/jsr305-3.0.0.jar:/usr/lib/hadoop/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop/lib/dnsjava-2.1.7.jar:/usr/lib/hadoop/lib/curator-recipes-2.13.0.jar:/usr/lib/hadoop/lib/stax2-api-3.1.4.jar:/usr/lib/hadoop/lib/slf4j-api-1.7.25.jar:/usr/lib/hadoop/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop/lib/kerb-client-1.0.1.jar:/usr/lib/hadoop/lib/commons-configuration2-2.1.1.jar:/usr/lib/hadoop/lib/asm-5.0.4.jar:/usr/lib/hadoop/lib/curator-client-2.13.0.jar:/usr/lib/hadoop/lib/commons-cli-1.2.jar:/usr/lib/hadoop/lib/gson-2.2.4.jar:/usr/lib/hadoop/lib/nimbus-jose-jwt-4.41.1.jar:/usr/lib/hadoop/lib/jetty-servlet-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/kerb-crypto-1.0.1.jar:/usr/lib/hadoop/lib/slf4j-log4j12-1.7.25.jar:/usr/lib/hadoop/lib/jetty-webapp-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/jetty-http-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/curator-framework-2.13.0.jar:/usr/lib/hadoop/lib/jersey-servlet-1.19.jar:/usr/lib/hadoop/lib/kerby-asn1-1.0.1.jar:/usr/lib/hadoop/lib/snappy-java-1.1.7.3.jar:/usr/lib/hadoop/lib/jsr311-api-1.1.1.jar:/usr/lib/hadoop/lib/commons-codec-1.11.jar:/usr/lib/hadoop/lib/jetty-xml-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/jetty-util-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop/lib/json-smart-2.3.jar:/usr/lib/hadoop/lib/commons-compress-1.18.jar:/usr/lib/hadoop/lib/jetty-server-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/accessors-smart-1.2.jar:/usr/lib/hadoop/lib/jersey-json-1.19.jar:/usr/lib/hadoop/lib/metrics-core-3.2.4.jar:/usr/lib/hadoop/lib/kerb-server-1.0.1.jar:/usr/lib/hadoop/lib/commons-io-2.5.jar:/usr/lib/hadoop/lib/jetty-security-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/jul-to-slf4j-1.7.25.jar:/usr/lib/hadoop/lib/netty-3.10.5.Final.jar:/usr/lib/hadoop/lib/token-provider-1.0.1.jar:/usr/lib/hadoop/lib/avro-1.7.7.jar:/usr/lib/hadoop/lib/log4j-1.2.17.jar:/usr/lib/hadoop/lib/commons-math3-3.1.1.jar:/usr/lib/hadoop/lib/jaxb-api-2.2.11.jar:/usr/lib/hadoop/lib/jackson-annotations-2.10.0.jar:/usr/lib/hadoop/lib/jettison-1.1.jar:/usr/lib/hadoop/lib/kerb-simplekdc-1.0.1.jar:/usr/lib/hadoop/lib/jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop/lib/kerby-util-1.0.1.jar:/usr/lib/hadoop/lib/commons-lang3-3.7.jar:/usr/lib/hadoop/lib/commons-beanutils-1.9.3.jar:/usr/lib/hadoop/lib/kerby-xdr-1.0.1.jar:/usr/lib/hadoop/lib/jersey-server-1.19.jar:/usr/lib/hadoop/lib/commons-net-3.6.jar:/usr/lib/hadoop/lib/kerb-identity-1.0.1.jar:/usr/lib/hadoop/lib/jsch-0.1.54.jar:/usr/lib/hadoop/lib/woodstox-core-5.0.3.jar:/usr/lib/hadoop/lib/jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop/lib/jcip-annotations-1.0-1.jar:/usr/lib/hadoop/lib/kerb-core-1.0.1.jar:/usr/lib/hadoop/.//hadoop-fs2img-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-extras.jar:/usr/lib/hadoop/.//hadoop-common-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop/.//hadoop-archives.jar:/usr/lib/hadoop/.//hadoop-sls-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-resourceestimator.jar:/usr/lib/hadoop/.//hadoop-sls.jar:/usr/lib/hadoop/.//hadoop-aws-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-aliyun-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-nfs-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-azure.jar:/usr/lib/hadoop/.//hadoop-nfs.jar:/usr/lib/hadoop/.//hadoop-auth.jar:/usr/lib/hadoop/.//hadoop-azure-datalake.jar:/usr/lib/hadoop/.//hadoop-annotations-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-openstack.jar:/usr/lib/hadoop/.//hadoop-streaming-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-distcp.jar:/usr/lib/hadoop/.//hadoop-archive-logs-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-azure-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-gridmix.jar:/usr/lib/hadoop/.//hadoop-auth-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-datajoin-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-openstack-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-rumen-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-datajoin.jar:/usr/lib/hadoop/.//hadoop-streaming.jar:/usr/lib/hadoop/.//hadoop-azure-datalake-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-extras-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-archive-logs.jar:/usr/lib/hadoop/.//hadoop-gridmix-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-resourceestimator-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-annotations.jar:/usr/lib/hadoop/.//hadoop-common-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-rumen.jar:/usr/lib/hadoop/.//hadoop-kms.jar:/usr/lib/hadoop/.//hadoop-kafka.jar:/usr/lib/hadoop/.//hadoop-archives-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-aliyun.jar:/usr/lib/hadoop/.//hadoop-aws.jar:/usr/lib/hadoop/.//hadoop-fs2img.jar:/usr/lib/hadoop/.//hadoop-kafka-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-kms-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-common.jar:/usr/lib/hadoop/.//hadoop-distcp-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/./:/usr/lib/hadoop-hdfs/lib/jackson-xc-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/kerby-pkix-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/httpclient-4.5.9.jar:/usr/lib/hadoop-hdfs/lib/zookeeper-3.4.14.jar:/usr/lib/hadoop-hdfs/lib/jersey-core-1.19.jar:/usr/lib/hadoop-hdfs/lib/re2j-1.1.jar:/usr/lib/hadoop-hdfs/lib/kerby-config-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop-hdfs/lib/guava-11.0.2.jar:/usr/lib/hadoop-hdfs/lib/jackson-core-2.10.0.jar:/usr/lib/hadoop-hdfs/lib/commons-text-1.4.jar:/usr/lib/hadoop-hdfs/lib/kerb-common-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop-hdfs/lib/kerb-admin-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jetty-io-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/javax.servlet-api-3.1.0.jar:/usr/lib/hadoop-hdfs/lib/commons-collections-3.2.2.jar:/usr/lib/hadoop-hdfs/lib/kerb-util-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/paranamer-2.3.jar:/usr/lib/hadoop-hdfs/lib/httpcore-4.4.11.jar:/usr/lib/hadoop-hdfs/lib/jackson-databind-2.10.0.jar:/usr/lib/hadoop-hdfs/lib/jsr305-3.0.0.jar:/usr/lib/hadoop-hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/dnsjava-2.1.7.jar:/usr/lib/hadoop-hdfs/lib/curator-recipes-2.13.0.jar:/usr/lib/hadoop-hdfs/lib/stax2-api-3.1.4.jar:/usr/lib/hadoop-hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/kerb-client-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/commons-configuration2-2.1.1.jar:/usr/lib/hadoop-hdfs/lib/asm-5.0.4.jar:/usr/lib/hadoop-hdfs/lib/curator-client-2.13.0.jar:/usr/lib/hadoop-hdfs/lib/commons-cli-1.2.jar:/usr/lib/hadoop-hdfs/lib/gson-2.2.4.jar:/usr/lib/hadoop-hdfs/lib/nimbus-jose-jwt-4.41.1.jar:/usr/lib/hadoop-hdfs/lib/jetty-servlet-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/kerb-crypto-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jetty-webapp-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/jetty-http-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/curator-framework-2.13.0.jar:/usr/lib/hadoop-hdfs/lib/jersey-servlet-1.19.jar:/usr/lib/hadoop-hdfs/lib/kerby-asn1-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/snappy-java-1.1.7.3.jar:/usr/lib/hadoop-hdfs/lib/jsr311-api-1.1.1.jar:/usr/lib/hadoop-hdfs/lib/json-simple-1.1.1.jar:/usr/lib/hadoop-hdfs/lib/commons-codec-1.11.jar:/usr/lib/hadoop-hdfs/lib/jetty-xml-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/jetty-util-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop-hdfs/lib/json-smart-2.3.jar:/usr/lib/hadoop-hdfs/lib/commons-compress-1.18.jar:/usr/lib/hadoop-hdfs/lib/jetty-server-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/netty-all-4.0.52.Final.jar:/usr/lib/hadoop-hdfs/lib/accessors-smart-1.2.jar:/usr/lib/hadoop-hdfs/lib/jersey-json-1.19.jar:/usr/lib/hadoop-hdfs/lib/kerb-server-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/commons-io-2.5.jar:/usr/lib/hadoop-hdfs/lib/jetty-security-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/netty-3.10.5.Final.jar:/usr/lib/hadoop-hdfs/lib/token-provider-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/avro-1.7.7.jar:/usr/lib/hadoop-hdfs/lib/jetty-util-ajax-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/log4j-1.2.17.jar:/usr/lib/hadoop-hdfs/lib/commons-math3-3.1.1.jar:/usr/lib/hadoop-hdfs/lib/jaxb-api-2.2.11.jar:/usr/lib/hadoop-hdfs/lib/jackson-annotations-2.10.0.jar:/usr/lib/hadoop-hdfs/lib/commons-httpclient-3.1.jar:/usr/lib/hadoop-hdfs/lib/jettison-1.1.jar:/usr/lib/hadoop-hdfs/lib/kerb-simplekdc-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop-hdfs/lib/kerby-util-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/okhttp-2.7.5.jar:/usr/lib/hadoop-hdfs/lib/commons-lang3-3.7.jar:/usr/lib/hadoop-hdfs/lib/commons-beanutils-1.9.3.jar:/usr/lib/hadoop-hdfs/lib/commons-daemon-1.0.13.jar:/usr/lib/hadoop-hdfs/lib/kerby-xdr-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/okio-1.6.0.jar:/usr/lib/hadoop-hdfs/lib/jersey-server-1.19.jar:/usr/lib/hadoop-hdfs/lib/commons-net-3.6.jar:/usr/lib/hadoop-hdfs/lib/kerb-identity-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jsch-0.1.54.jar:/usr/lib/hadoop-hdfs/lib/leveldbjni-all-1.8.jar:/usr/lib/hadoop-hdfs/lib/woodstox-core-5.0.3.jar:/usr/lib/hadoop-hdfs/lib/jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/jcip-annotations-1.0-1.jar:/usr/lib/hadoop-hdfs/lib/kerb-core-1.0.1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-httpfs-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-nfs-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-httpfs.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-nfs.jar:/usr/lib/hadoop-mapreduce/.//azure-data-lake-store-sdk-2.2.9.jar:/usr/lib/hadoop-mapreduce/.//kafka-clients-2.4.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-fs2img-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-extras.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-core.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archives.jar:/usr/lib/hadoop-mapreduce/.//hadoop-sls-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-resourceestimator.jar:/usr/lib/hadoop-mapreduce/.//azure-storage-7.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-sls.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aws-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aliyun-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-common-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-datalake.jar:/usr/lib/hadoop-mapreduce/.//hadoop-openstack.jar:/usr/lib/hadoop-mapreduce/.//hadoop-streaming-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-nativetask.jar:/usr/lib/hadoop-mapreduce/.//hadoop-distcp.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archive-logs-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//ojalgo-43.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-plugins.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-gridmix.jar:/usr/lib/hadoop-mapreduce/.//hadoop-datajoin-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//aws-java-sdk-bundle-1.11.828.jar:/usr/lib/hadoop-mapreduce/.//hadoop-openstack-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-common.jar:/usr/lib/hadoop-mapreduce/.//zstd-jni-1.4.3-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-rumen-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-datajoin.jar:/usr/lib/hadoop-mapreduce/.//hadoop-streaming.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-datalake-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-core-3.4.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-app.jar:/usr/lib/hadoop-mapreduce/.//hadoop-extras-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//aliyun-sdk-oss-3.4.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archive-logs.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-ecs-4.2.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-examples.jar:/usr/lib/hadoop-mapreduce/.//hadoop-gridmix-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-resourceestimator-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-uploader.jar:/usr/lib/hadoop-mapreduce/.//wildfly-openssl-1.0.7.Final.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-uploader-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-app-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-rumen.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-core-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-examples-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient.jar:/usr/lib/hadoop-mapreduce/.//hadoop-kafka.jar:/usr/lib/hadoop-mapreduce/.//azure-keyvault-core-1.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archives-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-shuffle-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aliyun.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aws.jar:/usr/lib/hadoop-mapreduce/.//hadoop-fs2img.jar:/usr/lib/hadoop-mapreduce/.//hadoop-kafka-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//lz4-java-1.6.0.jar:/usr/lib/hadoop-mapreduce/.//jdom-1.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-shuffle.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-nativetask-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-sts-3.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-plugins-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-ram-3.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-distcp-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop-yarn/lib/jackson-module-jaxb-annotations-2.10.0.jar:/usr/lib/hadoop-yarn/lib/jackson-jaxrs-json-provider-2.10.0.jar:/usr/lib/hadoop-yarn/lib/jackson-jaxrs-base-2.10.0.jar:/usr/lib/hadoop-yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/usr/lib/hadoop-yarn/lib/javax.inject-1.jar:/usr/lib/hadoop-yarn/lib/bcprov-jdk15on-1.60.jar:/usr/lib/hadoop-yarn/lib/bcpkix-jdk15on-1.60.jar:/usr/lib/hadoop-yarn/lib/swagger-annotations-1.5.4.jar:/usr/lib/hadoop-yarn/lib/jersey-client-1.19.jar:/usr/lib/hadoop-yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/usr/lib/hadoop-yarn/lib/jakarta.xml.bind-api-2.3.2.jar:/usr/lib/hadoop-yarn/lib/guice-servlet-4.0.jar:/usr/lib/hadoop-yarn/lib/objenesis-1.0.jar:/usr/lib/hadoop-yarn/lib/jersey-guice-1.19.jar:/usr/lib/hadoop-yarn/lib/jakarta.activation-api-1.2.1.jar:/usr/lib/hadoop-yarn/lib/ehcache-3.3.1.jar:/usr/lib/hadoop-yarn/lib/metrics-core-3.2.4.jar:/usr/lib/hadoop-yarn/lib/aopalliance-1.0.jar:/usr/lib/hadoop-yarn/lib/guice-4.0.jar:/usr/lib/hadoop-yarn/lib/java-util-1.9.0.jar:/usr/lib/hadoop-yarn/lib/fst-2.50.jar:/usr/lib/hadoop-yarn/lib/snakeyaml-1.16.jar:/usr/lib/hadoop-yarn/lib/HikariCP-java7-2.4.12.jar:/usr/lib/hadoop-yarn/lib/json-io-2.5.1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-unmanaged-am-launcher-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-tests.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-client.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-resourcemanager-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-\u001b[0m\n",
      "\u001b[34myarn-server-applicationhistoryservice-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-unmanaged-am-launcher.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-registry.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-api.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-timeline-pluginstorage-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-nodemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-common.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-core-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-tests-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-registry-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-common-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-timeline-pluginstorage.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-client-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-submarine.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-sharedcachemanager-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-common-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-web-proxy-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-sharedcachemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-applicationhistoryservice.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-api-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-router-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-common.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-core.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-resourcemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-submarine-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-api-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-distributedshell.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-router.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-api.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-nodemanager-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-distributedshell-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-web-proxy.jar\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   build = git@aws157git.com:/pkg/Aws157BigTop -r 520b97ed429ab8eb7a8d30068b0f34a8036c51a7; compiled by 'ec2-user' on 2020-07-30T08:02Z\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   java = 1.8.0_272\u001b[0m\n",
      "\u001b[34m************************************************************/\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:17,140 INFO datanode.DataNode: STARTUP_MSG: \u001b[0m\n",
      "\u001b[34m/************************************************************\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG: Starting DataNode\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   host = algo-1/10.0.189.145\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   args = []\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   version = 3.2.1-amzn-1\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   classpath = /usr/lib/hadoop/etc/hadoop:/usr/lib/hadoop/lib/jackson-xc-1.9.13.jar:/usr/lib/hadoop/lib/kerby-pkix-1.0.1.jar:/usr/lib/hadoop/lib/httpclient-4.5.9.jar:/usr/lib/hadoop/lib/zookeeper-3.4.14.jar:/usr/lib/hadoop/lib/jersey-core-1.19.jar:/usr/lib/hadoop/lib/re2j-1.1.jar:/usr/lib/hadoop/lib/jsp-api-2.1.jar:/usr/lib/hadoop/lib/kerby-config-1.0.1.jar:/usr/lib/hadoop/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop/lib/guava-11.0.2.jar:/usr/lib/hadoop/lib/jackson-core-2.10.0.jar:/usr/lib/hadoop/lib/commons-text-1.4.jar:/usr/lib/hadoop/lib/kerb-common-1.0.1.jar:/usr/lib/hadoop/lib/htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop/lib/kerb-admin-1.0.1.jar:/usr/lib/hadoop/lib/jetty-io-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/javax.servlet-api-3.1.0.jar:/usr/lib/hadoop/lib/commons-collections-3.2.2.jar:/usr/lib/hadoop/lib/kerb-util-1.0.1.jar:/usr/lib/hadoop/lib/paranamer-2.3.jar:/usr/lib/hadoop/lib/httpcore-4.4.11.jar:/usr/lib/hadoop/lib/jackson-databind-2.10.0.jar:/usr/lib/hadoop/lib/jsr305-3.0.0.jar:/usr/lib/hadoop/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop/lib/dnsjava-2.1.7.jar:/usr/lib/hadoop/lib/curator-recipes-2.13.0.jar:/usr/lib/hadoop/lib/stax2-api-3.1.4.jar:/usr/lib/hadoop/lib/slf4j-api-1.7.25.jar:/usr/lib/hadoop/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop/lib/kerb-client-1.0.1.jar:/usr/lib/hadoop/lib/commons-configuration2-2.1.1.jar:/usr/lib/hadoop/lib/asm-5.0.4.jar:/usr/lib/hadoop/lib/curator-client-2.13.0.jar:/usr/lib/hadoop/lib/commons-cli-1.2.jar:/usr/lib/hadoop/lib/gson-2.2.4.jar:/usr/lib/hadoop/lib/nimbus-jose-jwt-4.41.1.jar:/usr/lib/hadoop/lib/jetty-servlet-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/kerb-crypto-1.0.1.jar:/usr/lib/hadoop/lib/slf4j-log4j12-1.7.25.jar:/usr/lib/hadoop/lib/jetty-webapp-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/jetty-http-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/curator-framework-2.13.0.jar:/usr/lib/hadoop/lib/jersey-servlet-1.19.jar:/usr/lib/hadoop/lib/kerby-asn1-1.0.1.jar:/usr/lib/hadoop/lib/snappy-java-1.1.7.3.jar:/usr/lib/hadoop/lib/jsr311-api-1.1.1.jar:/usr/lib/hadoop/lib/commons-codec-1.11.jar:/usr/lib/hadoop/lib/jetty-xml-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/jetty-util-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop/lib/json-smart-2.3.jar:/usr/lib/hadoop/lib/commons-compress-1.18.jar:/usr/lib/hadoop/lib/jetty-server-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/accessors-smart-1.2.jar:/usr/lib/hadoop/lib/jersey-json-1.19.jar:/usr/lib/hadoop/lib/metrics-core-3.2.4.jar:/usr/lib/hadoop/lib/kerb-server-1.0.1.jar:/usr/lib/hadoop/lib/commons-io-2.5.jar:/usr/lib/hadoop/lib/jetty-security-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/jul-to-slf4j-1.7.25.jar:/usr/lib/hadoop/lib/netty-3.10.5.Final.jar:/usr/lib/hadoop/lib/token-provider-1.0.1.jar:/usr/lib/hadoop/lib/avro-1.7.7.jar:/usr/lib/hadoop/lib/log4j-1.2.17.jar:/usr/lib/hadoop/lib/commons-math3-3.1.1.jar:/usr/lib/hadoop/lib/jaxb-api-2.2.11.jar:/usr/lib/hadoop/lib/jackson-annotations-2.10.0.jar:/usr/lib/hadoop/lib/jettison-1.1.jar:/usr/lib/hadoop/lib/kerb-simplekdc-1.0.1.jar:/usr/lib/hadoop/lib/jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop/lib/kerby-util-1.0.1.jar:/usr/lib/hadoop/lib/commons-lang3-3.7.jar:/usr/lib/hadoop/lib/commons-beanutils-1.9.3.jar:/usr/lib/hadoop/lib/kerby-xdr-1.0.1.jar:/usr/lib/hadoop/lib/jersey-server-1.19.jar:/usr/lib/hadoop/lib/commons-net-3.6.jar:/usr/lib/hadoop/lib/kerb-identity-1.0.1.jar:/usr/lib/hadoop/lib/jsch-0.1.54.jar:/usr/lib/hadoop/lib/woodstox-core-5.0.3.jar:/usr/lib/hadoop/lib/jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop/lib/jcip-annotations-1.0-1.jar:/usr/lib/hadoop/lib/kerb-core-1.0.1.jar:/usr/lib/hadoop/.//hadoop-fs2img-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-extras.jar:/usr/lib/hadoop/.//hadoop-common-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop/.//hadoop-archives.jar:/usr/lib/hadoop/.//hadoop-sls-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-resourceestimator.jar:/usr/lib/hadoop/.//hadoop-sls.jar:/usr/lib/hadoop/.//hadoop-aws-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-aliyun-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-nfs-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-azure.jar:/usr/lib/hadoop/.//hadoop-nfs.jar:/usr/lib/hadoop/.//hadoop-auth.jar:/usr/lib/hadoop/.//hadoop-azure-datalake.jar:/usr/lib/hadoop/.//hadoop-annotations-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-openstack.jar:/usr/lib/hadoop/.//hadoop-streaming-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-distcp.jar:/usr/lib/hadoop/.//hadoop-archive-logs-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-azure-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-gridmix.jar:/usr/lib/hadoop/.//hadoop-auth-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-datajoin-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-openstack-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-rumen-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-datajoin.jar:/usr/lib/hadoop/.//hadoop-streaming.jar:/usr/lib/hadoop/.//hadoop-azure-datalake-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-extras-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-archive-logs.jar:/usr/lib/hadoop/.//hadoop-gridmix-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-resourceestimator-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-annotations.jar:/usr/lib/hadoop/.//hadoop-common-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-rumen.jar:/usr/lib/hadoop/.//hadoop-kms.jar:/usr/lib/hadoop/.//hadoop-kafka.jar:/usr/lib/hadoop/.//hadoop-archives-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-aliyun.jar:/usr/lib/hadoop/.//hadoop-aws.jar:/usr/lib/hadoop/.//hadoop-fs2img.jar:/usr/lib/hadoop/.//hadoop-kafka-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-kms-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-common.jar:/usr/lib/hadoop/.//hadoop-distcp-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/./:/usr/lib/hadoop-hdfs/lib/jackson-xc-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/kerby-pkix-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/httpclient-4.5.9.jar:/usr/lib/hadoop-hdfs/lib/zookeeper-3.4.14.jar:/usr/lib/hadoop-hdfs/lib/jersey-core-1.19.jar:/usr/lib/hadoop-hdfs/lib/re2j-1.1.jar:/usr/lib/hadoop-hdfs/lib/kerby-config-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop-hdfs/lib/guava-11.0.2.jar:/usr/lib/hadoop-hdfs/lib/jackson-core-2.10.0.jar:/usr/lib/hadoop-hdfs/lib/commons-text-1.4.jar:/usr/lib/hadoop-hdfs/lib/kerb-common-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop-hdfs/lib/kerb-admin-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jetty-io-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/javax.servlet-api-3.1.0.jar:/usr/lib/hadoop-hdfs/lib/commons-collections-3.2.2.jar:/usr/lib/hadoop-hdfs/lib/kerb-util-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/paranamer-2.3.jar:/usr/lib/hadoop-hdfs/lib/httpcore-4.4.11.jar:/usr/lib/hadoop-hdfs/lib/jackson-databind-2.10.0.jar:/usr/lib/hadoop-hdfs/lib/jsr305-3.0.0.jar:/usr/lib/hadoop-hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/dnsjava-2.1.7.jar:/usr/lib/hadoop-hdfs/lib/curator-recipes-2.13.0.jar:/usr/lib/hadoop-hdfs/lib/stax2-api-3.1.4.jar:/usr/lib/hadoop-hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/kerb-client-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/commons-configuration2-2.1.1.jar:/usr/lib/hadoop-hdfs/lib/asm-5.0.4.jar:/usr/lib/hadoop-hdfs/lib/curator-client-2.13.0.jar:/usr/lib/hadoop-hdfs/lib/commons-cli-1.2.jar:/usr/lib/hadoop-hdfs/lib/gson-2.2.4.jar:/usr/lib/hadoop-hdfs/lib/nimbus-jose-jwt-4.41.1.jar:/usr/lib/hadoop-hdfs/lib/jetty-servlet-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/kerb-crypto-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jetty-webapp-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/jetty-http-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/curator-framework-2.13.0.jar:/usr/lib/hadoop-hdfs/lib/jersey-servlet-1.19.jar:/usr/lib/hadoop-hdfs/lib/kerby-asn1-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/snappy-java-1.1.7.3.jar:/usr/lib/hadoop-hdfs/lib/jsr311-api-1.1.1.jar:/usr/lib/hadoop-hdfs/lib/json-simple-1.1.1.jar:/usr/lib/hadoop-hdfs/lib/commons-codec-1.11.jar:/usr/lib/hadoop-hdfs/lib/jetty-xml-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/jetty-util-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop-hdfs/lib/json-smart-2.3.jar:/usr/lib/hadoop-hdfs/lib/commons-compress-1.18.jar:/usr/lib/hadoop-hdfs/lib/jetty-server-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/netty-all-4.0.52.Final.jar:/usr/lib/hadoop-hdfs/lib/accessors-smart-1.2.jar:/usr/lib/hadoop-hdfs/lib/jersey-json-1.19.jar:/usr/lib/hadoop-hdfs/lib/kerb-server-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/commons-io-2.5.jar:/usr/lib/hadoop-hdfs/lib/jetty-security-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/netty-3.10.5.Final.jar:/usr/lib/hadoop-hdfs/lib/token-provider-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/avro-1.7.7.jar:/usr/lib/hadoop-hdfs/lib/jetty-util-ajax-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/log4j-1.2.17.jar:/usr/lib/hadoop-hdfs/lib/commons-math3-3.1.1.jar:/usr/lib/hadoop-hdfs/lib/jaxb-api-2.2.11.jar:/usr/lib/hadoop-hdfs/lib/jackson-annotations-2.10.0.jar:/usr/lib/hadoop-hdfs/lib/commons-httpclient-3.1.jar:/usr/lib/hadoop-hdfs/lib/jettison-1.1.jar:/usr/lib/hadoop-hdfs/lib/kerb-simplekdc-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop-hdfs/lib/kerby-util-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/okhttp-2.7.5.jar:/usr/lib/hadoop-hdfs/lib/commons-lang3-3.7.jar:/usr/lib/hadoop-hdfs/lib/commons-beanutils-1.9.3.jar:/usr/lib/hadoop-hdfs/lib/commons-daemon-1.0.13.jar:/usr/lib/hadoop-hdfs/lib/kerby-xdr-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/okio-1.6.0.jar:/usr/lib/hadoop-hdfs/lib/jersey-server-1.19.jar:/usr/lib/hadoop-hdfs/lib/commons-net-3.6.jar:/usr/lib/hadoop-hdfs/lib/kerb-identity-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jsch-0.1.54.jar:/usr/lib/hadoop-hdfs/lib/leveldbjni-all-1.8.jar:/usr/lib/hadoop-hdfs/lib/woodstox-core-5.0.3.jar:/usr/lib/hadoop-hdfs/lib/jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/jcip-annotations-1.0-1.jar:/usr/lib/hadoop-hdfs/lib/kerb-core-1.0.1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-httpfs-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-nfs-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-httpfs.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-nfs.jar:/usr/lib/hadoop-mapreduce/.//azure-data-lake-store-sdk-2.2.9.jar:/usr/lib/hadoop-mapreduce/.//kafka-clients-2.4.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-fs2img-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-extras.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-core.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archives.jar:/usr/lib/hadoop-mapreduce/.//hadoop-sls-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-resourceestimator.jar:/usr/lib/hadoop-mapreduce/.//azure-storage-7.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-sls.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aws-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aliyun-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-common-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-datalake.jar:/usr/lib/hadoop-mapreduce/.//hadoop-openstack.jar:/usr/lib/hadoop-mapreduce/.//hadoop-streaming-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-nativetask.jar:/usr/lib/hadoop-mapreduce/.//hadoop-distcp.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archive-logs-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//ojalgo-43.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-plugins.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-gridmix.jar:/usr/lib/hadoop-mapreduce/.//hadoop-datajoin-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//aws-java-sdk-bundle-1.11.828.jar:/usr/lib/hadoop-mapreduce/.//hadoop-openstack-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-common.jar:/usr/lib/hadoop-mapreduce/.//zstd-jni-1.4.3-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-rumen-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-datajoin.jar:/usr/lib/hadoop-mapreduce/.//hadoop-streaming.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-datalake-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-core-3.4.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-app.jar:/usr/lib/hadoop-mapreduce/.//hadoop-extras-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//aliyun-sdk-oss-3.4.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archive-logs.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-ecs-4.2.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-examples.jar:/usr/lib/hadoop-mapreduce/.//hadoop-gridmix-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-resourceestimator-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-uploader.jar:/usr/lib/hadoop-mapreduce/.//wildfly-openssl-1.0.7.Final.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-uploader-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-app-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-rumen.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-core-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-examples-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient.jar:/usr/lib/hadoop-mapreduce/.//hadoop-kafka.jar:/usr/lib/hadoop-mapreduce/.//azure-keyvault-core-1.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archives-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-shuffle-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aliyun.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aws.jar:/usr/lib/hadoop-mapreduce/.//hadoop-fs2img.jar:/usr/lib/hadoop-mapreduce/.//hadoop-kafka-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//lz4-java-1.6.0.jar:/usr/lib/hadoop-mapreduce/.//jdom-1.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-shuffle.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-nativetask-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-sts-3.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-plugins-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-ram-3.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-distcp-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop-yarn/lib/jackson-module-jaxb-annotations-2.10.0.jar:/usr/lib/hadoop-yarn/lib/jackson-jaxrs-json-provider-2.10.0.jar:/usr/lib/hadoop-yarn/lib/jackson-jaxrs-base-2.10.0.jar:/usr/lib/hadoop-yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/usr/lib/hadoop-yarn/lib/javax.inject-1.jar:/usr/lib/hadoop-yarn/lib/bcprov-jdk15on-1.60.jar:/usr/lib/hadoop-yarn/lib/bcpkix-jdk15on-1.60.jar:/usr/lib/hadoop-yarn/lib/swagger-annotations-1.5.4.jar:/usr/lib/hadoop-yarn/lib/jersey-client-1.19.jar:/usr/lib/hadoop-yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/usr/lib/hadoop-yarn/lib/jakarta.xml.bind-api-2.3.2.jar:/usr/lib/hadoop-yarn/lib/guice-servlet-4.0.jar:/usr/lib/hadoop-yarn/lib/objenesis-1.0.jar:/usr/lib/hadoop-yarn/lib/jersey-guice-1.19.jar:/usr/lib/hadoop-yarn/lib/jakarta.activation-api-1.2.1.jar:/usr/lib/hadoop-yarn/lib/ehcache-3.3.1.jar:/usr/lib/hadoop-yarn/lib/metrics-core-3.2.4.jar:/usr/lib/hadoop-yarn/lib/aopalliance-1.0.jar:/usr/lib/hadoop-yarn/lib/guice-4.0.jar:/usr/lib/hadoop-yarn/lib/java-util-1.9.0.jar:/usr/lib/hadoop-yarn/lib/fst-2.50.jar:/usr/lib/hadoop-yarn/lib/snakeyaml-1.16.jar:/usr/lib/hadoop-yarn/lib/HikariCP-java7-2.4.12.jar:/usr/lib/hadoop-yarn/lib/json-io-2.5.1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-unmanaged-am-launcher-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-tests.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-client.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-resourcemanager-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-\u001b[0m\n",
      "\u001b[34myarn-server-applicationhistoryservice-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-unmanaged-am-launcher.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-registry.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-api.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-timeline-pluginstorage-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-nodemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-common.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-core-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-tests-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-registry-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-common-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-timeline-pluginstorage.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-client-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-submarine.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-sharedcachemanager-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-common-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-web-proxy-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-sharedcachemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-applicationhistoryservice.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-api-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-router-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-common.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-core.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-resourcemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-submarine-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-api-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-distributedshell.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-router.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-api.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-nodemanager-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-distributedshell-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-web-proxy.jar\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   build = git@aws157git.com:/pkg/Aws157BigTop -r 520b97ed429ab8eb7a8d30068b0f34a8036c51a7; compiled by 'ec2-user' on 2020-07-30T08:02Z\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   java = 1.8.0_272\u001b[0m\n",
      "\u001b[34m************************************************************/\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:17,146 INFO namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:17,148 INFO datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:17,233 INFO namenode.NameNode: createNameNode []\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:17,349 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:17,416 INFO conf.Configuration: found resource core-site.xml at file:/etc/hadoop/conf.empty/core-site.xml\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:17,424 INFO resourceplugin.ResourcePluginManager: No Resource plugins found from configuration!\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:17,424 INFO resourceplugin.ResourcePluginManager: Found Resource plugins from configuration: null\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:17,435 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:17,435 INFO impl.MetricsSystemImpl: NameNode metrics system started\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:17,453 INFO conf.Configuration: resource-types.xml not found\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:17,454 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:17,458 INFO namenode.NameNodeUtils: fs.defaultFS is hdfs://10.0.189.145/\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:17,482 INFO nodemanager.NodeManager: Node Manager health check script is not available or doesn't have execute permission, so not starting the node health script runner.\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:17,489 INFO conf.Configuration: found resource yarn-site.xml at file:/etc/hadoop/conf.empty/yarn-site.xml\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:17,501 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.RMFatalEventType for class org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$RMFatalEventDispatcher\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:17,510 INFO checker.ThrottledAsyncChecker: Scheduling a check for [DISK]file:/opt/amazon/hadoop/hdfs/datanode\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:17,533 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl$ContainerEventDispatcher\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:17,534 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl$ApplicationEventDispatcher\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:17,535 INFO security.NMTokenSecretManagerInRM: NMTokenKeyRollingInterval: 86400000ms and NMTokenKeyActivationDelay: 900000ms\u001b[0m\n",
      "\u001b[35mSTARTUP_MSG:   classpath = /usr/lib/hadoop/etc/hadoop:/usr/lib/hadoop/lib/jackson-xc-1.9.13.jar:/usr/lib/hadoop/lib/kerby-pkix-1.0.1.jar:/usr/lib/hadoop/lib/httpclient-4.5.9.jar:/usr/lib/hadoop/lib/zookeeper-3.4.14.jar:/usr/lib/hadoop/lib/jersey-core-1.19.jar:/usr/lib/hadoop/lib/re2j-1.1.jar:/usr/lib/hadoop/lib/jsp-api-2.1.jar:/usr/lib/hadoop/lib/kerby-config-1.0.1.jar:/usr/lib/hadoop/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop/lib/guava-11.0.2.jar:/usr/lib/hadoop/lib/jackson-core-2.10.0.jar:/usr/lib/hadoop/lib/commons-text-1.4.jar:/usr/lib/hadoop/lib/kerb-common-1.0.1.jar:/usr/lib/hadoop/lib/htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop/lib/kerb-admin-1.0.1.jar:/usr/lib/hadoop/lib/jetty-io-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/javax.servlet-api-3.1.0.jar:/usr/lib/hadoop/lib/commons-collections-3.2.2.jar:/usr/lib/hadoop/lib/kerb-util-1.0.1.jar:/usr/lib/hadoop/lib/paranamer-2.3.jar:/usr/lib/hadoop/lib/httpcore-4.4.11.jar:/usr/lib/hadoop/lib/jackson-databind-2.10.0.jar:/usr/lib/hadoop/lib/jsr305-3.0.0.jar:/usr/lib/hadoop/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop/lib/dnsjava-2.1.7.jar:/usr/lib/hadoop/lib/curator-recipes-2.13.0.jar:/usr/lib/hadoop/lib/stax2-api-3.1.4.jar:/usr/lib/hadoop/lib/slf4j-api-1.7.25.jar:/usr/lib/hadoop/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop/lib/kerb-client-1.0.1.jar:/usr/lib/hadoop/lib/commons-configuration2-2.1.1.jar:/usr/lib/hadoop/lib/asm-5.0.4.jar:/usr/lib/hadoop/lib/curator-client-2.13.0.jar:/usr/lib/hadoop/lib/commons-cli-1.2.jar:/usr/lib/hadoop/lib/gson-2.2.4.jar:/usr/lib/hadoop/lib/nimbus-jose-jwt-4.41.1.jar:/usr/lib/hadoop/lib/jetty-servlet-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/kerb-crypto-1.0.1.jar:/usr/lib/hadoop/lib/slf4j-log4j12-1.7.25.jar:/usr/lib/hadoop/lib/jetty-webapp-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/jetty-http-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/curator-framework-2.13.0.jar:/usr/lib/hadoop/lib/jersey-servlet-1.19.jar:/usr/lib/hadoop/lib/kerby-asn1-1.0.1.jar:/usr/lib/hadoop/lib/snappy-java-1.1.7.3.jar:/usr/lib/hadoop/lib/jsr311-api-1.1.1.jar:/usr/lib/hadoop/lib/commons-codec-1.11.jar:/usr/lib/hadoop/lib/jetty-xml-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/jetty-util-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop/lib/json-smart-2.3.jar:/usr/lib/hadoop/lib/commons-compress-1.18.jar:/usr/lib/hadoop/lib/jetty-server-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/accessors-smart-1.2.jar:/usr/lib/hadoop/lib/jersey-json-1.19.jar:/usr/lib/hadoop/lib/metrics-core-3.2.4.jar:/usr/lib/hadoop/lib/kerb-server-1.0.1.jar:/usr/lib/hadoop/lib/commons-io-2.5.jar:/usr/lib/hadoop/lib/jetty-security-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/jul-to-slf4j-1.7.25.jar:/usr/lib/hadoop/lib/netty-3.10.5.Final.jar:/usr/lib/hadoop/lib/token-provider-1.0.1.jar:/usr/lib/hadoop/lib/avro-1.7.7.jar:/usr/lib/hadoop/lib/log4j-1.2.17.jar:/usr/lib/hadoop/lib/commons-math3-3.1.1.jar:/usr/lib/hadoop/lib/jaxb-api-2.2.11.jar:/usr/lib/hadoop/lib/jackson-annotations-2.10.0.jar:/usr/lib/hadoop/lib/jettison-1.1.jar:/usr/lib/hadoop/lib/kerb-simplekdc-1.0.1.jar:/usr/lib/hadoop/lib/jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop/lib/kerby-util-1.0.1.jar:/usr/lib/hadoop/lib/commons-lang3-3.7.jar:/usr/lib/hadoop/lib/commons-beanutils-1.9.3.jar:/usr/lib/hadoop/lib/kerby-xdr-1.0.1.jar:/usr/lib/hadoop/lib/jersey-server-1.19.jar:/usr/lib/hadoop/lib/commons-net-3.6.jar:/usr/lib/hadoop/lib/kerb-identity-1.0.1.jar:/usr/lib/hadoop/lib/jsch-0.1.54.jar:/usr/lib/hadoop/lib/woodstox-core-5.0.3.jar:/usr/lib/hadoop/lib/jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop/lib/jcip-annotations-1.0-1.jar:/usr/lib/hadoop/lib/kerb-core-1.0.1.jar:/usr/lib/hadoop/.//hadoop-fs2img-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-extras.jar:/usr/lib/hadoop/.//hadoop-common-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop/.//hadoop-archives.jar:/usr/lib/hadoop/.//hadoop-sls-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-resourceestimator.jar:/usr/lib/hadoop/.//hadoop-sls.jar:/usr/lib/hadoop/.//hadoop-aws-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-aliyun-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-nfs-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-azure.jar:/usr/lib/hadoop/.//hadoop-nfs.jar:/usr/lib/hadoop/.//hadoop-auth.jar:/usr/lib/hadoop/.//hadoop-azure-datalake.jar:/usr/lib/hadoop/.//hadoop-annotations-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-openstack.jar:/usr/lib/hadoop/.//hadoop-streaming-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-distcp.jar:/usr/lib/hadoop/.//hadoop-archive-logs-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-azure-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-gridmix.jar:/usr/lib/hadoop/.//hadoop-auth-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-datajoin-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-openstack-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-rumen-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-datajoin.jar:/usr/lib/hadoop/.//hadoop-streaming.jar:/usr/lib/hadoop/.//hadoop-azure-datalake-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-extras-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-archive-logs.jar:/usr/lib/hadoop/.//hadoop-gridmix-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-resourceestimator-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-annotations.jar:/usr/lib/hadoop/.//hadoop-common-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-rumen.jar:/usr/lib/hadoop/.//hadoop-kms.jar:/usr/lib/hadoop/.//hadoop-kafka.jar:/usr/lib/hadoop/.//hadoop-archives-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-aliyun.jar:/usr/lib/hadoop/.//hadoop-aws.jar:/usr/lib/hadoop/.//hadoop-fs2img.jar:/usr/lib/hadoop/.//hadoop-kafka-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-kms-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-common.jar:/usr/lib/hadoop/.//hadoop-distcp-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/./:/usr/lib/hadoop-hdfs/lib/jackson-xc-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/kerby-pkix-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/httpclient-4.5.9.jar:/usr/lib/hadoop-hdfs/lib/zookeeper-3.4.14.jar:/usr/lib/hadoop-hdfs/lib/jersey-core-1.19.jar:/usr/lib/hadoop-hdfs/lib/re2j-1.1.jar:/usr/lib/hadoop-hdfs/lib/kerby-config-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop-hdfs/lib/guava-11.0.2.jar:/usr/lib/hadoop-hdfs/lib/jackson-core-2.10.0.jar:/usr/lib/hadoop-hdfs/lib/commons-text-1.4.jar:/usr/lib/hadoop-hdfs/lib/kerb-common-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop-hdfs/lib/kerb-admin-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jetty-io-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/javax.servlet-api-3.1.0.jar:/usr/lib/hadoop-hdfs/lib/commons-collections-3.2.2.jar:/usr/lib/hadoop-hdfs/lib/kerb-util-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/paranamer-2.3.jar:/usr/lib/hadoop-hdfs/lib/httpcore-4.4.11.jar:/usr/lib/hadoop-hdfs/lib/jackson-databind-2.10.0.jar:/usr/lib/hadoop-hdfs/lib/jsr305-3.0.0.jar:/usr/lib/hadoop-hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/dnsjava-2.1.7.jar:/usr/lib/hadoop-hdfs/lib/curator-recipes-2.13.0.jar:/usr/lib/hadoop-hdfs/lib/stax2-api-3.1.4.jar:/usr/lib/hadoop-hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/kerb-client-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/commons-configuration2-2.1.1.jar:/usr/lib/hadoop-hdfs/lib/asm-5.0.4.jar:/usr/lib/hadoop-hdfs/lib/curator-client-2.13.0.jar:/usr/lib/hadoop-hdfs/lib/commons-cli-1.2.jar:/usr/lib/hadoop-hdfs/lib/gson-2.2.4.jar:/usr/lib/hadoop-hdfs/lib/nimbus-jose-jwt-4.41.1.jar:/usr/lib/hadoop-hdfs/lib/jetty-servlet-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/kerb-crypto-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jetty-webapp-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/jetty-http-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/curator-framework-2.13.0.jar:/usr/lib/hadoop-hdfs/lib/jersey-servlet-1.19.jar:/usr/lib/hadoop-hdfs/lib/kerby-asn1-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/snappy-java-1.1.7.3.jar:/usr/lib/hadoop-hdfs/lib/jsr311-api-1.1.1.jar:/usr/lib/hadoop-hdfs/lib/json-simple-1.1.1.jar:/usr/lib/hadoop-hdfs/lib/commons-codec-1.11.jar:/usr/lib/hadoop-hdfs/lib/jetty-xml-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/jetty-util-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop-hdfs/lib/json-smart-2.3.jar:/usr/lib/hadoop-hdfs/lib/commons-compress-1.18.jar:/usr/lib/hadoop-hdfs/lib/jetty-server-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/netty-all-4.0.52.Final.jar:/usr/lib/hadoop-hdfs/lib/accessors-smart-1.2.jar:/usr/lib/hadoop-hdfs/lib/jersey-json-1.19.jar:/usr/lib/hadoop-hdfs/lib/kerb-server-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/commons-io-2.5.jar:/usr/lib/hadoop-hdfs/lib/jetty-security-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/netty-3.10.5.Final.jar:/usr/lib/hadoop-hdfs/lib/token-provider-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/avro-1.7.7.jar:/usr/lib/hadoop-hdfs/lib/jetty-util-ajax-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/log4j-1.2.17.jar:/usr/lib/hadoop-hdfs/lib/commons-math3-3.1.1.jar:/usr/lib/hadoop-hdfs/lib/jaxb-api-2.2.11.jar:/usr/lib/hadoop-hdfs/lib/jackson-annotations-2.10.0.jar:/usr/lib/hadoop-hdfs/lib/commons-httpclient-3.1.jar:/usr/lib/hadoop-hdfs/lib/jettison-1.1.jar:/usr/lib/hadoop-hdfs/lib/kerb-simplekdc-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop-hdfs/lib/kerby-util-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/okhttp-2.7.5.jar:/usr/lib/hadoop-hdfs/lib/commons-lang3-3.7.jar:/usr/lib/hadoop-hdfs/lib/commons-beanutils-1.9.3.jar:/usr/lib/hadoop-hdfs/lib/commons-daemon-1.0.13.jar:/usr/lib/hadoop-hdfs/lib/kerby-xdr-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/okio-1.6.0.jar:/usr/lib/hadoop-hdfs/lib/jersey-server-1.19.jar:/usr/lib/hadoop-hdfs/lib/commons-net-3.6.jar:/usr/lib/hadoop-hdfs/lib/kerb-identity-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jsch-0.1.54.jar:/usr/lib/hadoop-hdfs/lib/leveldbjni-all-1.8.jar:/usr/lib/hadoop-hdfs/lib/woodstox-core-5.0.3.jar:/usr/lib/hadoop-hdfs/lib/jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/jcip-annotations-1.0-1.jar:/usr/lib/hadoop-hdfs/lib/kerb-core-1.0.1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-httpfs-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-nfs-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-httpfs.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-nfs.jar:/usr/lib/hadoop-mapreduce/.//azure-data-lake-store-sdk-2.2.9.jar:/usr/lib/hadoop-mapreduce/.//kafka-clients-2.4.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-fs2img-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-extras.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-core.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archives.jar:/usr/lib/hadoop-mapreduce/.//hadoop-sls-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-resourceestimator.jar:/usr/lib/hadoop-mapreduce/.//azure-storage-7.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-sls.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aws-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aliyun-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-common-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-datalake.jar:/usr/lib/hadoop-mapreduce/.//hadoop-openstack.jar:/usr/lib/hadoop-mapreduce/.//hadoop-streaming-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-nativetask.jar:/usr/lib/hadoop-mapreduce/.//hadoop-distcp.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archive-logs-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//ojalgo-43.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-plugins.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-gridmix.jar:/usr/lib/hadoop-mapreduce/.//hadoop-datajoin-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//aws-java-sdk-bundle-1.11.828.jar:/usr/lib/hadoop-mapreduce/.//hadoop-openstack-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-common.jar:/usr/lib/hadoop-mapreduce/.//zstd-jni-1.4.3-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-rumen-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-datajoin.jar:/usr/lib/hadoop-mapreduce/.//hadoop-streaming.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-datalake-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-core-3.4.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-app.jar:/usr/lib/hadoop-mapreduce/.//hadoop-extras-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//aliyun-sdk-oss-3.4.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archive-logs.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-ecs-4.2.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-examples.jar:/usr/lib/hadoop-mapreduce/.//hadoop-gridmix-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-resourceestimator-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-uploader.jar:/usr/lib/hadoop-mapreduce/.//wildfly-openssl-1.0.7.Final.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-uploader-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-app-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-rumen.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-core-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-examples-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient.jar:/usr/lib/hadoop-mapreduce/.//hadoop-kafka.jar:/usr/lib/hadoop-mapreduce/.//azure-keyvault-core-1.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archives-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-shuffle-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aliyun.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aws.jar:/usr/lib/hadoop-mapreduce/.//hadoop-fs2img.jar:/usr/lib/hadoop-mapreduce/.//hadoop-kafka-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//lz4-java-1.6.0.jar:/usr/lib/hadoop-mapreduce/.//jdom-1.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-shuffle.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-nativetask-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-sts-3.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-plugins-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-ram-3.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-distcp-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop-yarn/lib/jackson-module-jaxb-annotations-2.10.0.jar:/usr/lib/hadoop-yarn/lib/jackson-jaxrs-json-provider-2.10.0.jar:/usr/lib/hadoop-yarn/lib/jackson-jaxrs-base-2.10.0.jar:/usr/lib/hadoop-yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/usr/lib/hadoop-yarn/lib/javax.inject-1.jar:/usr/lib/hadoop-yarn/lib/bcprov-jdk15on-1.60.jar:/usr/lib/hadoop-yarn/lib/bcpkix-jdk15on-1.60.jar:/usr/lib/hadoop-yarn/lib/swagger-annotations-1.5.4.jar:/usr/lib/hadoop-yarn/lib/jersey-client-1.19.jar:/usr/lib/hadoop-yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/usr/lib/hadoop-yarn/lib/jakarta.xml.bind-api-2.3.2.jar:/usr/lib/hadoop-yarn/lib/guice-servlet-4.0.jar:/usr/lib/hadoop-yarn/lib/objenesis-1.0.jar:/usr/lib/hadoop-yarn/lib/jersey-guice-1.19.jar:/usr/lib/hadoop-yarn/lib/jakarta.activation-api-1.2.1.jar:/usr/lib/hadoop-yarn/lib/ehcache-3.3.1.jar:/usr/lib/hadoop-yarn/lib/metrics-core-3.2.4.jar:/usr/lib/hadoop-yarn/lib/aopalliance-1.0.jar:/usr/lib/hadoop-yarn/lib/guice-4.0.jar:/usr/lib/hadoop-yarn/lib/java-util-1.9.0.jar:/usr/lib/hadoop-yarn/lib/fst-2.50.jar:/usr/lib/hadoop-yarn/lib/snakeyaml-1.16.jar:/usr/lib/hadoop-yarn/lib/HikariCP-java7-2.4.12.jar:/usr/lib/hadoop-yarn/lib/json-io-2.5.1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-unmanaged-am-launcher-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-tests.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-client.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-resourcemanager-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-\u001b[0m\n",
      "\u001b[35myarn-server-applicationhistoryservice-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-unmanaged-am-launcher.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-registry.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-api.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-timeline-pluginstorage-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-nodemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-common.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-core-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-tests-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-registry-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-common-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-timeline-pluginstorage.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-client-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-submarine.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-sharedcachemanager-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-common-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-web-proxy-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-sharedcachemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-applicationhistoryservice.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-api-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-router-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-common.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-core.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-resourcemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-submarine-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-api-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-distributedshell.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-router.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-api.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-nodemanager-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-distributedshell-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-web-proxy.jar\u001b[0m\n",
      "\u001b[35mSTARTUP_MSG:   build = git@aws157git.com:/pkg/Aws157BigTop -r 520b97ed429ab8eb7a8d30068b0f34a8036c51a7; compiled by 'ec2-user' on 2020-07-30T08:02Z\u001b[0m\n",
      "\u001b[35mSTARTUP_MSG:   java = 1.8.0_272\u001b[0m\n",
      "\u001b[35m************************************************************/\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:17,140 INFO datanode.DataNode: STARTUP_MSG: \u001b[0m\n",
      "\u001b[35m/************************************************************\u001b[0m\n",
      "\u001b[35mSTARTUP_MSG: Starting DataNode\u001b[0m\n",
      "\u001b[35mSTARTUP_MSG:   host = algo-1/10.0.189.145\u001b[0m\n",
      "\u001b[35mSTARTUP_MSG:   args = []\u001b[0m\n",
      "\u001b[35mSTARTUP_MSG:   version = 3.2.1-amzn-1\u001b[0m\n",
      "\u001b[35mSTARTUP_MSG:   classpath = /usr/lib/hadoop/etc/hadoop:/usr/lib/hadoop/lib/jackson-xc-1.9.13.jar:/usr/lib/hadoop/lib/kerby-pkix-1.0.1.jar:/usr/lib/hadoop/lib/httpclient-4.5.9.jar:/usr/lib/hadoop/lib/zookeeper-3.4.14.jar:/usr/lib/hadoop/lib/jersey-core-1.19.jar:/usr/lib/hadoop/lib/re2j-1.1.jar:/usr/lib/hadoop/lib/jsp-api-2.1.jar:/usr/lib/hadoop/lib/kerby-config-1.0.1.jar:/usr/lib/hadoop/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop/lib/guava-11.0.2.jar:/usr/lib/hadoop/lib/jackson-core-2.10.0.jar:/usr/lib/hadoop/lib/commons-text-1.4.jar:/usr/lib/hadoop/lib/kerb-common-1.0.1.jar:/usr/lib/hadoop/lib/htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop/lib/kerb-admin-1.0.1.jar:/usr/lib/hadoop/lib/jetty-io-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/javax.servlet-api-3.1.0.jar:/usr/lib/hadoop/lib/commons-collections-3.2.2.jar:/usr/lib/hadoop/lib/kerb-util-1.0.1.jar:/usr/lib/hadoop/lib/paranamer-2.3.jar:/usr/lib/hadoop/lib/httpcore-4.4.11.jar:/usr/lib/hadoop/lib/jackson-databind-2.10.0.jar:/usr/lib/hadoop/lib/jsr305-3.0.0.jar:/usr/lib/hadoop/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop/lib/dnsjava-2.1.7.jar:/usr/lib/hadoop/lib/curator-recipes-2.13.0.jar:/usr/lib/hadoop/lib/stax2-api-3.1.4.jar:/usr/lib/hadoop/lib/slf4j-api-1.7.25.jar:/usr/lib/hadoop/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop/lib/kerb-client-1.0.1.jar:/usr/lib/hadoop/lib/commons-configuration2-2.1.1.jar:/usr/lib/hadoop/lib/asm-5.0.4.jar:/usr/lib/hadoop/lib/curator-client-2.13.0.jar:/usr/lib/hadoop/lib/commons-cli-1.2.jar:/usr/lib/hadoop/lib/gson-2.2.4.jar:/usr/lib/hadoop/lib/nimbus-jose-jwt-4.41.1.jar:/usr/lib/hadoop/lib/jetty-servlet-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/kerb-crypto-1.0.1.jar:/usr/lib/hadoop/lib/slf4j-log4j12-1.7.25.jar:/usr/lib/hadoop/lib/jetty-webapp-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/jetty-http-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/curator-framework-2.13.0.jar:/usr/lib/hadoop/lib/jersey-servlet-1.19.jar:/usr/lib/hadoop/lib/kerby-asn1-1.0.1.jar:/usr/lib/hadoop/lib/snappy-java-1.1.7.3.jar:/usr/lib/hadoop/lib/jsr311-api-1.1.1.jar:/usr/lib/hadoop/lib/commons-codec-1.11.jar:/usr/lib/hadoop/lib/jetty-xml-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/jetty-util-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop/lib/json-smart-2.3.jar:/usr/lib/hadoop/lib/commons-compress-1.18.jar:/usr/lib/hadoop/lib/jetty-server-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/accessors-smart-1.2.jar:/usr/lib/hadoop/lib/jersey-json-1.19.jar:/usr/lib/hadoop/lib/metrics-core-3.2.4.jar:/usr/lib/hadoop/lib/kerb-server-1.0.1.jar:/usr/lib/hadoop/lib/commons-io-2.5.jar:/usr/lib/hadoop/lib/jetty-security-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/jul-to-slf4j-1.7.25.jar:/usr/lib/hadoop/lib/netty-3.10.5.Final.jar:/usr/lib/hadoop/lib/token-provider-1.0.1.jar:/usr/lib/hadoop/lib/avro-1.7.7.jar:/usr/lib/hadoop/lib/log4j-1.2.17.jar:/usr/lib/hadoop/lib/commons-math3-3.1.1.jar:/usr/lib/hadoop/lib/jaxb-api-2.2.11.jar:/usr/lib/hadoop/lib/jackson-annotations-2.10.0.jar:/usr/lib/hadoop/lib/jettison-1.1.jar:/usr/lib/hadoop/lib/kerb-simplekdc-1.0.1.jar:/usr/lib/hadoop/lib/jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop/lib/kerby-util-1.0.1.jar:/usr/lib/hadoop/lib/commons-lang3-3.7.jar:/usr/lib/hadoop/lib/commons-beanutils-1.9.3.jar:/usr/lib/hadoop/lib/kerby-xdr-1.0.1.jar:/usr/lib/hadoop/lib/jersey-server-1.19.jar:/usr/lib/hadoop/lib/commons-net-3.6.jar:/usr/lib/hadoop/lib/kerb-identity-1.0.1.jar:/usr/lib/hadoop/lib/jsch-0.1.54.jar:/usr/lib/hadoop/lib/woodstox-core-5.0.3.jar:/usr/lib/hadoop/lib/jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop/lib/jcip-annotations-1.0-1.jar:/usr/lib/hadoop/lib/kerb-core-1.0.1.jar:/usr/lib/hadoop/.//hadoop-fs2img-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-extras.jar:/usr/lib/hadoop/.//hadoop-common-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop/.//hadoop-archives.jar:/usr/lib/hadoop/.//hadoop-sls-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-resourceestimator.jar:/usr/lib/hadoop/.//hadoop-sls.jar:/usr/lib/hadoop/.//hadoop-aws-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-aliyun-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-nfs-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-azure.jar:/usr/lib/hadoop/.//hadoop-nfs.jar:/usr/lib/hadoop/.//hadoop-auth.jar:/usr/lib/hadoop/.//hadoop-azure-datalake.jar:/usr/lib/hadoop/.//hadoop-annotations-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-openstack.jar:/usr/lib/hadoop/.//hadoop-streaming-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-distcp.jar:/usr/lib/hadoop/.//hadoop-archive-logs-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-azure-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-gridmix.jar:/usr/lib/hadoop/.//hadoop-auth-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-datajoin-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-openstack-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-rumen-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-datajoin.jar:/usr/lib/hadoop/.//hadoop-streaming.jar:/usr/lib/hadoop/.//hadoop-azure-datalake-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-extras-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-archive-logs.jar:/usr/lib/hadoop/.//hadoop-gridmix-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-resourceestimator-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-annotations.jar:/usr/lib/hadoop/.//hadoop-common-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-rumen.jar:/usr/lib/hadoop/.//hadoop-kms.jar:/usr/lib/hadoop/.//hadoop-kafka.jar:/usr/lib/hadoop/.//hadoop-archives-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-aliyun.jar:/usr/lib/hadoop/.//hadoop-aws.jar:/usr/lib/hadoop/.//hadoop-fs2img.jar:/usr/lib/hadoop/.//hadoop-kafka-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-kms-3.2.1-amzn-1.jar:/usr/lib/hadoop/.//hadoop-common.jar:/usr/lib/hadoop/.//hadoop-distcp-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/./:/usr/lib/hadoop-hdfs/lib/jackson-xc-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/kerby-pkix-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/httpclient-4.5.9.jar:/usr/lib/hadoop-hdfs/lib/zookeeper-3.4.14.jar:/usr/lib/hadoop-hdfs/lib/jersey-core-1.19.jar:/usr/lib/hadoop-hdfs/lib/re2j-1.1.jar:/usr/lib/hadoop-hdfs/lib/kerby-config-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop-hdfs/lib/guava-11.0.2.jar:/usr/lib/hadoop-hdfs/lib/jackson-core-2.10.0.jar:/usr/lib/hadoop-hdfs/lib/commons-text-1.4.jar:/usr/lib/hadoop-hdfs/lib/kerb-common-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop-hdfs/lib/kerb-admin-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jetty-io-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/javax.servlet-api-3.1.0.jar:/usr/lib/hadoop-hdfs/lib/commons-collections-3.2.2.jar:/usr/lib/hadoop-hdfs/lib/kerb-util-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/paranamer-2.3.jar:/usr/lib/hadoop-hdfs/lib/httpcore-4.4.11.jar:/usr/lib/hadoop-hdfs/lib/jackson-databind-2.10.0.jar:/usr/lib/hadoop-hdfs/lib/jsr305-3.0.0.jar:/usr/lib/hadoop-hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/dnsjava-2.1.7.jar:/usr/lib/hadoop-hdfs/lib/curator-recipes-2.13.0.jar:/usr/lib/hadoop-hdfs/lib/stax2-api-3.1.4.jar:/usr/lib/hadoop-hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/kerb-client-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/commons-configuration2-2.1.1.jar:/usr/lib/hadoop-hdfs/lib/asm-5.0.4.jar:/usr/lib/hadoop-hdfs/lib/curator-client-2.13.0.jar:/usr/lib/hadoop-hdfs/lib/commons-cli-1.2.jar:/usr/lib/hadoop-hdfs/lib/gson-2.2.4.jar:/usr/lib/hadoop-hdfs/lib/nimbus-jose-jwt-4.41.1.jar:/usr/lib/hadoop-hdfs/lib/jetty-servlet-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/kerb-crypto-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jetty-webapp-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/jetty-http-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/curator-framework-2.13.0.jar:/usr/lib/hadoop-hdfs/lib/jersey-servlet-1.19.jar:/usr/lib/hadoop-hdfs/lib/kerby-asn1-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/snappy-java-1.1.7.3.jar:/usr/lib/hadoop-hdfs/lib/jsr311-api-1.1.1.jar:/usr/lib/hadoop-hdfs/lib/json-simple-1.1.1.jar:/usr/lib/hadoop-hdfs/lib/commons-codec-1.11.jar:/usr/lib/hadoop-hdfs/lib/jetty-xml-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/jetty-util-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop-hdfs/lib/json-smart-2.3.jar:/usr/lib/hadoop-hdfs/lib/commons-compress-1.18.jar:/usr/lib/hadoop-hdfs/lib/jetty-server-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/netty-all-4.0.52.Final.jar:/usr/lib/hadoop-hdfs/lib/accessors-smart-1.2.jar:/usr/lib/hadoop-hdfs/lib/jersey-json-1.19.jar:/usr/lib/hadoop-hdfs/lib/kerb-server-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/commons-io-2.5.jar:/usr/lib/hadoop-hdfs/lib/jetty-security-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/netty-3.10.5.Final.jar:/usr/lib/hadoop-hdfs/lib/token-provider-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/avro-1.7.7.jar:/usr/lib/hadoop-hdfs/lib/jetty-util-ajax-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/log4j-1.2.17.jar:/usr/lib/hadoop-hdfs/lib/commons-math3-3.1.1.jar:/usr/lib/hadoop-hdfs/lib/jaxb-api-2.2.11.jar:/usr/lib/hadoop-hdfs/lib/jackson-annotations-2.10.0.jar:/usr/lib/hadoop-hdfs/lib/commons-httpclient-3.1.jar:/usr/lib/hadoop-hdfs/lib/jettison-1.1.jar:/usr/lib/hadoop-hdfs/lib/kerb-simplekdc-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop-hdfs/lib/kerby-util-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/okhttp-2.7.5.jar:/usr/lib/hadoop-hdfs/lib/commons-lang3-3.7.jar:/usr/lib/hadoop-hdfs/lib/commons-beanutils-1.9.3.jar:/usr/lib/hadoop-hdfs/lib/commons-daemon-1.0.13.jar:/usr/lib/hadoop-hdfs/lib/kerby-xdr-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/okio-1.6.0.jar:/usr/lib/hadoop-hdfs/lib/jersey-server-1.19.jar:/usr/lib/hadoop-hdfs/lib/commons-net-3.6.jar:/usr/lib/hadoop-hdfs/lib/kerb-identity-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jsch-0.1.54.jar:/usr/lib/hadoop-hdfs/lib/leveldbjni-all-1.8.jar:/usr/lib/hadoop-hdfs/lib/woodstox-core-5.0.3.jar:/usr/lib/hadoop-hdfs/lib/jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/jcip-annotations-1.0-1.jar:/usr/lib/hadoop-hdfs/lib/kerb-core-1.0.1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-httpfs-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-nfs-3.2.1-amzn-1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-httpfs.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-nfs.jar:/usr/lib/hadoop-mapreduce/.//azure-data-lake-store-sdk-2.2.9.jar:/usr/lib/hadoop-mapreduce/.//kafka-clients-2.4.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-fs2img-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-extras.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-core.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archives.jar:/usr/lib/hadoop-mapreduce/.//hadoop-sls-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-resourceestimator.jar:/usr/lib/hadoop-mapreduce/.//azure-storage-7.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-sls.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aws-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aliyun-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-common-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-datalake.jar:/usr/lib/hadoop-mapreduce/.//hadoop-openstack.jar:/usr/lib/hadoop-mapreduce/.//hadoop-streaming-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-nativetask.jar:/usr/lib/hadoop-mapreduce/.//hadoop-distcp.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archive-logs-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//ojalgo-43.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-plugins.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-gridmix.jar:/usr/lib/hadoop-mapreduce/.//hadoop-datajoin-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//aws-java-sdk-bundle-1.11.828.jar:/usr/lib/hadoop-mapreduce/.//hadoop-openstack-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-common.jar:/usr/lib/hadoop-mapreduce/.//zstd-jni-1.4.3-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-rumen-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-datajoin.jar:/usr/lib/hadoop-mapreduce/.//hadoop-streaming.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-datalake-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-core-3.4.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-app.jar:/usr/lib/hadoop-mapreduce/.//hadoop-extras-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//aliyun-sdk-oss-3.4.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archive-logs.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-ecs-4.2.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-examples.jar:/usr/lib/hadoop-mapreduce/.//hadoop-gridmix-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-resourceestimator-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-uploader.jar:/usr/lib/hadoop-mapreduce/.//wildfly-openssl-1.0.7.Final.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-uploader-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-app-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-rumen.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-core-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-examples-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient.jar:/usr/lib/hadoop-mapreduce/.//hadoop-kafka.jar:/usr/lib/hadoop-mapreduce/.//azure-keyvault-core-1.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archives-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-shuffle-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aliyun.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aws.jar:/usr/lib/hadoop-mapreduce/.//hadoop-fs2img.jar:/usr/lib/hadoop-mapreduce/.//hadoop-kafka-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//lz4-java-1.6.0.jar:/usr/lib/hadoop-mapreduce/.//jdom-1.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-shuffle.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-nativetask-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-sts-3.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-plugins-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-ram-3.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-distcp-3.2.1-amzn-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient-3.2.1-amzn-1-tests.jar:/usr/lib/hadoop-yarn/lib/jackson-module-jaxb-annotations-2.10.0.jar:/usr/lib/hadoop-yarn/lib/jackson-jaxrs-json-provider-2.10.0.jar:/usr/lib/hadoop-yarn/lib/jackson-jaxrs-base-2.10.0.jar:/usr/lib/hadoop-yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/usr/lib/hadoop-yarn/lib/javax.inject-1.jar:/usr/lib/hadoop-yarn/lib/bcprov-jdk15on-1.60.jar:/usr/lib/hadoop-yarn/lib/bcpkix-jdk15on-1.60.jar:/usr/lib/hadoop-yarn/lib/swagger-annotations-1.5.4.jar:/usr/lib/hadoop-yarn/lib/jersey-client-1.19.jar:/usr/lib/hadoop-yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/usr/lib/hadoop-yarn/lib/jakarta.xml.bind-api-2.3.2.jar:/usr/lib/hadoop-yarn/lib/guice-servlet-4.0.jar:/usr/lib/hadoop-yarn/lib/objenesis-1.0.jar:/usr/lib/hadoop-yarn/lib/jersey-guice-1.19.jar:/usr/lib/hadoop-yarn/lib/jakarta.activation-api-1.2.1.jar:/usr/lib/hadoop-yarn/lib/ehcache-3.3.1.jar:/usr/lib/hadoop-yarn/lib/metrics-core-3.2.4.jar:/usr/lib/hadoop-yarn/lib/aopalliance-1.0.jar:/usr/lib/hadoop-yarn/lib/guice-4.0.jar:/usr/lib/hadoop-yarn/lib/java-util-1.9.0.jar:/usr/lib/hadoop-yarn/lib/fst-2.50.jar:/usr/lib/hadoop-yarn/lib/snakeyaml-1.16.jar:/usr/lib/hadoop-yarn/lib/HikariCP-java7-2.4.12.jar:/usr/lib/hadoop-yarn/lib/json-io-2.5.1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-unmanaged-am-launcher-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-tests.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-client.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-resourcemanager-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-\u001b[0m\n",
      "\u001b[35myarn-server-applicationhistoryservice-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-unmanaged-am-launcher.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-registry.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-api.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-timeline-pluginstorage-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-nodemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-common.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-core-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-tests-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-registry-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-common-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-timeline-pluginstorage.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-client-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-submarine.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-sharedcachemanager-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-common-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-web-proxy-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-sharedcachemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-applicationhistoryservice.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-api-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-router-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-common.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-core.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-resourcemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-submarine-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-api-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-distributedshell.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-router.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-api.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-nodemanager-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-distributedshell-3.2.1-amzn-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-web-proxy.jar\u001b[0m\n",
      "\u001b[35mSTARTUP_MSG:   build = git@aws157git.com:/pkg/Aws157BigTop -r 520b97ed429ab8eb7a8d30068b0f34a8036c51a7; compiled by 'ec2-user' on 2020-07-30T08:02Z\u001b[0m\n",
      "\u001b[35mSTARTUP_MSG:   java = 1.8.0_272\u001b[0m\n",
      "\u001b[35m************************************************************/\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:17,146 INFO namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:17,148 INFO datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:17,233 INFO namenode.NameNode: createNameNode []\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:17,349 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:17,416 INFO conf.Configuration: found resource core-site.xml at file:/etc/hadoop/conf.empty/core-site.xml\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:17,424 INFO resourceplugin.ResourcePluginManager: No Resource plugins found from configuration!\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:17,424 INFO resourceplugin.ResourcePluginManager: Found Resource plugins from configuration: null\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:17,435 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:17,435 INFO impl.MetricsSystemImpl: NameNode metrics system started\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:17,453 INFO conf.Configuration: resource-types.xml not found\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:17,454 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:17,458 INFO namenode.NameNodeUtils: fs.defaultFS is hdfs://10.0.189.145/\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:17,482 INFO nodemanager.NodeManager: Node Manager health check script is not available or doesn't have execute permission, so not starting the node health script runner.\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:17,489 INFO conf.Configuration: found resource yarn-site.xml at file:/etc/hadoop/conf.empty/yarn-site.xml\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:17,501 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.RMFatalEventType for class org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$RMFatalEventDispatcher\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:17,510 INFO checker.ThrottledAsyncChecker: Scheduling a check for [DISK]file:/opt/amazon/hadoop/hdfs/datanode\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:17,533 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl$ContainerEventDispatcher\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:17,534 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl$ApplicationEventDispatcher\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:17,535 INFO security.NMTokenSecretManagerInRM: NMTokenKeyRollingInterval: 86400000ms and NMTokenKeyActivationDelay: 900000ms\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:17,535 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.event.LocalizationEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl$LocalizationEventHandlerWrapper\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:17,536 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.AuxServicesEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.AuxServices\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:17,536 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:17,537 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainersLauncherEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainersLauncher\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:17,537 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.scheduler.ContainerSchedulerEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.scheduler.ContainerScheduler\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:17,538 INFO security.RMContainerTokenSecretManager: ContainerTokenKeyRollingInterval: 86400000ms and ContainerTokenKeyActivationDelay: 900000ms\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:17,539 INFO tracker.NMLogAggregationStatusTracker: the rolling interval seconds for the NodeManager Cached Log aggregation status is 600\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:17,542 INFO security.AMRMTokenSecretManager: AMRMTokenKeyRollingInterval: 86400000ms and AMRMTokenKeyActivationDelay: 900000 ms\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:17,554 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.ContainerManagerEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:17,554 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.NodeManagerEventType for class org.apache.hadoop.yarn.server.nodemanager.NodeManager\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:17,579 INFO util.JvmPauseMonitor: Starting JVM pause monitor\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:17,580 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStoreEventType for class org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore$ForwardingEventHandler\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:17,584 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.NodesListManagerEventType for class org.apache.hadoop.yarn.server.resourcemanager.NodesListManager\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:17,584 INFO resourcemanager.ResourceManager: Using Scheduler: org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:17,587 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:17,602 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:17,603 INFO hdfs.DFSUtil: Starting Web-server for hdfs at: http://0.0.0.0:9870\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:17,616 INFO util.log: Logging initialized @952ms to org.eclipse.jetty.util.log.Slf4jLog\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:17,617 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.scheduler.event.SchedulerEventType for class org.apache.hadoop.yarn.event.EventDispatcher\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:17,618 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppEventType for class org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$ApplicationEventDispatcher\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:17,619 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptEventType for class org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$ApplicationAttemptEventDispatcher\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:17,620 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNodeEventType for class org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$NodeEventDispatcher\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:17,651 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:17,651 INFO impl.MetricsSystemImpl: DataNode metrics system started\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:17,662 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:17,662 INFO impl.MetricsSystemImpl: NodeManager metrics system started\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:17,674 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:17,678 INFO nodemanager.DirectoryCollection: Disk Validator 'basic' is loaded.\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:17,687 INFO nodemanager.DirectoryCollection: Disk Validator 'basic' is loaded.\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:17,535 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.event.LocalizationEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl$LocalizationEventHandlerWrapper\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:17,536 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.AuxServicesEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.AuxServices\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:17,536 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:17,537 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainersLauncherEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainersLauncher\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:17,537 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.scheduler.ContainerSchedulerEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.scheduler.ContainerScheduler\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:17,538 INFO security.RMContainerTokenSecretManager: ContainerTokenKeyRollingInterval: 86400000ms and ContainerTokenKeyActivationDelay: 900000ms\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:17,539 INFO tracker.NMLogAggregationStatusTracker: the rolling interval seconds for the NodeManager Cached Log aggregation status is 600\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:17,542 INFO security.AMRMTokenSecretManager: AMRMTokenKeyRollingInterval: 86400000ms and AMRMTokenKeyActivationDelay: 900000 ms\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:17,554 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.ContainerManagerEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:17,554 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.NodeManagerEventType for class org.apache.hadoop.yarn.server.nodemanager.NodeManager\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:17,579 INFO util.JvmPauseMonitor: Starting JVM pause monitor\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:17,580 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStoreEventType for class org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore$ForwardingEventHandler\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:17,584 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.NodesListManagerEventType for class org.apache.hadoop.yarn.server.resourcemanager.NodesListManager\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:17,584 INFO resourcemanager.ResourceManager: Using Scheduler: org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:17,587 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:17,602 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:17,603 INFO hdfs.DFSUtil: Starting Web-server for hdfs at: http://0.0.0.0:9870\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:17,616 INFO util.log: Logging initialized @952ms to org.eclipse.jetty.util.log.Slf4jLog\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:17,617 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.scheduler.event.SchedulerEventType for class org.apache.hadoop.yarn.event.EventDispatcher\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:17,618 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppEventType for class org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$ApplicationEventDispatcher\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:17,619 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptEventType for class org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$ApplicationAttemptEventDispatcher\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:17,620 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNodeEventType for class org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$NodeEventDispatcher\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:17,651 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:17,651 INFO impl.MetricsSystemImpl: DataNode metrics system started\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:17,662 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:17,662 INFO impl.MetricsSystemImpl: NodeManager metrics system started\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:17,674 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:17,678 INFO nodemanager.DirectoryCollection: Disk Validator 'basic' is loaded.\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:17,687 INFO nodemanager.DirectoryCollection: Disk Validator 'basic' is loaded.\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:17,709 INFO server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:17,717 INFO nodemanager.NodeResourceMonitorImpl:  Using ResourceCalculatorPlugin : org.apache.hadoop.yarn.util.ResourceCalculatorPlugin@15043a2f\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:17,719 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.loghandler.event.LogHandlerEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.loghandler.NonAggregatingLogHandler\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:17,720 INFO http.HttpRequestLog: Http request log for http.requests.namenode is not defined\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:17,721 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.sharedcache.SharedCacheUploadEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.sharedcache.SharedCacheUploadService\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:17,709 INFO server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:17,717 INFO nodemanager.NodeResourceMonitorImpl:  Using ResourceCalculatorPlugin : org.apache.hadoop.yarn.util.ResourceCalculatorPlugin@15043a2f\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:17,719 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.loghandler.event.LogHandlerEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.loghandler.NonAggregatingLogHandler\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:17,720 INFO http.HttpRequestLog: Http request log for http.requests.namenode is not defined\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:17,721 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.sharedcache.SharedCacheUploadEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.sharedcache.SharedCacheUploadService\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:17,721 INFO containermanager.ContainerManagerImpl: AMRMProxyService is disabled\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:17,721 INFO localizer.ResourceLocalizationService: per directory file limit = 8192\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:17,726 INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:17,728 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hdfs\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:17,728 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:17,728 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:17,746 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:17,746 INFO impl.MetricsSystemImpl: ResourceManager metrics system started\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:17,747 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.event.LocalizerEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService$LocalizerTracker\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:17,752 INFO resources.ResourceHandlerModule: Using traffic control bandwidth handler\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:17,755 INFO monitor.ContainersMonitorImpl:  Using ResourceCalculatorPlugin : org.apache.hadoop.yarn.util.ResourceCalculatorPlugin@451001e5\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:17,755 INFO http.HttpServer2: Added filter 'org.apache.hadoop.hdfs.web.AuthFilter' (class=org.apache.hadoop.hdfs.web.AuthFilter)\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:17,755 INFO http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.namenode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:17,755 INFO monitor.ContainersMonitorImpl:  Using ResourceCalculatorProcessTree : null\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:17,756 INFO monitor.ContainersMonitorImpl: Physical memory check enabled: true\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:17,756 INFO monitor.ContainersMonitorImpl: Virtual memory check enabled: true\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:17,756 INFO monitor.ContainersMonitorImpl: Elastic memory control enabled: false\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:17,756 INFO monitor.ContainersMonitorImpl: Strict memory control enabled: true\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:17,756 INFO monitor.ContainersMonitorImpl: ContainersMonitor enabled: true\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:17,759 WARN monitor.ContainersMonitorImpl: NodeManager configured with 62.1 G physical memory allocated to containers, which is more than 80% of the total physical memory available (61.5 G). Thrashing might happen.\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:17,759 INFO containermanager.ContainerManagerImpl: Not a recoverable state store. Nothing to recover.\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:17,761 INFO security.YarnAuthorizationProvider: org.apache.hadoop.yarn.security.ConfiguredYarnAuthorizer is instantiated.\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:17,763 INFO http.HttpServer2: Jetty bound to port 9870\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:17,764 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.RMAppManagerEventType for class org.apache.hadoop.yarn.server.resourcemanager.RMAppManager\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:17,764 INFO server.Server: jetty-9.4.20.v20190813; built: 2019-08-13T21:28:18.144Z; git: 84700530e645e812b336747464d6fbbf370c9a20; jvm 1.8.0_272-b10\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:17,771 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncherEventType for class org.apache.hadoop.yarn.server.resourcemanager.amlauncher.ApplicationMasterLauncher\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:17,772 INFO resourcemanager.RMNMInfo: Registered RMNMInfo MBean\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:17,773 INFO monitor.RMAppLifetimeMonitor: Application lifelime monitor interval set to 3000 ms.\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:17,776 INFO conf.Configuration: resource-types.xml not found\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:17,777 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:17,779 INFO placement.MultiNodeSortingManager: Initializing NodeSortingService=MultiNodeSortingManager\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:17,781 INFO util.HostsFileReader: Refreshing hosts (include/exclude) list\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:17,782 INFO conf.Configuration: node-resources.xml not found\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:17,782 INFO resource.ResourceUtils: Unable to find 'node-resources.xml'.\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:17,783 INFO nodemanager.NodeStatusUpdaterImpl: Nodemanager resources is set to: <memory:63569, vCores:16>\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:17,786 INFO conf.Configuration: found resource capacity-scheduler.xml at file:/etc/hadoop/conf.empty/capacity-scheduler.xml\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:17,795 INFO common.Util: dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:17,796 INFO server.session: DefaultSessionIdManager workerName=node0\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:17,796 INFO server.session: No SessionScavenger set, using defaults\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:17,797 INFO datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:17,721 INFO containermanager.ContainerManagerImpl: AMRMProxyService is disabled\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:17,721 INFO localizer.ResourceLocalizationService: per directory file limit = 8192\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:17,726 INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:17,728 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hdfs\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:17,728 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:17,728 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:17,746 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:17,746 INFO impl.MetricsSystemImpl: ResourceManager metrics system started\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:17,747 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.event.LocalizerEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService$LocalizerTracker\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:17,752 INFO resources.ResourceHandlerModule: Using traffic control bandwidth handler\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:17,755 INFO monitor.ContainersMonitorImpl:  Using ResourceCalculatorPlugin : org.apache.hadoop.yarn.util.ResourceCalculatorPlugin@451001e5\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:17,755 INFO http.HttpServer2: Added filter 'org.apache.hadoop.hdfs.web.AuthFilter' (class=org.apache.hadoop.hdfs.web.AuthFilter)\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:17,755 INFO http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.namenode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:17,755 INFO monitor.ContainersMonitorImpl:  Using ResourceCalculatorProcessTree : null\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:17,756 INFO monitor.ContainersMonitorImpl: Physical memory check enabled: true\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:17,756 INFO monitor.ContainersMonitorImpl: Virtual memory check enabled: true\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:17,756 INFO monitor.ContainersMonitorImpl: Elastic memory control enabled: false\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:17,756 INFO monitor.ContainersMonitorImpl: Strict memory control enabled: true\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:17,756 INFO monitor.ContainersMonitorImpl: ContainersMonitor enabled: true\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:17,759 WARN monitor.ContainersMonitorImpl: NodeManager configured with 62.1 G physical memory allocated to containers, which is more than 80% of the total physical memory available (61.5 G). Thrashing might happen.\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:17,759 INFO containermanager.ContainerManagerImpl: Not a recoverable state store. Nothing to recover.\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:17,761 INFO security.YarnAuthorizationProvider: org.apache.hadoop.yarn.security.ConfiguredYarnAuthorizer is instantiated.\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:17,763 INFO http.HttpServer2: Jetty bound to port 9870\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:17,764 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.RMAppManagerEventType for class org.apache.hadoop.yarn.server.resourcemanager.RMAppManager\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:17,764 INFO server.Server: jetty-9.4.20.v20190813; built: 2019-08-13T21:28:18.144Z; git: 84700530e645e812b336747464d6fbbf370c9a20; jvm 1.8.0_272-b10\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:17,771 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncherEventType for class org.apache.hadoop.yarn.server.resourcemanager.amlauncher.ApplicationMasterLauncher\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:17,772 INFO resourcemanager.RMNMInfo: Registered RMNMInfo MBean\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:17,773 INFO monitor.RMAppLifetimeMonitor: Application lifelime monitor interval set to 3000 ms.\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:17,776 INFO conf.Configuration: resource-types.xml not found\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:17,777 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:17,779 INFO placement.MultiNodeSortingManager: Initializing NodeSortingService=MultiNodeSortingManager\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:17,781 INFO util.HostsFileReader: Refreshing hosts (include/exclude) list\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:17,782 INFO conf.Configuration: node-resources.xml not found\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:17,782 INFO resource.ResourceUtils: Unable to find 'node-resources.xml'.\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:17,783 INFO nodemanager.NodeStatusUpdaterImpl: Nodemanager resources is set to: <memory:63569, vCores:16>\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:17,786 INFO conf.Configuration: found resource capacity-scheduler.xml at file:/etc/hadoop/conf.empty/capacity-scheduler.xml\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:17,795 INFO common.Util: dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:17,796 INFO server.session: DefaultSessionIdManager workerName=node0\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:17,796 INFO server.session: No SessionScavenger set, using defaults\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:17,797 INFO datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:17,797 INFO server.session: node0 Scavenging every 600000ms\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:17,798 INFO scheduler.AbstractYarnScheduler: Minimum allocation = <memory:1, vCores:1>\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:17,798 INFO scheduler.AbstractYarnScheduler: Maximum allocation = <memory:63569, vCores:16>\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:17,800 INFO datanode.DataNode: Configured hostname is algo-1\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:17,801 INFO common.Util: dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:17,804 INFO datanode.DataNode: Starting DataNode with maxLockedMemory = 0\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:17,806 INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@305ffe9e{logs,/logs,file:///usr/lib/hadoop/logs/,AVAILABLE}\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:17,807 INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@35841320{static,/static,file:///usr/lib/hadoop-hdfs/webapps/static/,AVAILABLE}\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:17,811 INFO nodemanager.NodeStatusUpdaterImpl: Initialized nodemanager with : physical-memory=63569 virtual-memory=317845 virtual-cores=16\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:17,823 INFO datanode.DataNode: Opened streaming server at /0.0.0.0:9866\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:17,825 INFO datanode.DataNode: Balancing bandwidth is 10485760 bytes/s\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:17,825 INFO datanode.DataNode: Number threads for balancing is 50\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:17,830 INFO capacity.CapacitySchedulerConfiguration: max alloc mb per queue for root is undefined\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:17,830 INFO capacity.CapacitySchedulerConfiguration: max alloc vcore per queue for root is undefined\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:17,838 INFO capacity.ParentQueue: root, capacity=1.0, absoluteCapacity=1.0, maxCapacity=1.0, absoluteMaxCapacity=1.0, state=RUNNING, acls=SUBMIT_APP:*ADMINISTER_QUEUE:*, labels=*,\u001b[0m\n",
      "\u001b[34m, reservationsContinueLooking=true, orderingPolicy=utilization, priority=0\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:17,838 INFO capacity.ParentQueue: Initialized parent-queue root name=root, fullname=root\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:17,851 INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 2000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:17,852 INFO capacity.CapacitySchedulerConfiguration: max alloc mb per queue for root.default is undefined\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:17,853 INFO capacity.CapacitySchedulerConfiguration: max alloc vcore per queue for root.default is undefined\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:17,854 INFO util.TypeUtil: JVM Runtime does not support Modules\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:17,855 INFO capacity.LeafQueue: Initializing default\u001b[0m\n",
      "\u001b[34mcapacity = 1.0 [= (float) configuredCapacity / 100 ]\u001b[0m\n",
      "\u001b[34mabsoluteCapacity = 1.0 [= parentAbsoluteCapacity * capacity ]\u001b[0m\n",
      "\u001b[34mmaxCapacity = 1.0 [= configuredMaxCapacity ]\u001b[0m\n",
      "\u001b[34mabsoluteMaxCapacity = 1.0 [= 1.0 maximumCapacity undefined, (parentAbsoluteMaxCapacity * maximumCapacity) / 100 otherwise ]\u001b[0m\n",
      "\u001b[34meffectiveMinResource=<memory:0, vCores:0>\n",
      " , effectiveMaxResource=<memory:0, vCores:0>\u001b[0m\n",
      "\u001b[34muserLimit = 100 [= configuredUserLimit ]\u001b[0m\n",
      "\u001b[34muserLimitFactor = 1.0 [= configuredUserLimitFactor ]\u001b[0m\n",
      "\u001b[34mmaxApplications = 10000 [= configuredMaximumSystemApplicationsPerQueue or (int)(configuredMaximumSystemApplications * absoluteCapacity)]\u001b[0m\n",
      "\u001b[34mmaxApplicationsPerUser = 10000 [= (int)(maxApplications * (userLimit / 100.0f) * userLimitFactor) ]\u001b[0m\n",
      "\u001b[34musedCapacity = 0.0 [= usedResourcesMemory / (clusterResourceMemory * absoluteCapacity)]\u001b[0m\n",
      "\u001b[34mabsoluteUsedCapacity = 0.0 [= usedResourcesMemory / clusterResourceMemory]\u001b[0m\n",
      "\u001b[34mmaxAMResourcePerQueuePercent = 0.1 [= configuredMaximumAMResourcePercent ]\u001b[0m\n",
      "\u001b[34mminimumAllocationFactor = 0.99998426 [= (float)(maximumAllocationMemory - minimumAllocationMemory) / maximumAllocationMemory ]\u001b[0m\n",
      "\u001b[34mmaximumAllocation = <memory:63569, vCores:16> [= configuredMaxAllocation ]\u001b[0m\n",
      "\u001b[34mnumContainers = 0 [= currentNumContainers ]\u001b[0m\n",
      "\u001b[34mstate = RUNNING [= configuredState ]\u001b[0m\n",
      "\u001b[34macls = SUBMIT_APP:*ADMINISTER_QUEUE:* [= configuredAcls ]\u001b[0m\n",
      "\u001b[34mnodeLocalityDelay = 40\u001b[0m\n",
      "\u001b[34mrackLocalityAdditionalDelay = -1\u001b[0m\n",
      "\u001b[34mlabels=*,\u001b[0m\n",
      "\u001b[34mreservationsContinueLooking = true\u001b[0m\n",
      "\u001b[34mpreemptionDisabled = true\u001b[0m\n",
      "\u001b[34mdefaultAppPriorityPerQueue = 0\u001b[0m\n",
      "\u001b[34mpriority = 0\u001b[0m\n",
      "\u001b[34mmaxLifetime = -1 seconds\u001b[0m\n",
      "\u001b[34mdefaultLifetime = -1 seconds\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:17,855 INFO capacity.CapacitySchedulerQueueManager: Initialized queue: default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:0, vCores:0>, usedCapacity=0.0, absoluteUsedCapacity=0.0, numApps=0, numContainers=0, effectiveMinResource=<memory:0, vCores:0> , effectiveMaxResource=<memory:0, vCores:0>\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:17,855 INFO capacity.CapacitySchedulerQueueManager: Initialized queue: root: numChildQueue= 1, capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:0, vCores:0>usedCapacity=0.0, numApps=0, numContainers=0\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:17,797 INFO server.session: node0 Scavenging every 600000ms\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:17,798 INFO scheduler.AbstractYarnScheduler: Minimum allocation = <memory:1, vCores:1>\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:17,798 INFO scheduler.AbstractYarnScheduler: Maximum allocation = <memory:63569, vCores:16>\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:17,800 INFO datanode.DataNode: Configured hostname is algo-1\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:17,801 INFO common.Util: dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:17,804 INFO datanode.DataNode: Starting DataNode with maxLockedMemory = 0\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:17,806 INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@305ffe9e{logs,/logs,file:///usr/lib/hadoop/logs/,AVAILABLE}\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:17,807 INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@35841320{static,/static,file:///usr/lib/hadoop-hdfs/webapps/static/,AVAILABLE}\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:17,811 INFO nodemanager.NodeStatusUpdaterImpl: Initialized nodemanager with : physical-memory=63569 virtual-memory=317845 virtual-cores=16\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:17,823 INFO datanode.DataNode: Opened streaming server at /0.0.0.0:9866\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:17,825 INFO datanode.DataNode: Balancing bandwidth is 10485760 bytes/s\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:17,825 INFO datanode.DataNode: Number threads for balancing is 50\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:17,830 INFO capacity.CapacitySchedulerConfiguration: max alloc mb per queue for root is undefined\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:17,830 INFO capacity.CapacitySchedulerConfiguration: max alloc vcore per queue for root is undefined\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:17,838 INFO capacity.ParentQueue: root, capacity=1.0, absoluteCapacity=1.0, maxCapacity=1.0, absoluteMaxCapacity=1.0, state=RUNNING, acls=SUBMIT_APP:*ADMINISTER_QUEUE:*, labels=*,\u001b[0m\n",
      "\u001b[35m, reservationsContinueLooking=true, orderingPolicy=utilization, priority=0\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:17,838 INFO capacity.ParentQueue: Initialized parent-queue root name=root, fullname=root\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:17,851 INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 2000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:17,852 INFO capacity.CapacitySchedulerConfiguration: max alloc mb per queue for root.default is undefined\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:17,853 INFO capacity.CapacitySchedulerConfiguration: max alloc vcore per queue for root.default is undefined\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:17,854 INFO util.TypeUtil: JVM Runtime does not support Modules\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:17,855 INFO capacity.LeafQueue: Initializing default\u001b[0m\n",
      "\u001b[35mcapacity = 1.0 [= (float) configuredCapacity / 100 ]\u001b[0m\n",
      "\u001b[35mabsoluteCapacity = 1.0 [= parentAbsoluteCapacity * capacity ]\u001b[0m\n",
      "\u001b[35mmaxCapacity = 1.0 [= configuredMaxCapacity ]\u001b[0m\n",
      "\u001b[35mabsoluteMaxCapacity = 1.0 [= 1.0 maximumCapacity undefined, (parentAbsoluteMaxCapacity * maximumCapacity) / 100 otherwise ]\u001b[0m\n",
      "\u001b[35meffectiveMinResource=<memory:0, vCores:0>\n",
      " , effectiveMaxResource=<memory:0, vCores:0>\u001b[0m\n",
      "\u001b[35muserLimit = 100 [= configuredUserLimit ]\u001b[0m\n",
      "\u001b[35muserLimitFactor = 1.0 [= configuredUserLimitFactor ]\u001b[0m\n",
      "\u001b[35mmaxApplications = 10000 [= configuredMaximumSystemApplicationsPerQueue or (int)(configuredMaximumSystemApplications * absoluteCapacity)]\u001b[0m\n",
      "\u001b[35mmaxApplicationsPerUser = 10000 [= (int)(maxApplications * (userLimit / 100.0f) * userLimitFactor) ]\u001b[0m\n",
      "\u001b[35musedCapacity = 0.0 [= usedResourcesMemory / (clusterResourceMemory * absoluteCapacity)]\u001b[0m\n",
      "\u001b[35mabsoluteUsedCapacity = 0.0 [= usedResourcesMemory / clusterResourceMemory]\u001b[0m\n",
      "\u001b[35mmaxAMResourcePerQueuePercent = 0.1 [= configuredMaximumAMResourcePercent ]\u001b[0m\n",
      "\u001b[35mminimumAllocationFactor = 0.99998426 [= (float)(maximumAllocationMemory - minimumAllocationMemory) / maximumAllocationMemory ]\u001b[0m\n",
      "\u001b[35mmaximumAllocation = <memory:63569, vCores:16> [= configuredMaxAllocation ]\u001b[0m\n",
      "\u001b[35mnumContainers = 0 [= currentNumContainers ]\u001b[0m\n",
      "\u001b[35mstate = RUNNING [= configuredState ]\u001b[0m\n",
      "\u001b[35macls = SUBMIT_APP:*ADMINISTER_QUEUE:* [= configuredAcls ]\u001b[0m\n",
      "\u001b[35mnodeLocalityDelay = 40\u001b[0m\n",
      "\u001b[35mrackLocalityAdditionalDelay = -1\u001b[0m\n",
      "\u001b[35mlabels=*,\u001b[0m\n",
      "\u001b[35mreservationsContinueLooking = true\u001b[0m\n",
      "\u001b[35mpreemptionDisabled = true\u001b[0m\n",
      "\u001b[35mdefaultAppPriorityPerQueue = 0\u001b[0m\n",
      "\u001b[35mpriority = 0\u001b[0m\n",
      "\u001b[35mmaxLifetime = -1 seconds\u001b[0m\n",
      "\u001b[35mdefaultLifetime = -1 seconds\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:17,855 INFO capacity.CapacitySchedulerQueueManager: Initialized queue: default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:0, vCores:0>, usedCapacity=0.0, absoluteUsedCapacity=0.0, numApps=0, numContainers=0, effectiveMinResource=<memory:0, vCores:0> , effectiveMaxResource=<memory:0, vCores:0>\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:17,855 INFO capacity.CapacitySchedulerQueueManager: Initialized queue: root: numChildQueue= 1, capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:0, vCores:0>usedCapacity=0.0, numApps=0, numContainers=0\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:17,857 INFO capacity.CapacitySchedulerQueueManager: Initialized root queue root: numChildQueue= 1, capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:0, vCores:0>usedCapacity=0.0, numApps=0, numContainers=0\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:17,857 INFO placement.UserGroupMappingPlacementRule: Initialized queue mappings, override: false\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:17,858 INFO placement.MultiNodeSortingManager: MultiNode scheduling is 'false', and configured policies are \u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:17,858 INFO capacity.CapacityScheduler: Initialized CapacityScheduler with calculator=class org.apache.hadoop.yarn.util.resource.DefaultResourceCalculator, minimumAllocation=<<memory:1, vCores:1>>, maximumAllocation=<<memory:63569, vCores:16>>, asynchronousScheduling=false, asyncScheduleInterval=5ms,multiNodePlacementEnabled=false\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:17,858 INFO util.log: Logging initialized @1179ms to org.eclipse.jetty.util.log.Slf4jLog\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:17,861 INFO conf.Configuration: dynamic-resources.xml not found\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:17,862 INFO resourcemanager.AMSProcessingChain: Initializing AMS Processing chain. Root Processor=[org.apache.hadoop.yarn.server.resourcemanager.DefaultAMSProcessor].\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:17,862 INFO resourcemanager.ApplicationMasterService: disabled placement handler will be used, all scheduling requests will be rejected.\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:17,863 INFO resourcemanager.AMSProcessingChain: Adding [org.apache.hadoop.yarn.server.resourcemanager.scheduler.constraint.processor.DisabledPlacementProcessor] tp top of AMS Processing chain. \u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:17,865 INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@39d76cb5{hdfs,/,file:///usr/lib/hadoop-hdfs/webapps/hdfs/,AVAILABLE}{file:/usr/lib/hadoop-hdfs/webapps/hdfs}\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:17,869 INFO resourcemanager.ResourceManager: TimelineServicePublisher is not configured\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:17,869 INFO ipc.Server: Starting Socket Reader #1 for port 0\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:17,873 INFO server.AbstractConnector: Started ServerConnector@5884a914{HTTP/1.1,[http/1.1]}{0.0.0.0:9870}\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:17,873 INFO server.Server: Started @1208ms\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:17,903 INFO util.log: Logging initialized @1223ms to org.eclipse.jetty.util.log.Slf4jLog\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:17,993 INFO server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:18,001 INFO http.HttpRequestLog: Http request log for http.requests.resourcemanager is not defined\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:18,008 INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:18,010 INFO http.HttpServer2: Added filter RMAuthenticationFilter (class=org.apache.hadoop.yarn.server.security.http.RMAuthenticationFilter) to context cluster\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:18,011 INFO http.HttpServer2: Added filter RMAuthenticationFilter (class=org.apache.hadoop.yarn.server.security.http.RMAuthenticationFilter) to context static\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:18,011 INFO http.HttpServer2: Added filter RMAuthenticationFilter (class=org.apache.hadoop.yarn.server.security.http.RMAuthenticationFilter) to context logs\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:18,011 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context cluster\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:18,011 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:18,011 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:18,013 INFO http.HttpServer2: adding path spec: /cluster/*\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:18,013 INFO http.HttpServer2: adding path spec: /ws/*\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:18,013 INFO http.HttpServer2: adding path spec: /app/*\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:18,016 INFO server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:18,022 INFO http.HttpRequestLog: Http request log for http.requests.datanode is not defined\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:18,027 INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:18,028 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:18,028 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:18,028 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:18,052 INFO http.HttpServer2: Jetty bound to port 41275\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:18,053 INFO server.Server: jetty-9.4.20.v20190813; built: 2019-08-13T21:28:18.144Z; git: 84700530e645e812b336747464d6fbbf370c9a20; jvm 1.8.0_272-b10\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:18,073 INFO pb.RpcServerFactoryPBImpl: Adding protocol org.apache.hadoop.yarn.api.ContainerManagementProtocolPB to the server\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:18,074 INFO ipc.Server: IPC Server Responder: starting\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:18,074 INFO ipc.Server: IPC Server listener on 0: starting\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:18,076 INFO server.session: DefaultSessionIdManager workerName=node0\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:18,076 INFO server.session: No SessionScavenger set, using defaults\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:18,077 INFO server.session: node0 Scavenging every 600000ms\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:18,085 INFO security.NMContainerTokenSecretManager: Updating node address : algo-1:46605\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:18,085 WARN namenode.FSNamesystem: Only one image storage directory (dfs.namenode.name.dir) configured. Beware of data loss due to lack of redundant storage directories!\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:18,085 WARN namenode.FSNamesystem: Only one namespace edits storage directory (dfs.namenode.edits.dir) configured. Beware of data loss due to lack of redundant storage directories!\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:18,088 INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@3fce8fd9{logs,/logs,file:///usr/lib/hadoop/logs/,AVAILABLE}\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:18,089 INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@1df8b5b8{static,/static,file:///usr/lib/hadoop-hdfs/webapps/static/,AVAILABLE}\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:18,092 INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 500, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:18,093 INFO ipc.Server: Starting Socket Reader #1 for port 8040\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:18,098 INFO pb.RpcServerFactoryPBImpl: Adding protocol org.apache.hadoop.yarn.server.nodemanager.api.LocalizationProtocolPB to the server\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:18,099 INFO ipc.Server: IPC Server Responder: starting\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:18,107 INFO ipc.Server: IPC Server listener on 8040: starting\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:18,111 INFO localizer.ResourceLocalizationService: Localizer started on port 8040\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:17,857 INFO capacity.CapacitySchedulerQueueManager: Initialized root queue root: numChildQueue= 1, capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:0, vCores:0>usedCapacity=0.0, numApps=0, numContainers=0\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:17,857 INFO placement.UserGroupMappingPlacementRule: Initialized queue mappings, override: false\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:17,858 INFO placement.MultiNodeSortingManager: MultiNode scheduling is 'false', and configured policies are \u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:17,858 INFO capacity.CapacityScheduler: Initialized CapacityScheduler with calculator=class org.apache.hadoop.yarn.util.resource.DefaultResourceCalculator, minimumAllocation=<<memory:1, vCores:1>>, maximumAllocation=<<memory:63569, vCores:16>>, asynchronousScheduling=false, asyncScheduleInterval=5ms,multiNodePlacementEnabled=false\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:17,858 INFO util.log: Logging initialized @1179ms to org.eclipse.jetty.util.log.Slf4jLog\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:17,861 INFO conf.Configuration: dynamic-resources.xml not found\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:17,862 INFO resourcemanager.AMSProcessingChain: Initializing AMS Processing chain. Root Processor=[org.apache.hadoop.yarn.server.resourcemanager.DefaultAMSProcessor].\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:17,862 INFO resourcemanager.ApplicationMasterService: disabled placement handler will be used, all scheduling requests will be rejected.\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:17,863 INFO resourcemanager.AMSProcessingChain: Adding [org.apache.hadoop.yarn.server.resourcemanager.scheduler.constraint.processor.DisabledPlacementProcessor] tp top of AMS Processing chain. \u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:17,865 INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@39d76cb5{hdfs,/,file:///usr/lib/hadoop-hdfs/webapps/hdfs/,AVAILABLE}{file:/usr/lib/hadoop-hdfs/webapps/hdfs}\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:17,869 INFO resourcemanager.ResourceManager: TimelineServicePublisher is not configured\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:17,869 INFO ipc.Server: Starting Socket Reader #1 for port 0\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:17,873 INFO server.AbstractConnector: Started ServerConnector@5884a914{HTTP/1.1,[http/1.1]}{0.0.0.0:9870}\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:17,873 INFO server.Server: Started @1208ms\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:17,903 INFO util.log: Logging initialized @1223ms to org.eclipse.jetty.util.log.Slf4jLog\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:17,993 INFO server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:18,001 INFO http.HttpRequestLog: Http request log for http.requests.resourcemanager is not defined\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:18,008 INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:18,010 INFO http.HttpServer2: Added filter RMAuthenticationFilter (class=org.apache.hadoop.yarn.server.security.http.RMAuthenticationFilter) to context cluster\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:18,011 INFO http.HttpServer2: Added filter RMAuthenticationFilter (class=org.apache.hadoop.yarn.server.security.http.RMAuthenticationFilter) to context static\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:18,011 INFO http.HttpServer2: Added filter RMAuthenticationFilter (class=org.apache.hadoop.yarn.server.security.http.RMAuthenticationFilter) to context logs\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:18,011 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context cluster\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:18,011 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:18,011 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:18,013 INFO http.HttpServer2: adding path spec: /cluster/*\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:18,013 INFO http.HttpServer2: adding path spec: /ws/*\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:18,013 INFO http.HttpServer2: adding path spec: /app/*\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:18,016 INFO server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:18,022 INFO http.HttpRequestLog: Http request log for http.requests.datanode is not defined\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:18,027 INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:18,028 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:18,028 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:18,028 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:18,052 INFO http.HttpServer2: Jetty bound to port 41275\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:18,053 INFO server.Server: jetty-9.4.20.v20190813; built: 2019-08-13T21:28:18.144Z; git: 84700530e645e812b336747464d6fbbf370c9a20; jvm 1.8.0_272-b10\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:18,073 INFO pb.RpcServerFactoryPBImpl: Adding protocol org.apache.hadoop.yarn.api.ContainerManagementProtocolPB to the server\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:18,074 INFO ipc.Server: IPC Server Responder: starting\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:18,074 INFO ipc.Server: IPC Server listener on 0: starting\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:18,076 INFO server.session: DefaultSessionIdManager workerName=node0\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:18,076 INFO server.session: No SessionScavenger set, using defaults\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:18,077 INFO server.session: node0 Scavenging every 600000ms\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:18,085 INFO security.NMContainerTokenSecretManager: Updating node address : algo-1:46605\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:18,085 WARN namenode.FSNamesystem: Only one image storage directory (dfs.namenode.name.dir) configured. Beware of data loss due to lack of redundant storage directories!\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:18,085 WARN namenode.FSNamesystem: Only one namespace edits storage directory (dfs.namenode.edits.dir) configured. Beware of data loss due to lack of redundant storage directories!\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:18,088 INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@3fce8fd9{logs,/logs,file:///usr/lib/hadoop/logs/,AVAILABLE}\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:18,089 INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@1df8b5b8{static,/static,file:///usr/lib/hadoop-hdfs/webapps/static/,AVAILABLE}\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:18,092 INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 500, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:18,093 INFO ipc.Server: Starting Socket Reader #1 for port 8040\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:18,098 INFO pb.RpcServerFactoryPBImpl: Adding protocol org.apache.hadoop.yarn.server.nodemanager.api.LocalizationProtocolPB to the server\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:18,099 INFO ipc.Server: IPC Server Responder: starting\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:18,107 INFO ipc.Server: IPC Server listener on 8040: starting\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:18,111 INFO localizer.ResourceLocalizationService: Localizer started on port 8040\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:18,114 INFO containermanager.ContainerManagerImpl: ContainerManager started at /10.0.189.145:46605\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:18,114 INFO containermanager.ContainerManagerImpl: ContainerManager bound to algo-1/10.0.189.145:0\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:18,114 WARN tracker.NMLogAggregationStatusTracker: Log Aggregation is disabled.So is the LogAggregationStatusTracker.\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:18,118 INFO webapp.WebServer: Instantiating NMWebApp at algo-1:8042\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:18,142 INFO namenode.FSEditLog: Edit logging is async:true\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:18,145 INFO util.TypeUtil: JVM Runtime does not support Modules\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:18,147 INFO util.log: Logging initialized @1477ms to org.eclipse.jetty.util.log.Slf4jLog\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:18,155 INFO namenode.FSNamesystem: KeyProvider: null\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:18,156 INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@7d446ed1{datanode,/,file:///usr/lib/hadoop-hdfs/webapps/datanode/,AVAILABLE}{file:/usr/lib/hadoop-hdfs/webapps/datanode}\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:18,156 INFO namenode.FSNamesystem: fsLock is fair: true\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:18,157 INFO namenode.FSNamesystem: Detailed lock hold time metrics enabled: false\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:18,162 INFO namenode.FSNamesystem: fsOwner             = root (auth:SIMPLE)\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:18,162 INFO namenode.FSNamesystem: supergroup          = supergroup\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:18,162 INFO namenode.FSNamesystem: isPermissionEnabled = true\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:18,163 INFO namenode.FSNamesystem: HA Enabled: false\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:18,164 INFO server.AbstractConnector: Started ServerConnector@27e47833{HTTP/1.1,[http/1.1]}{localhost:41275}\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:18,164 INFO server.Server: Started @1485ms\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:18,191 INFO common.Util: dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:18,197 INFO blockmanagement.DatanodeManager: dfs.block.invalidate.limit: configured=1000, counted=60, effected=1000\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:18,197 INFO blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:18,200 INFO blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:18,200 INFO blockmanagement.BlockManager: The block deletion will start around 2021 Mar 02 05:26:18\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:18,202 INFO util.GSet: Computing capacity for map BlocksMap\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:18,202 INFO util.GSet: VM type       = 64-bit\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:18,203 INFO util.GSet: 2.0% max memory 13.5 GB = 277.2 MB\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:18,203 INFO util.GSet: capacity      = 2^25 = 33554432 entries\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:18,243 INFO server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:18,246 INFO http.HttpRequestLog: Http request log for http.requests.nodemanager is not defined\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:18,250 INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:18,251 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context node\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:18,251 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:18,251 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:18,252 INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context node\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:18,252 INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context logs\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:18,252 INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context static\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:18,253 INFO http.HttpServer2: adding path spec: /node/*\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:18,253 INFO http.HttpServer2: adding path spec: /ws/*\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:18,257 INFO blockmanagement.BlockManager: Storage policy satisfier is disabled\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:18,257 INFO blockmanagement.BlockManager: dfs.block.access.token.enable = false\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:18,263 INFO Configuration.deprecation: No unit for dfs.namenode.safemode.extension(30000) assuming MILLISECONDS\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:18,264 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.threshold-pct = 0.9990000128746033\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:18,264 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.min.datanodes = 0\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:18,264 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.extension = 30000\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:18,264 INFO blockmanagement.BlockManager: defaultReplication         = 3\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:18,264 INFO blockmanagement.BlockManager: maxReplication             = 512\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:18,264 INFO blockmanagement.BlockManager: minReplication             = 1\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:18,264 INFO blockmanagement.BlockManager: maxReplicationStreams      = 2\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:18,264 INFO blockmanagement.BlockManager: redundancyRecheckInterval  = 3000ms\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:18,264 INFO blockmanagement.BlockManager: encryptDataTransfer        = false\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:18,264 INFO blockmanagement.BlockManager: maxNumBlocksToLog          = 1000\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:18,291 INFO namenode.FSDirectory: GLOBAL serial map: bits=29 maxEntries=536870911\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:18,114 INFO containermanager.ContainerManagerImpl: ContainerManager started at /10.0.189.145:46605\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:18,114 INFO containermanager.ContainerManagerImpl: ContainerManager bound to algo-1/10.0.189.145:0\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:18,114 WARN tracker.NMLogAggregationStatusTracker: Log Aggregation is disabled.So is the LogAggregationStatusTracker.\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:18,118 INFO webapp.WebServer: Instantiating NMWebApp at algo-1:8042\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:18,142 INFO namenode.FSEditLog: Edit logging is async:true\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:18,145 INFO util.TypeUtil: JVM Runtime does not support Modules\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:18,147 INFO util.log: Logging initialized @1477ms to org.eclipse.jetty.util.log.Slf4jLog\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:18,155 INFO namenode.FSNamesystem: KeyProvider: null\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:18,156 INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@7d446ed1{datanode,/,file:///usr/lib/hadoop-hdfs/webapps/datanode/,AVAILABLE}{file:/usr/lib/hadoop-hdfs/webapps/datanode}\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:18,156 INFO namenode.FSNamesystem: fsLock is fair: true\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:18,157 INFO namenode.FSNamesystem: Detailed lock hold time metrics enabled: false\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:18,162 INFO namenode.FSNamesystem: fsOwner             = root (auth:SIMPLE)\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:18,162 INFO namenode.FSNamesystem: supergroup          = supergroup\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:18,162 INFO namenode.FSNamesystem: isPermissionEnabled = true\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:18,163 INFO namenode.FSNamesystem: HA Enabled: false\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:18,164 INFO server.AbstractConnector: Started ServerConnector@27e47833{HTTP/1.1,[http/1.1]}{localhost:41275}\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:18,164 INFO server.Server: Started @1485ms\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:18,191 INFO common.Util: dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:18,197 INFO blockmanagement.DatanodeManager: dfs.block.invalidate.limit: configured=1000, counted=60, effected=1000\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:18,197 INFO blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:18,200 INFO blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:18,200 INFO blockmanagement.BlockManager: The block deletion will start around 2021 Mar 02 05:26:18\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:18,202 INFO util.GSet: Computing capacity for map BlocksMap\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:18,202 INFO util.GSet: VM type       = 64-bit\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:18,203 INFO util.GSet: 2.0% max memory 13.5 GB = 277.2 MB\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:18,203 INFO util.GSet: capacity      = 2^25 = 33554432 entries\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:18,243 INFO server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:18,246 INFO http.HttpRequestLog: Http request log for http.requests.nodemanager is not defined\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:18,250 INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:18,251 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context node\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:18,251 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:18,251 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:18,252 INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context node\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:18,252 INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context logs\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:18,252 INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context static\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:18,253 INFO http.HttpServer2: adding path spec: /node/*\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:18,253 INFO http.HttpServer2: adding path spec: /ws/*\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:18,257 INFO blockmanagement.BlockManager: Storage policy satisfier is disabled\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:18,257 INFO blockmanagement.BlockManager: dfs.block.access.token.enable = false\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:18,263 INFO Configuration.deprecation: No unit for dfs.namenode.safemode.extension(30000) assuming MILLISECONDS\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:18,264 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.threshold-pct = 0.9990000128746033\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:18,264 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.min.datanodes = 0\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:18,264 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.extension = 30000\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:18,264 INFO blockmanagement.BlockManager: defaultReplication         = 3\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:18,264 INFO blockmanagement.BlockManager: maxReplication             = 512\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:18,264 INFO blockmanagement.BlockManager: minReplication             = 1\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:18,264 INFO blockmanagement.BlockManager: maxReplicationStreams      = 2\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:18,264 INFO blockmanagement.BlockManager: redundancyRecheckInterval  = 3000ms\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:18,264 INFO blockmanagement.BlockManager: encryptDataTransfer        = false\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:18,264 INFO blockmanagement.BlockManager: maxNumBlocksToLog          = 1000\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:18,291 INFO namenode.FSDirectory: GLOBAL serial map: bits=29 maxEntries=536870911\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:18,291 INFO namenode.FSDirectory: USER serial map: bits=24 maxEntries=16777215\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:18,291 INFO namenode.FSDirectory: GROUP serial map: bits=24 maxEntries=16777215\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:18,291 INFO namenode.FSDirectory: XATTR serial map: bits=24 maxEntries=16777215\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:18,295 INFO web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:9864\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:18,299 INFO util.JvmPauseMonitor: Starting JVM pause monitor\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:18,299 INFO datanode.DataNode: dnUserName = root\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:18,299 INFO datanode.DataNode: supergroup = supergroup\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:18,307 INFO util.GSet: Computing capacity for map INodeMap\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:18,307 INFO util.GSet: VM type       = 64-bit\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:18,307 INFO util.GSet: 1.0% max memory 13.5 GB = 138.6 MB\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:18,307 INFO util.GSet: capacity      = 2^24 = 16777216 entries\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:18,330 INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 1000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:18,347 INFO ipc.Server: Starting Socket Reader #1 for port 9867\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:18,411 INFO webapp.WebApps: Registered webapp guice modules\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:18,416 INFO http.HttpServer2: Jetty bound to port 8088\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:18,417 INFO server.Server: jetty-9.4.20.v20190813; built: 2019-08-13T21:28:18.144Z; git: 84700530e645e812b336747464d6fbbf370c9a20; jvm 1.8.0_272-b10\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:18,493 INFO server.session: DefaultSessionIdManager workerName=node0\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:18,493 INFO server.session: No SessionScavenger set, using defaults\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:18,495 INFO server.session: node0 Scavenging every 660000ms\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:18,505 INFO server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:18,514 INFO delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:18,516 INFO delegation.AbstractDelegationTokenSecretManager: Starting expired delegation token remover thread, tokenRemoverScanInterval=60 min(s)\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:18,517 INFO delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:18,533 INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@7d898981{logs,/logs,file:///var/log/yarn/,AVAILABLE}\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:18,534 INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@14f9390f{static,/static,jar:file:/usr/lib/hadoop-yarn/hadoop-yarn-common-3.2.1-amzn-1.jar!/webapps/static,AVAILABLE}\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:18,543 WARN webapp.WebInfConfiguration: Can't generate resourceBase as part of webapp tmp dir name: java.lang.NullPointerException\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:18,545 INFO datanode.DataNode: Opened IPC server at /0.0.0.0:9867\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:18,561 INFO datanode.DataNode: Refresh request received for nameservices: null\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:18,567 INFO namenode.FSDirectory: ACLs enabled? false\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:18,567 INFO namenode.FSDirectory: POSIX ACL inheritance enabled? true\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:18,567 INFO namenode.FSDirectory: XAttrs enabled? true\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:18,567 INFO namenode.NameNode: Caching file names occurring more than 10 times\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:18,569 INFO datanode.DataNode: Starting BPOfferServices for nameservices: <default>\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:18,573 INFO snapshot.SnapshotManager: Loaded config captureOpenFiles: false, skipCaptureAccessTimeOnlyChange: false, snapshotDiffAllowSnapRootDescendant: true, maxSnapshotLimit: 65536\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:18,575 INFO snapshot.SnapshotManager: SkipList is disabled\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:18,579 INFO datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to algo-1/10.0.189.145:8020 starting to offer service\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:18,580 INFO util.GSet: Computing capacity for map cachedBlocks\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:18,580 INFO util.GSet: VM type       = 64-bit\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:18,580 INFO util.GSet: 0.25% max memory 13.5 GB = 34.6 MB\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:18,580 INFO util.GSet: capacity      = 2^22 = 4194304 entries\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:18,581 INFO webapp.WebApps: Registered webapp guice modules\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:18,583 INFO http.HttpServer2: Jetty bound to port 8042\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:18,585 INFO server.Server: jetty-9.4.20.v20190813; built: 2019-08-13T21:28:18.144Z; git: 84700530e645e812b336747464d6fbbf370c9a20; jvm 1.8.0_272-b10\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:18,587 INFO ipc.Server: IPC Server Responder: starting\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:18,588 INFO ipc.Server: IPC Server listener on 9867: starting\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:18,590 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:18,590 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:18,590 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:18,291 INFO namenode.FSDirectory: USER serial map: bits=24 maxEntries=16777215\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:18,291 INFO namenode.FSDirectory: GROUP serial map: bits=24 maxEntries=16777215\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:18,291 INFO namenode.FSDirectory: XATTR serial map: bits=24 maxEntries=16777215\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:18,295 INFO web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:9864\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:18,299 INFO util.JvmPauseMonitor: Starting JVM pause monitor\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:18,299 INFO datanode.DataNode: dnUserName = root\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:18,299 INFO datanode.DataNode: supergroup = supergroup\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:18,307 INFO util.GSet: Computing capacity for map INodeMap\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:18,307 INFO util.GSet: VM type       = 64-bit\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:18,307 INFO util.GSet: 1.0% max memory 13.5 GB = 138.6 MB\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:18,307 INFO util.GSet: capacity      = 2^24 = 16777216 entries\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:18,330 INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 1000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:18,347 INFO ipc.Server: Starting Socket Reader #1 for port 9867\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:18,411 INFO webapp.WebApps: Registered webapp guice modules\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:18,416 INFO http.HttpServer2: Jetty bound to port 8088\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:18,417 INFO server.Server: jetty-9.4.20.v20190813; built: 2019-08-13T21:28:18.144Z; git: 84700530e645e812b336747464d6fbbf370c9a20; jvm 1.8.0_272-b10\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:18,493 INFO server.session: DefaultSessionIdManager workerName=node0\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:18,493 INFO server.session: No SessionScavenger set, using defaults\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:18,495 INFO server.session: node0 Scavenging every 660000ms\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:18,505 INFO server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:18,514 INFO delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:18,516 INFO delegation.AbstractDelegationTokenSecretManager: Starting expired delegation token remover thread, tokenRemoverScanInterval=60 min(s)\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:18,517 INFO delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:18,533 INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@7d898981{logs,/logs,file:///var/log/yarn/,AVAILABLE}\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:18,534 INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@14f9390f{static,/static,jar:file:/usr/lib/hadoop-yarn/hadoop-yarn-common-3.2.1-amzn-1.jar!/webapps/static,AVAILABLE}\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:18,543 WARN webapp.WebInfConfiguration: Can't generate resourceBase as part of webapp tmp dir name: java.lang.NullPointerException\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:18,545 INFO datanode.DataNode: Opened IPC server at /0.0.0.0:9867\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:18,561 INFO datanode.DataNode: Refresh request received for nameservices: null\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:18,567 INFO namenode.FSDirectory: ACLs enabled? false\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:18,567 INFO namenode.FSDirectory: POSIX ACL inheritance enabled? true\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:18,567 INFO namenode.FSDirectory: XAttrs enabled? true\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:18,567 INFO namenode.NameNode: Caching file names occurring more than 10 times\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:18,569 INFO datanode.DataNode: Starting BPOfferServices for nameservices: <default>\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:18,573 INFO snapshot.SnapshotManager: Loaded config captureOpenFiles: false, skipCaptureAccessTimeOnlyChange: false, snapshotDiffAllowSnapRootDescendant: true, maxSnapshotLimit: 65536\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:18,575 INFO snapshot.SnapshotManager: SkipList is disabled\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:18,579 INFO datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to algo-1/10.0.189.145:8020 starting to offer service\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:18,580 INFO util.GSet: Computing capacity for map cachedBlocks\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:18,580 INFO util.GSet: VM type       = 64-bit\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:18,580 INFO util.GSet: 0.25% max memory 13.5 GB = 34.6 MB\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:18,580 INFO util.GSet: capacity      = 2^22 = 4194304 entries\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:18,581 INFO webapp.WebApps: Registered webapp guice modules\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:18,583 INFO http.HttpServer2: Jetty bound to port 8042\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:18,585 INFO server.Server: jetty-9.4.20.v20190813; built: 2019-08-13T21:28:18.144Z; git: 84700530e645e812b336747464d6fbbf370c9a20; jvm 1.8.0_272-b10\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:18,587 INFO ipc.Server: IPC Server Responder: starting\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:18,588 INFO ipc.Server: IPC Server listener on 9867: starting\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:18,590 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:18,590 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:18,590 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:18,593 INFO namenode.FSNamesystem: Retry cache on namenode is enabled\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:18,593 INFO namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:18,594 INFO util.GSet: Computing capacity for map NameNodeRetryCache\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:18,594 INFO util.GSet: VM type       = 64-bit\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:18,595 INFO util.GSet: 0.029999999329447746% max memory 13.5 GB = 4.2 MB\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:18,595 INFO util.GSet: capacity      = 2^19 = 524288 entries\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:18,610 INFO common.Storage: Lock on /opt/amazon/hadoop/hdfs/namenode/in_use.lock acquired by nodename 138@algo-1\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:18,622 INFO server.session: DefaultSessionIdManager workerName=node0\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:18,622 INFO server.session: No SessionScavenger set, using defaults\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:18,623 INFO server.session: node0 Scavenging every 660000ms\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:18,628 INFO namenode.FileJournalManager: Recovering unfinalized segments in /opt/amazon/hadoop/hdfs/namenode/current\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:18,628 INFO namenode.FSImage: No edit log streams selected.\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:18,629 INFO namenode.FSImage: Planning to load image: FSImageFile(file=/opt/amazon/hadoop/hdfs/namenode/current/fsimage_0000000000000000000, cpktTxId=0000000000000000000)\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:18,635 INFO server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:18,641 INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@c2db68f{logs,/logs,file:///var/log/yarn/,AVAILABLE}\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:18,641 INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@3668d4{static,/static,jar:file:/usr/lib/hadoop-yarn/hadoop-yarn-common-3.2.1-amzn-1.jar!/webapps/static,AVAILABLE}\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:18,644 INFO util.TypeUtil: JVM Runtime does not support Modules\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:18,650 WARN webapp.WebInfConfiguration: Can't generate resourceBase as part of webapp tmp dir name: java.lang.NullPointerException\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:18,682 INFO namenode.FSImageFormatPBINode: Loading 1 INodes.\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:18,703 INFO namenode.FSImageFormatProtobuf: Loaded FSImage in 0 seconds.\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:18,703 INFO namenode.FSImage: Loaded image for txid 0 from /opt/amazon/hadoop/hdfs/namenode/current/fsimage_0000000000000000000\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:18,707 INFO namenode.FSNamesystem: Need to save fs image? false (staleImage=false, haEnabled=false, isRollingUpgrade=false)\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:18,707 INFO namenode.FSEditLog: Starting log segment at 1\u001b[0m\n",
      "\u001b[34mMar 02, 2021 5:26:18 AM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory register\u001b[0m\n",
      "\u001b[34mINFO: Registering org.apache.hadoop.yarn.server.resourcemanager.webapp.JAXBContextResolver as a provider class\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:18,593 INFO namenode.FSNamesystem: Retry cache on namenode is enabled\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:18,593 INFO namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:18,594 INFO util.GSet: Computing capacity for map NameNodeRetryCache\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:18,594 INFO util.GSet: VM type       = 64-bit\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:18,595 INFO util.GSet: 0.029999999329447746% max memory 13.5 GB = 4.2 MB\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:18,595 INFO util.GSet: capacity      = 2^19 = 524288 entries\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:18,610 INFO common.Storage: Lock on /opt/amazon/hadoop/hdfs/namenode/in_use.lock acquired by nodename 138@algo-1\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:18,622 INFO server.session: DefaultSessionIdManager workerName=node0\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:18,622 INFO server.session: No SessionScavenger set, using defaults\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:18,623 INFO server.session: node0 Scavenging every 660000ms\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:18,628 INFO namenode.FileJournalManager: Recovering unfinalized segments in /opt/amazon/hadoop/hdfs/namenode/current\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:18,628 INFO namenode.FSImage: No edit log streams selected.\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:18,629 INFO namenode.FSImage: Planning to load image: FSImageFile(file=/opt/amazon/hadoop/hdfs/namenode/current/fsimage_0000000000000000000, cpktTxId=0000000000000000000)\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:18,635 INFO server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:18,641 INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@c2db68f{logs,/logs,file:///var/log/yarn/,AVAILABLE}\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:18,641 INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@3668d4{static,/static,jar:file:/usr/lib/hadoop-yarn/hadoop-yarn-common-3.2.1-amzn-1.jar!/webapps/static,AVAILABLE}\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:18,644 INFO util.TypeUtil: JVM Runtime does not support Modules\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:18,650 WARN webapp.WebInfConfiguration: Can't generate resourceBase as part of webapp tmp dir name: java.lang.NullPointerException\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:18,682 INFO namenode.FSImageFormatPBINode: Loading 1 INodes.\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:18,703 INFO namenode.FSImageFormatProtobuf: Loaded FSImage in 0 seconds.\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:18,703 INFO namenode.FSImage: Loaded image for txid 0 from /opt/amazon/hadoop/hdfs/namenode/current/fsimage_0000000000000000000\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:18,707 INFO namenode.FSNamesystem: Need to save fs image? false (staleImage=false, haEnabled=false, isRollingUpgrade=false)\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:18,707 INFO namenode.FSEditLog: Starting log segment at 1\u001b[0m\n",
      "\u001b[35mMar 02, 2021 5:26:18 AM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory register\u001b[0m\n",
      "\u001b[35mINFO: Registering org.apache.hadoop.yarn.server.resourcemanager.webapp.JAXBContextResolver as a provider class\u001b[0m\n",
      "\u001b[34mMar 02, 2021 5:26:18 AM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory register\u001b[0m\n",
      "\u001b[34mINFO: Registering org.apache.hadoop.yarn.server.resourcemanager.webapp.RMWebServices as a root resource class\u001b[0m\n",
      "\u001b[34mMar 02, 2021 5:26:18 AM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory register\u001b[0m\n",
      "\u001b[34mINFO: Registering org.apache.hadoop.yarn.webapp.GenericExceptionHandler as a provider class\u001b[0m\n",
      "\u001b[35mMar 02, 2021 5:26:18 AM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory register\u001b[0m\n",
      "\u001b[35mINFO: Registering org.apache.hadoop.yarn.server.resourcemanager.webapp.RMWebServices as a root resource class\u001b[0m\n",
      "\u001b[35mMar 02, 2021 5:26:18 AM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory register\u001b[0m\n",
      "\u001b[35mINFO: Registering org.apache.hadoop.yarn.webapp.GenericExceptionHandler as a provider class\u001b[0m\n",
      "\u001b[34mMar 02, 2021 5:26:18 AM com.sun.jersey.server.impl.application.WebApplicationImpl _initiate\u001b[0m\n",
      "\u001b[34mINFO: Initiating Jersey application, version 'Jersey: 1.19 02/11/2015 03:25 AM'\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:18,741 INFO util.TypeUtil: JVM Runtime does not support Modules\u001b[0m\n",
      "\u001b[34mMar 02, 2021 5:26:18 AM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory getComponentProvider\u001b[0m\n",
      "\u001b[34mINFO: Binding org.apache.hadoop.yarn.server.resourcemanager.webapp.JAXBContextResolver to GuiceManagedComponentProvider with the scope \"Singleton\"\u001b[0m\n",
      "\u001b[34mMar 02, 2021 5:26:18 AM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory register\u001b[0m\n",
      "\u001b[34mINFO: Registering org.apache.hadoop.yarn.server.nodemanager.webapp.NMWebServices as a root resource class\u001b[0m\n",
      "\u001b[34mMar 02, 2021 5:26:18 AM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory register\u001b[0m\n",
      "\u001b[34mINFO: Registering org.apache.hadoop.yarn.webapp.GenericExceptionHandler as a provider class\u001b[0m\n",
      "\u001b[34mMar 02, 2021 5:26:18 AM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory register\u001b[0m\n",
      "\u001b[34mINFO: Registering org.apache.hadoop.yarn.server.nodemanager.webapp.JAXBContextResolver as a provider class\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:18,804 INFO namenode.NameCache: initialized with 0 entries 0 lookups\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:18,805 INFO namenode.FSNamesystem: Finished loading FSImage in 207 msecs\u001b[0m\n",
      "\u001b[34mMar 02, 2021 5:26:18 AM com.sun.jersey.server.impl.application.WebApplicationImpl _initiate\u001b[0m\n",
      "\u001b[34mINFO: Initiating Jersey application, version 'Jersey: 1.19 02/11/2015 03:25 AM'\u001b[0m\n",
      "\u001b[34mMar 02, 2021 5:26:18 AM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory getComponentProvider\u001b[0m\n",
      "\u001b[34mINFO: Binding org.apache.hadoop.yarn.server.nodemanager.webapp.JAXBContextResolver to GuiceManagedComponentProvider with the scope \"Singleton\"\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:18,943 INFO namenode.NameNode: RPC server is binding to algo-1:8020\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:18,968 INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 1000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:18,976 INFO ipc.Server: Starting Socket Reader #1 for port 8020\u001b[0m\n",
      "\u001b[34mMar 02, 2021 5:26:18 AM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory getComponentProvider\u001b[0m\n",
      "\u001b[34mINFO: Binding org.apache.hadoop.yarn.webapp.GenericExceptionHandler to GuiceManagedComponentProvider with the scope \"Singleton\"\u001b[0m\n",
      "\u001b[34mMar 02, 2021 5:26:19 AM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory getComponentProvider\u001b[0m\n",
      "\u001b[34mINFO: Binding org.apache.hadoop.yarn.webapp.GenericExceptionHandler to GuiceManagedComponentProvider with the scope \"Singleton\"\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:19,147 INFO namenode.NameNode: Clients are to use algo-1:8020 to access this namenode/service.\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:19,149 INFO namenode.FSNamesystem: Registered FSNamesystemState, ReplicatedBlocksState and ECBlockGroupsState MBeans.\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:19,166 INFO namenode.LeaseManager: Number of blocks under construction: 0\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:19,207 INFO blockmanagement.BlockManager: initializing replication queues\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:19,215 INFO hdfs.StateChange: STATE* Leaving safe mode after 0 secs\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:19,216 INFO hdfs.StateChange: STATE* Network topology has 0 racks and 0 datanodes\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:19,216 INFO hdfs.StateChange: STATE* UnderReplicatedBlocks has 0 blocks\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:19,250 INFO ipc.Server: IPC Server listener on 8020: starting\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:19,251 INFO ipc.Server: IPC Server Responder: starting\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:19,263 INFO blockmanagement.BlockManager: Total number of blocks            = 0\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:19,263 INFO blockmanagement.BlockManager: Number of invalid blocks          = 0\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:19,263 INFO blockmanagement.BlockManager: Number of under-replicated blocks = 0\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:19,263 INFO blockmanagement.BlockManager: Number of  over-replicated blocks = 0\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:19,263 INFO blockmanagement.BlockManager: Number of blocks being written    = 0\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:19,263 INFO hdfs.StateChange: STATE* Replication Queue initialization scan for invalid, over- and under-replicated blocks completed in 47 msec\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:19,316 INFO namenode.NameNode: NameNode RPC up at: algo-1/10.0.189.145:8020\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:19,323 INFO namenode.FSNamesystem: Starting services required for active state\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:19,323 INFO namenode.FSDirectory: Initializing quota with 4 thread(s)\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:19,335 INFO namenode.FSDirectory: Quota initialization completed in 11 milliseconds\u001b[0m\n",
      "\u001b[34mname space=1\u001b[0m\n",
      "\u001b[34mstorage space=0\u001b[0m\n",
      "\u001b[34mstorage types=RAM_DISK=0, SSD=0, DISK=0, ARCHIVE=0, PROVIDED=0\u001b[0m\n",
      "\u001b[34mMar 02, 2021 5:26:19 AM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory getComponentProvider\u001b[0m\n",
      "\u001b[35mMar 02, 2021 5:26:18 AM com.sun.jersey.server.impl.application.WebApplicationImpl _initiate\u001b[0m\n",
      "\u001b[35mINFO: Initiating Jersey application, version 'Jersey: 1.19 02/11/2015 03:25 AM'\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:18,741 INFO util.TypeUtil: JVM Runtime does not support Modules\u001b[0m\n",
      "\u001b[35mMar 02, 2021 5:26:18 AM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory getComponentProvider\u001b[0m\n",
      "\u001b[35mINFO: Binding org.apache.hadoop.yarn.server.resourcemanager.webapp.JAXBContextResolver to GuiceManagedComponentProvider with the scope \"Singleton\"\u001b[0m\n",
      "\u001b[35mMar 02, 2021 5:26:18 AM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory register\u001b[0m\n",
      "\u001b[35mINFO: Registering org.apache.hadoop.yarn.server.nodemanager.webapp.NMWebServices as a root resource class\u001b[0m\n",
      "\u001b[35mMar 02, 2021 5:26:18 AM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory register\u001b[0m\n",
      "\u001b[35mINFO: Registering org.apache.hadoop.yarn.webapp.GenericExceptionHandler as a provider class\u001b[0m\n",
      "\u001b[35mMar 02, 2021 5:26:18 AM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory register\u001b[0m\n",
      "\u001b[35mINFO: Registering org.apache.hadoop.yarn.server.nodemanager.webapp.JAXBContextResolver as a provider class\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:18,804 INFO namenode.NameCache: initialized with 0 entries 0 lookups\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:18,805 INFO namenode.FSNamesystem: Finished loading FSImage in 207 msecs\u001b[0m\n",
      "\u001b[35mMar 02, 2021 5:26:18 AM com.sun.jersey.server.impl.application.WebApplicationImpl _initiate\u001b[0m\n",
      "\u001b[35mINFO: Initiating Jersey application, version 'Jersey: 1.19 02/11/2015 03:25 AM'\u001b[0m\n",
      "\u001b[35mMar 02, 2021 5:26:18 AM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory getComponentProvider\u001b[0m\n",
      "\u001b[35mINFO: Binding org.apache.hadoop.yarn.server.nodemanager.webapp.JAXBContextResolver to GuiceManagedComponentProvider with the scope \"Singleton\"\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:18,943 INFO namenode.NameNode: RPC server is binding to algo-1:8020\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:18,968 INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 1000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:18,976 INFO ipc.Server: Starting Socket Reader #1 for port 8020\u001b[0m\n",
      "\u001b[35mMar 02, 2021 5:26:18 AM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory getComponentProvider\u001b[0m\n",
      "\u001b[35mINFO: Binding org.apache.hadoop.yarn.webapp.GenericExceptionHandler to GuiceManagedComponentProvider with the scope \"Singleton\"\u001b[0m\n",
      "\u001b[35mMar 02, 2021 5:26:19 AM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory getComponentProvider\u001b[0m\n",
      "\u001b[35mINFO: Binding org.apache.hadoop.yarn.webapp.GenericExceptionHandler to GuiceManagedComponentProvider with the scope \"Singleton\"\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:19,147 INFO namenode.NameNode: Clients are to use algo-1:8020 to access this namenode/service.\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:19,149 INFO namenode.FSNamesystem: Registered FSNamesystemState, ReplicatedBlocksState and ECBlockGroupsState MBeans.\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:19,166 INFO namenode.LeaseManager: Number of blocks under construction: 0\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:19,207 INFO blockmanagement.BlockManager: initializing replication queues\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:19,215 INFO hdfs.StateChange: STATE* Leaving safe mode after 0 secs\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:19,216 INFO hdfs.StateChange: STATE* Network topology has 0 racks and 0 datanodes\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:19,216 INFO hdfs.StateChange: STATE* UnderReplicatedBlocks has 0 blocks\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:19,250 INFO ipc.Server: IPC Server listener on 8020: starting\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:19,251 INFO ipc.Server: IPC Server Responder: starting\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:19,263 INFO blockmanagement.BlockManager: Total number of blocks            = 0\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:19,263 INFO blockmanagement.BlockManager: Number of invalid blocks          = 0\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:19,263 INFO blockmanagement.BlockManager: Number of under-replicated blocks = 0\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:19,263 INFO blockmanagement.BlockManager: Number of  over-replicated blocks = 0\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:19,263 INFO blockmanagement.BlockManager: Number of blocks being written    = 0\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:19,263 INFO hdfs.StateChange: STATE* Replication Queue initialization scan for invalid, over- and under-replicated blocks completed in 47 msec\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:19,316 INFO namenode.NameNode: NameNode RPC up at: algo-1/10.0.189.145:8020\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:19,323 INFO namenode.FSNamesystem: Starting services required for active state\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:19,323 INFO namenode.FSDirectory: Initializing quota with 4 thread(s)\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:19,335 INFO namenode.FSDirectory: Quota initialization completed in 11 milliseconds\u001b[0m\n",
      "\u001b[35mname space=1\u001b[0m\n",
      "\u001b[35mstorage space=0\u001b[0m\n",
      "\u001b[35mstorage types=RAM_DISK=0, SSD=0, DISK=0, ARCHIVE=0, PROVIDED=0\u001b[0m\n",
      "\u001b[35mMar 02, 2021 5:26:19 AM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory getComponentProvider\u001b[0m\n",
      "\u001b[34mINFO: Binding org.apache.hadoop.yarn.server.nodemanager.webapp.NMWebServices to GuiceManagedComponentProvider with the scope \"Singleton\"\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:19,386 INFO blockmanagement.CacheReplicationMonitor: Starting CacheReplicationMonitor with interval 30000 milliseconds\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:19,415 INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@164a62bf{node,/,file:///tmp/jetty-algo-1-8042-_-any-1283960859370680251.dir/webapp/,AVAILABLE}{jar:file:/usr/lib/hadoop-yarn/hadoop-yarn-common-3.2.1-amzn-1.jar!/webapps/node}\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:19,434 INFO server.AbstractConnector: Started ServerConnector@2374d36a{HTTP/1.1,[http/1.1]}{algo-1:8042}\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:19,434 INFO server.Server: Started @2764ms\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:19,434 INFO webapp.WebApps: Web app node started at 8042\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:19,441 INFO nodemanager.NodeStatusUpdaterImpl: Node ID assigned is : algo-1:46605\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:19,448 INFO util.JvmPauseMonitor: Starting JVM pause monitor\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:19,453 INFO client.RMProxy: Connecting to ResourceManager at /10.0.189.145:8031\u001b[0m\n",
      "\u001b[34mMar 02, 2021 5:26:19 AM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory getComponentProvider\u001b[0m\n",
      "\u001b[34mINFO: Binding org.apache.hadoop.yarn.server.resourcemanager.webapp.RMWebServices to GuiceManagedComponentProvider with the scope \"Singleton\"\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:19,508 INFO nodemanager.NodeStatusUpdaterImpl: Sending out 0 NM container statuses: []\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:19,521 INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@396e6d9{cluster,/,file:///tmp/jetty-10_0_189_145-8088-_-any-7238770674507701485.dir/webapp/,AVAILABLE}{jar:file:/usr/lib/hadoop-yarn/hadoop-yarn-common-3.2.1-amzn-1.jar!/webapps/cluster}\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:19,530 INFO nodemanager.NodeStatusUpdaterImpl: Registering with RM using containers :[]\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:19,536 INFO server.AbstractConnector: Started ServerConnector@432038ec{HTTP/1.1,[http/1.1]}{10.0.189.145:8088}\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:19,536 INFO server.Server: Started @2857ms\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:19,536 INFO webapp.WebApps: Web app cluster started at 8088\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:19,617 INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 100, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:19,625 INFO ipc.Server: Starting Socket Reader #1 for port 8033\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:19,651 INFO ipc.Client: Retrying connect to server: algo-1/10.0.189.145:8020. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)\u001b[0m\n",
      "\u001b[35mINFO: Binding org.apache.hadoop.yarn.server.nodemanager.webapp.NMWebServices to GuiceManagedComponentProvider with the scope \"Singleton\"\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:19,386 INFO blockmanagement.CacheReplicationMonitor: Starting CacheReplicationMonitor with interval 30000 milliseconds\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:19,415 INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@164a62bf{node,/,file:///tmp/jetty-algo-1-8042-_-any-1283960859370680251.dir/webapp/,AVAILABLE}{jar:file:/usr/lib/hadoop-yarn/hadoop-yarn-common-3.2.1-amzn-1.jar!/webapps/node}\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:19,434 INFO server.AbstractConnector: Started ServerConnector@2374d36a{HTTP/1.1,[http/1.1]}{algo-1:8042}\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:19,434 INFO server.Server: Started @2764ms\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:19,434 INFO webapp.WebApps: Web app node started at 8042\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:19,441 INFO nodemanager.NodeStatusUpdaterImpl: Node ID assigned is : algo-1:46605\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:19,448 INFO util.JvmPauseMonitor: Starting JVM pause monitor\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:19,453 INFO client.RMProxy: Connecting to ResourceManager at /10.0.189.145:8031\u001b[0m\n",
      "\u001b[35mMar 02, 2021 5:26:19 AM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory getComponentProvider\u001b[0m\n",
      "\u001b[35mINFO: Binding org.apache.hadoop.yarn.server.resourcemanager.webapp.RMWebServices to GuiceManagedComponentProvider with the scope \"Singleton\"\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:19,508 INFO nodemanager.NodeStatusUpdaterImpl: Sending out 0 NM container statuses: []\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:19,521 INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@396e6d9{cluster,/,file:///tmp/jetty-10_0_189_145-8088-_-any-7238770674507701485.dir/webapp/,AVAILABLE}{jar:file:/usr/lib/hadoop-yarn/hadoop-yarn-common-3.2.1-amzn-1.jar!/webapps/cluster}\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:19,530 INFO nodemanager.NodeStatusUpdaterImpl: Registering with RM using containers :[]\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:19,536 INFO server.AbstractConnector: Started ServerConnector@432038ec{HTTP/1.1,[http/1.1]}{10.0.189.145:8088}\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:19,536 INFO server.Server: Started @2857ms\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:19,536 INFO webapp.WebApps: Web app cluster started at 8088\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:19,617 INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 100, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:19,625 INFO ipc.Server: Starting Socket Reader #1 for port 8033\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:19,651 INFO ipc.Client: Retrying connect to server: algo-1/10.0.189.145:8020. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:19,754 INFO pb.RpcServerFactoryPBImpl: Adding protocol org.apache.hadoop.yarn.server.api.ResourceManagerAdministrationProtocolPB to the server\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:19,763 INFO ipc.Server: IPC Server Responder: starting\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:19,771 INFO ipc.Server: IPC Server listener on 8033: starting\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:19,780 INFO resourcemanager.ResourceManager: Transitioning to active state\u001b[0m\n",
      "\u001b[34m03-02 05:26 smspark-submit INFO     cluster is up\u001b[0m\n",
      "\u001b[34m03-02 05:26 smspark-submit INFO     transitioning from status BOOTSTRAPPING to WAITING\u001b[0m\n",
      "\u001b[34m03-02 05:26 smspark-submit INFO     starting executor logs watcher\u001b[0m\n",
      "\u001b[34m03-02 05:26 smspark-submit INFO     start log event log publisher\u001b[0m\n",
      "\u001b[34mStarting executor logs watcher on log_dir: /var/log/yarn\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:19,801 INFO recovery.RMStateStore: Updating AMRMToken\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:19,801 INFO security.RMContainerTokenSecretManager: Rolling master-key for container-tokens\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:19,801 INFO security.NMTokenSecretManagerInRM: Rolling master-key for nm-tokens\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:19,802 INFO delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:19,802 INFO security.RMDelegationTokenSecretManager: storing master key with keyID 1\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:19,754 INFO pb.RpcServerFactoryPBImpl: Adding protocol org.apache.hadoop.yarn.server.api.ResourceManagerAdministrationProtocolPB to the server\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:19,763 INFO ipc.Server: IPC Server Responder: starting\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:19,771 INFO ipc.Server: IPC Server listener on 8033: starting\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:19,780 INFO resourcemanager.ResourceManager: Transitioning to active state\u001b[0m\n",
      "\u001b[35m03-02 05:26 smspark-submit INFO     cluster is up\u001b[0m\n",
      "\u001b[35m03-02 05:26 smspark-submit INFO     transitioning from status BOOTSTRAPPING to WAITING\u001b[0m\n",
      "\u001b[35m03-02 05:26 smspark-submit INFO     starting executor logs watcher\u001b[0m\n",
      "\u001b[35m03-02 05:26 smspark-submit INFO     start log event log publisher\u001b[0m\n",
      "\u001b[35mStarting executor logs watcher on log_dir: /var/log/yarn\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:19,801 INFO recovery.RMStateStore: Updating AMRMToken\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:19,801 INFO security.RMContainerTokenSecretManager: Rolling master-key for container-tokens\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:19,801 INFO security.NMTokenSecretManagerInRM: Rolling master-key for nm-tokens\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:19,802 INFO delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:19,802 INFO security.RMDelegationTokenSecretManager: storing master key with keyID 1\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:19,802 INFO recovery.RMStateStore: Storing RMDTMasterKey.\u001b[0m\n",
      "\u001b[34m03-02 05:26 sagemaker-spark-event-logs-publisher INFO     Spark event log not enabled.\u001b[0m\n",
      "\u001b[34m03-02 05:26 smspark-submit INFO     Waiting for hosts to bootstrap: ['algo-1']\u001b[0m\n",
      "\u001b[34m03-02 05:26 smspark-submit INFO     Received host statuses: dict_items([('algo-1', StatusMessage(status='WAITING', timestamp='2021-03-02T05:26:19.810819'))])\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:19,813 INFO delegation.AbstractDelegationTokenSecretManager: Starting expired delegation token remover thread, tokenRemoverScanInterval=60 min(s)\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:19,813 INFO delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:19,813 INFO security.RMDelegationTokenSecretManager: storing master key with keyID 2\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:19,813 INFO recovery.RMStateStore: Storing RMDTMasterKey.\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:19,866 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.nodelabels.event.NodeLabelsStoreEventType for class org.apache.hadoop.yarn.nodelabels.CommonNodeLabelsManager$ForwardingEventHandler\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:20,037 INFO store.AbstractFSNodeStore: Created store directory :file:/tmp/hadoop-yarn-root/node-attribute\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:20,049 INFO store.AbstractFSNodeStore: Finished write mirror at:file:/tmp/hadoop-yarn-root/node-attribute/nodeattribute.mirror\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:20,049 INFO store.AbstractFSNodeStore: Finished create editlog file at:file:/tmp/hadoop-yarn-root/node-attribute/nodeattribute.editlog\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:20,060 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.nodelabels.NodeAttributesStoreEventType for class org.apache.hadoop.yarn.server.resourcemanager.nodelabels.NodeAttributesManagerImpl$ForwardingEventHandler\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:20,060 INFO placement.MultiNodeSortingManager: Starting NodeSortingService=MultiNodeSortingManager\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:20,069 INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 5000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:20,070 INFO ipc.Server: Starting Socket Reader #1 for port 8031\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:20,071 INFO pb.RpcServerFactoryPBImpl: Adding protocol org.apache.hadoop.yarn.server.api.ResourceTrackerPB to the server\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:20,072 INFO ipc.Server: IPC Server Responder: starting\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:20,072 INFO ipc.Server: IPC Server listener on 8031: starting\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:20,086 INFO datanode.DataNode: Acknowledging ACTIVE Namenode during handshakeBlock pool <registering> (Datanode Uuid unassigned) service to algo-1/10.0.189.145:8020\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:20,088 INFO common.Storage: Using 1 threads to upgrade data directories (dfs.datanode.parallel.volumes.load.threads.num=1, dataDirs=1)\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:20,089 INFO util.JvmPauseMonitor: Starting JVM pause monitor\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:20,093 INFO common.Storage: Lock on /opt/amazon/hadoop/hdfs/datanode/in_use.lock acquired by nodename 139@algo-1\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:20,094 INFO common.Storage: Storage directory with location [DISK]file:/opt/amazon/hadoop/hdfs/datanode is not formatted for namespace 810777946. Formatting...\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:20,095 INFO common.Storage: Generated new storageID DS-2c74b07d-cf4c-42aa-8592-68c3f03002e6 for directory /opt/amazon/hadoop/hdfs/datanode \u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:20,097 INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 5000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:20,101 INFO ipc.Server: Starting Socket Reader #1 for port 8030\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:20,106 INFO pb.RpcServerFactoryPBImpl: Adding protocol org.apache.hadoop.yarn.api.ApplicationMasterProtocolPB to the server\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:20,107 INFO ipc.Server: IPC Server Responder: starting\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:20,110 INFO ipc.Server: IPC Server listener on 8030: starting\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:20,117 INFO common.Storage: Analyzing storage directories for bpid BP-2073241018-10.0.189.145-1614662776271\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:20,117 INFO common.Storage: Locking is disabled for /opt/amazon/hadoop/hdfs/datanode/current/BP-2073241018-10.0.189.145-1614662776271\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:20,117 INFO common.Storage: Block pool storage directory for location [DISK]file:/opt/amazon/hadoop/hdfs/datanode and block pool id BP-2073241018-10.0.189.145-1614662776271 is not formatted. Formatting ...\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:20,117 INFO common.Storage: Formatting block pool BP-2073241018-10.0.189.145-1614662776271 directory /opt/amazon/hadoop/hdfs/datanode/current/BP-2073241018-10.0.189.145-1614662776271/current\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:20,123 INFO datanode.DataNode: Setting up storage: nsid=810777946;bpid=BP-2073241018-10.0.189.145-1614662776271;lv=-57;nsInfo=lv=-65;cid=CID-82b1b13a-dc26-466f-a555-0a30873ad261;nsid=810777946;c=1614662776271;bpid=BP-2073241018-10.0.189.145-1614662776271;dnuuid=null\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:20,126 INFO datanode.DataNode: Generated and persisted new Datanode UUID 23cb743c-3799-42f3-adda-d5642c6fe121\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:20,198 INFO impl.FsDatasetImpl: Added new volume: DS-2c74b07d-cf4c-42aa-8592-68c3f03002e6\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:20,199 INFO impl.FsDatasetImpl: Added volume - [DISK]file:/opt/amazon/hadoop/hdfs/datanode, StorageType: DISK\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:20,200 INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 5000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:20,201 INFO ipc.Server: Starting Socket Reader #1 for port 8032\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:20,202 INFO impl.FsDatasetImpl: Registered FSDatasetState MBean\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:20,204 INFO pb.RpcServerFactoryPBImpl: Adding protocol org.apache.hadoop.yarn.api.ApplicationClientProtocolPB to the server\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:20,207 INFO ipc.Server: IPC Server Responder: starting\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:20,207 INFO ipc.Server: IPC Server listener on 8032: starting\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:20,208 INFO checker.ThrottledAsyncChecker: Scheduling a check for /opt/amazon/hadoop/hdfs/datanode\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:20,217 INFO checker.DatasetVolumeChecker: Scheduled health check for volume /opt/amazon/hadoop/hdfs/datanode\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:20,218 INFO impl.FsDatasetImpl: Adding block pool BP-2073241018-10.0.189.145-1614662776271\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:20,219 INFO impl.FsDatasetImpl: Scanning block pool BP-2073241018-10.0.189.145-1614662776271 on volume /opt/amazon/hadoop/hdfs/datanode...\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:20,238 INFO resourcemanager.ResourceManager: Transitioned to active state\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:19,802 INFO recovery.RMStateStore: Storing RMDTMasterKey.\u001b[0m\n",
      "\u001b[35m03-02 05:26 sagemaker-spark-event-logs-publisher INFO     Spark event log not enabled.\u001b[0m\n",
      "\u001b[35m03-02 05:26 smspark-submit INFO     Waiting for hosts to bootstrap: ['algo-1']\u001b[0m\n",
      "\u001b[35m03-02 05:26 smspark-submit INFO     Received host statuses: dict_items([('algo-1', StatusMessage(status='WAITING', timestamp='2021-03-02T05:26:19.810819'))])\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:19,813 INFO delegation.AbstractDelegationTokenSecretManager: Starting expired delegation token remover thread, tokenRemoverScanInterval=60 min(s)\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:19,813 INFO delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:19,813 INFO security.RMDelegationTokenSecretManager: storing master key with keyID 2\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:19,813 INFO recovery.RMStateStore: Storing RMDTMasterKey.\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:19,866 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.nodelabels.event.NodeLabelsStoreEventType for class org.apache.hadoop.yarn.nodelabels.CommonNodeLabelsManager$ForwardingEventHandler\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:20,037 INFO store.AbstractFSNodeStore: Created store directory :file:/tmp/hadoop-yarn-root/node-attribute\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:20,049 INFO store.AbstractFSNodeStore: Finished write mirror at:file:/tmp/hadoop-yarn-root/node-attribute/nodeattribute.mirror\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:20,049 INFO store.AbstractFSNodeStore: Finished create editlog file at:file:/tmp/hadoop-yarn-root/node-attribute/nodeattribute.editlog\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:20,060 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.nodelabels.NodeAttributesStoreEventType for class org.apache.hadoop.yarn.server.resourcemanager.nodelabels.NodeAttributesManagerImpl$ForwardingEventHandler\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:20,060 INFO placement.MultiNodeSortingManager: Starting NodeSortingService=MultiNodeSortingManager\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:20,069 INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 5000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:20,070 INFO ipc.Server: Starting Socket Reader #1 for port 8031\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:20,071 INFO pb.RpcServerFactoryPBImpl: Adding protocol org.apache.hadoop.yarn.server.api.ResourceTrackerPB to the server\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:20,072 INFO ipc.Server: IPC Server Responder: starting\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:20,072 INFO ipc.Server: IPC Server listener on 8031: starting\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:20,086 INFO datanode.DataNode: Acknowledging ACTIVE Namenode during handshakeBlock pool <registering> (Datanode Uuid unassigned) service to algo-1/10.0.189.145:8020\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:20,088 INFO common.Storage: Using 1 threads to upgrade data directories (dfs.datanode.parallel.volumes.load.threads.num=1, dataDirs=1)\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:20,089 INFO util.JvmPauseMonitor: Starting JVM pause monitor\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:20,093 INFO common.Storage: Lock on /opt/amazon/hadoop/hdfs/datanode/in_use.lock acquired by nodename 139@algo-1\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:20,094 INFO common.Storage: Storage directory with location [DISK]file:/opt/amazon/hadoop/hdfs/datanode is not formatted for namespace 810777946. Formatting...\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:20,095 INFO common.Storage: Generated new storageID DS-2c74b07d-cf4c-42aa-8592-68c3f03002e6 for directory /opt/amazon/hadoop/hdfs/datanode \u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:20,097 INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 5000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:20,101 INFO ipc.Server: Starting Socket Reader #1 for port 8030\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:20,106 INFO pb.RpcServerFactoryPBImpl: Adding protocol org.apache.hadoop.yarn.api.ApplicationMasterProtocolPB to the server\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:20,107 INFO ipc.Server: IPC Server Responder: starting\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:20,110 INFO ipc.Server: IPC Server listener on 8030: starting\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:20,117 INFO common.Storage: Analyzing storage directories for bpid BP-2073241018-10.0.189.145-1614662776271\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:20,117 INFO common.Storage: Locking is disabled for /opt/amazon/hadoop/hdfs/datanode/current/BP-2073241018-10.0.189.145-1614662776271\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:20,117 INFO common.Storage: Block pool storage directory for location [DISK]file:/opt/amazon/hadoop/hdfs/datanode and block pool id BP-2073241018-10.0.189.145-1614662776271 is not formatted. Formatting ...\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:20,117 INFO common.Storage: Formatting block pool BP-2073241018-10.0.189.145-1614662776271 directory /opt/amazon/hadoop/hdfs/datanode/current/BP-2073241018-10.0.189.145-1614662776271/current\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:20,123 INFO datanode.DataNode: Setting up storage: nsid=810777946;bpid=BP-2073241018-10.0.189.145-1614662776271;lv=-57;nsInfo=lv=-65;cid=CID-82b1b13a-dc26-466f-a555-0a30873ad261;nsid=810777946;c=1614662776271;bpid=BP-2073241018-10.0.189.145-1614662776271;dnuuid=null\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:20,126 INFO datanode.DataNode: Generated and persisted new Datanode UUID 23cb743c-3799-42f3-adda-d5642c6fe121\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:20,198 INFO impl.FsDatasetImpl: Added new volume: DS-2c74b07d-cf4c-42aa-8592-68c3f03002e6\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:20,199 INFO impl.FsDatasetImpl: Added volume - [DISK]file:/opt/amazon/hadoop/hdfs/datanode, StorageType: DISK\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:20,200 INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 5000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:20,201 INFO ipc.Server: Starting Socket Reader #1 for port 8032\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:20,202 INFO impl.FsDatasetImpl: Registered FSDatasetState MBean\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:20,204 INFO pb.RpcServerFactoryPBImpl: Adding protocol org.apache.hadoop.yarn.api.ApplicationClientProtocolPB to the server\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:20,207 INFO ipc.Server: IPC Server Responder: starting\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:20,207 INFO ipc.Server: IPC Server listener on 8032: starting\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:20,208 INFO checker.ThrottledAsyncChecker: Scheduling a check for /opt/amazon/hadoop/hdfs/datanode\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:20,217 INFO checker.DatasetVolumeChecker: Scheduled health check for volume /opt/amazon/hadoop/hdfs/datanode\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:20,218 INFO impl.FsDatasetImpl: Adding block pool BP-2073241018-10.0.189.145-1614662776271\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:20,219 INFO impl.FsDatasetImpl: Scanning block pool BP-2073241018-10.0.189.145-1614662776271 on volume /opt/amazon/hadoop/hdfs/datanode...\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:20,238 INFO resourcemanager.ResourceManager: Transitioned to active state\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:20,243 INFO impl.FsDatasetImpl: Time taken to scan block pool BP-2073241018-10.0.189.145-1614662776271 on /opt/amazon/hadoop/hdfs/datanode: 24ms\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:20,243 INFO impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-2073241018-10.0.189.145-1614662776271: 26ms\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:20,246 INFO impl.FsDatasetImpl: Adding replicas to map for block pool BP-2073241018-10.0.189.145-1614662776271 on volume /opt/amazon/hadoop/hdfs/datanode...\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:20,246 INFO impl.BlockPoolSlice: Replica Cache file: /opt/amazon/hadoop/hdfs/datanode/current/BP-2073241018-10.0.189.145-1614662776271/current/replicas doesn't exist \u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:20,248 INFO impl.FsDatasetImpl: Time to add replicas to map for block pool BP-2073241018-10.0.189.145-1614662776271 on volume /opt/amazon/hadoop/hdfs/datanode: 2ms\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:20,248 INFO impl.FsDatasetImpl: Total time to add all replicas to map for block pool BP-2073241018-10.0.189.145-1614662776271: 3ms\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:20,249 INFO datanode.VolumeScanner: Now scanning bpid BP-2073241018-10.0.189.145-1614662776271 on volume /opt/amazon/hadoop/hdfs/datanode\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:20,251 INFO datanode.VolumeScanner: VolumeScanner(/opt/amazon/hadoop/hdfs/datanode, DS-2c74b07d-cf4c-42aa-8592-68c3f03002e6): finished scanning block pool BP-2073241018-10.0.189.145-1614662776271\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:20,262 INFO datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 3/2/21 11:15 AM with interval of 21600000ms\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:20,264 INFO datanode.VolumeScanner: VolumeScanner(/opt/amazon/hadoop/hdfs/datanode, DS-2c74b07d-cf4c-42aa-8592-68c3f03002e6): no suitable block pools found to scan.  Waiting 1814399985 ms.\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:20,268 INFO datanode.DataNode: Block pool BP-2073241018-10.0.189.145-1614662776271 (Datanode Uuid 23cb743c-3799-42f3-adda-d5642c6fe121) service to algo-1/10.0.189.145:8020 beginning handshake with NN\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:20,299 INFO hdfs.StateChange: BLOCK* registerDatanode: from DatanodeRegistration(10.0.189.145:9866, datanodeUuid=23cb743c-3799-42f3-adda-d5642c6fe121, infoPort=9864, infoSecurePort=0, ipcPort=9867, storageInfo=lv=-57;cid=CID-82b1b13a-dc26-466f-a555-0a30873ad261;nsid=810777946;c=1614662776271) storage 23cb743c-3799-42f3-adda-d5642c6fe121\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:20,300 INFO net.NetworkTopology: Adding a new node: /default-rack/10.0.189.145:9866\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:20,300 INFO blockmanagement.BlockReportLeaseManager: Registered DN 23cb743c-3799-42f3-adda-d5642c6fe121 (10.0.189.145:9866).\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:20,309 INFO datanode.DataNode: Block pool Block pool BP-2073241018-10.0.189.145-1614662776271 (Datanode Uuid 23cb743c-3799-42f3-adda-d5642c6fe121) service to algo-1/10.0.189.145:8020 successfully registered with NN\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:20,309 INFO datanode.DataNode: For namenode algo-1/10.0.189.145:8020 using BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:20,370 INFO blockmanagement.DatanodeDescriptor: Adding new storage ID DS-2c74b07d-cf4c-42aa-8592-68c3f03002e6 for DN 10.0.189.145:9866\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:20,446 INFO BlockStateChange: BLOCK* processReport 0x9fca81be42b50879: Processing first storage report for DS-2c74b07d-cf4c-42aa-8592-68c3f03002e6 from datanode 23cb743c-3799-42f3-adda-d5642c6fe121\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:20,447 INFO BlockStateChange: BLOCK* processReport 0x9fca81be42b50879: from storage DS-2c74b07d-cf4c-42aa-8592-68c3f03002e6 node DatanodeRegistration(10.0.189.145:9866, datanodeUuid=23cb743c-3799-42f3-adda-d5642c6fe121, infoPort=9864, infoSecurePort=0, ipcPort=9867, storageInfo=lv=-57;cid=CID-82b1b13a-dc26-466f-a555-0a30873ad261;nsid=810777946;c=1614662776271), blocks: 0, hasStaleStorage: false, processing time: 1 msecs, invalidatedBlocks: 0\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:20,484 INFO datanode.DataNode: Successfully sent block report 0x9fca81be42b50879,  containing 1 storage report(s), of which we sent 1. The reports had 0 total blocks and used 1 RPC(s). This took 3 msec to generate and 90 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:20,484 INFO datanode.DataNode: Got finalize command for block pool BP-2073241018-10.0.189.145-1614662776271\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:20,650 INFO ipc.Client: Retrying connect to server: algo-1/10.0.189.145:8031. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:20,243 INFO impl.FsDatasetImpl: Time taken to scan block pool BP-2073241018-10.0.189.145-1614662776271 on /opt/amazon/hadoop/hdfs/datanode: 24ms\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:20,243 INFO impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-2073241018-10.0.189.145-1614662776271: 26ms\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:20,246 INFO impl.FsDatasetImpl: Adding replicas to map for block pool BP-2073241018-10.0.189.145-1614662776271 on volume /opt/amazon/hadoop/hdfs/datanode...\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:20,246 INFO impl.BlockPoolSlice: Replica Cache file: /opt/amazon/hadoop/hdfs/datanode/current/BP-2073241018-10.0.189.145-1614662776271/current/replicas doesn't exist \u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:20,248 INFO impl.FsDatasetImpl: Time to add replicas to map for block pool BP-2073241018-10.0.189.145-1614662776271 on volume /opt/amazon/hadoop/hdfs/datanode: 2ms\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:20,248 INFO impl.FsDatasetImpl: Total time to add all replicas to map for block pool BP-2073241018-10.0.189.145-1614662776271: 3ms\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:20,249 INFO datanode.VolumeScanner: Now scanning bpid BP-2073241018-10.0.189.145-1614662776271 on volume /opt/amazon/hadoop/hdfs/datanode\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:20,251 INFO datanode.VolumeScanner: VolumeScanner(/opt/amazon/hadoop/hdfs/datanode, DS-2c74b07d-cf4c-42aa-8592-68c3f03002e6): finished scanning block pool BP-2073241018-10.0.189.145-1614662776271\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:20,262 INFO datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 3/2/21 11:15 AM with interval of 21600000ms\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:20,264 INFO datanode.VolumeScanner: VolumeScanner(/opt/amazon/hadoop/hdfs/datanode, DS-2c74b07d-cf4c-42aa-8592-68c3f03002e6): no suitable block pools found to scan.  Waiting 1814399985 ms.\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:20,268 INFO datanode.DataNode: Block pool BP-2073241018-10.0.189.145-1614662776271 (Datanode Uuid 23cb743c-3799-42f3-adda-d5642c6fe121) service to algo-1/10.0.189.145:8020 beginning handshake with NN\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:20,299 INFO hdfs.StateChange: BLOCK* registerDatanode: from DatanodeRegistration(10.0.189.145:9866, datanodeUuid=23cb743c-3799-42f3-adda-d5642c6fe121, infoPort=9864, infoSecurePort=0, ipcPort=9867, storageInfo=lv=-57;cid=CID-82b1b13a-dc26-466f-a555-0a30873ad261;nsid=810777946;c=1614662776271) storage 23cb743c-3799-42f3-adda-d5642c6fe121\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:20,300 INFO net.NetworkTopology: Adding a new node: /default-rack/10.0.189.145:9866\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:20,300 INFO blockmanagement.BlockReportLeaseManager: Registered DN 23cb743c-3799-42f3-adda-d5642c6fe121 (10.0.189.145:9866).\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:20,309 INFO datanode.DataNode: Block pool Block pool BP-2073241018-10.0.189.145-1614662776271 (Datanode Uuid 23cb743c-3799-42f3-adda-d5642c6fe121) service to algo-1/10.0.189.145:8020 successfully registered with NN\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:20,309 INFO datanode.DataNode: For namenode algo-1/10.0.189.145:8020 using BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:20,370 INFO blockmanagement.DatanodeDescriptor: Adding new storage ID DS-2c74b07d-cf4c-42aa-8592-68c3f03002e6 for DN 10.0.189.145:9866\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:20,446 INFO BlockStateChange: BLOCK* processReport 0x9fca81be42b50879: Processing first storage report for DS-2c74b07d-cf4c-42aa-8592-68c3f03002e6 from datanode 23cb743c-3799-42f3-adda-d5642c6fe121\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:20,447 INFO BlockStateChange: BLOCK* processReport 0x9fca81be42b50879: from storage DS-2c74b07d-cf4c-42aa-8592-68c3f03002e6 node DatanodeRegistration(10.0.189.145:9866, datanodeUuid=23cb743c-3799-42f3-adda-d5642c6fe121, infoPort=9864, infoSecurePort=0, ipcPort=9867, storageInfo=lv=-57;cid=CID-82b1b13a-dc26-466f-a555-0a30873ad261;nsid=810777946;c=1614662776271), blocks: 0, hasStaleStorage: false, processing time: 1 msecs, invalidatedBlocks: 0\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:20,484 INFO datanode.DataNode: Successfully sent block report 0x9fca81be42b50879,  containing 1 storage report(s), of which we sent 1. The reports had 0 total blocks and used 1 RPC(s). This took 3 msec to generate and 90 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:20,484 INFO datanode.DataNode: Got finalize command for block pool BP-2073241018-10.0.189.145-1614662776271\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:20,650 INFO ipc.Client: Retrying connect to server: algo-1/10.0.189.145:8031. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:23,510 INFO spark.SparkEnv: Registering MapOutputTracker\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:23,544 INFO spark.SparkEnv: Registering BlockManagerMaster\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:23,571 INFO storage.BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:23,571 INFO storage.BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:23,604 INFO spark.SparkEnv: Registering BlockManagerMasterHeartbeat\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:23,618 INFO storage.DiskBlockManager: Created local directory at /tmp/blockmgr-5d54647d-1211-47bb-bbc2-a0ceea48e154\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:23,510 INFO spark.SparkEnv: Registering MapOutputTracker\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:23,544 INFO spark.SparkEnv: Registering BlockManagerMaster\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:23,571 INFO storage.BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:23,571 INFO storage.BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:23,604 INFO spark.SparkEnv: Registering BlockManagerMasterHeartbeat\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:23,618 INFO storage.DiskBlockManager: Created local directory at /tmp/blockmgr-5d54647d-1211-47bb-bbc2-a0ceea48e154\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:23,640 INFO memory.MemoryStore: MemoryStore started with capacity 1007.8 MiB\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:23,683 INFO spark.SparkEnv: Registering OutputCommitCoordinator\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:23,640 INFO memory.MemoryStore: MemoryStore started with capacity 1007.8 MiB\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:23,683 INFO spark.SparkEnv: Registering OutputCommitCoordinator\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:23,786 INFO util.log: Logging initialized @3563ms to org.sparkproject.jetty.util.log.Slf4jLog\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:23,887 INFO server.Server: jetty-9.4.20.v20190813; built: 2019-08-13T21:28:18.144Z; git: 84700530e645e812b336747464d6fbbf370c9a20; jvm 1.8.0_272-b10\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:23,910 INFO server.Server: Started @3688ms\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:23,943 INFO server.AbstractConnector: Started ServerConnector@715c241{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:23,943 INFO util.Utils: Successfully started service 'SparkUI' on port 4040.\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:23,966 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3dda3f7c{/jobs,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:23,969 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@449e88{/jobs/json,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:23,969 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@630da388{/jobs/job,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:23,970 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2274bd09{/jobs/job/json,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:23,971 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4d5fd0b{/stages,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:23,971 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2ceb0719{/stages/json,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:23,972 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@71398878{/stages/stage,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:23,973 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@328c586c{/stages/stage/json,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:23,974 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@7e58c9aa{/stages/pool,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:23,974 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@7a05e110{/stages/pool/json,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:23,975 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3957b7f6{/storage,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:23,976 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1273ede4{/storage/json,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:23,976 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@466ad69a{/storage/rdd,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:23,977 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5000edb5{/storage/rdd/json,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:23,977 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@33b2dd14{/environment,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:23,978 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@30f48092{/environment/json,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:23,978 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@56bbf5d3{/executors,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:23,979 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@502781e9{/executors/json,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:23,980 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@c4c9de7{/executors/threadDump,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:23,981 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3f5b8aaf{/executors/threadDump/json,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:23,989 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@55ed1e2c{/static,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:23,989 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6738bc67{/,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:23,991 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4e52dbfa{/api,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:23,991 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5d708069{/jobs/job/kill,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:23,992 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@75f84709{/stages/stage/kill,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:23,993 INFO ui.SparkUI: Bound SparkUI to 0.0.0.0, and started at http://10.0.189.145:4040\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:23,786 INFO util.log: Logging initialized @3563ms to org.sparkproject.jetty.util.log.Slf4jLog\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:23,887 INFO server.Server: jetty-9.4.20.v20190813; built: 2019-08-13T21:28:18.144Z; git: 84700530e645e812b336747464d6fbbf370c9a20; jvm 1.8.0_272-b10\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:23,910 INFO server.Server: Started @3688ms\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:23,943 INFO server.AbstractConnector: Started ServerConnector@715c241{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:23,943 INFO util.Utils: Successfully started service 'SparkUI' on port 4040.\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:23,966 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3dda3f7c{/jobs,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:23,969 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@449e88{/jobs/json,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:23,969 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@630da388{/jobs/job,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:23,970 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2274bd09{/jobs/job/json,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:23,971 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4d5fd0b{/stages,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:23,971 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2ceb0719{/stages/json,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:23,972 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@71398878{/stages/stage,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:23,973 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@328c586c{/stages/stage/json,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:23,974 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@7e58c9aa{/stages/pool,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:23,974 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@7a05e110{/stages/pool/json,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:23,975 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3957b7f6{/storage,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:23,976 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1273ede4{/storage/json,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:23,976 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@466ad69a{/storage/rdd,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:23,977 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5000edb5{/storage/rdd/json,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:23,977 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@33b2dd14{/environment,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:23,978 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@30f48092{/environment/json,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:23,978 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@56bbf5d3{/executors,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:23,979 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@502781e9{/executors/json,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:23,980 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@c4c9de7{/executors/threadDump,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:23,981 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3f5b8aaf{/executors/threadDump/json,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:23,989 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@55ed1e2c{/static,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:23,989 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6738bc67{/,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:23,991 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4e52dbfa{/api,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:23,991 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5d708069{/jobs/job/kill,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:23,992 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@75f84709{/stages/stage/kill,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:23,993 INFO ui.SparkUI: Bound SparkUI to 0.0.0.0, and started at http://10.0.189.145:4040\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:24,236 INFO client.RMProxy: Connecting to ResourceManager at /10.0.189.145:8032\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:24,517 INFO yarn.Client: Requesting a new application from cluster with 1 NodeManagers\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:24,538 INFO resourcemanager.ClientRMService: Allocated new applicationId: 1\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:24,671 INFO conf.Configuration: resource-types.xml not found\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:24,671 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:24,689 INFO yarn.Client: Verifying our application has not requested more than the maximum memory capability of the cluster (63569 MB per container)\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:24,690 INFO yarn.Client: Will allocate AM container, with 896 MB memory including 384 MB overhead\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:24,690 INFO yarn.Client: Setting up container launch context for our AM\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:24,692 INFO yarn.Client: Setting up the launch environment for our AM container\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:24,698 INFO yarn.Client: Preparing resources for our AM container\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:24,718 WARN yarn.Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:24,236 INFO client.RMProxy: Connecting to ResourceManager at /10.0.189.145:8032\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:24,517 INFO yarn.Client: Requesting a new application from cluster with 1 NodeManagers\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:24,538 INFO resourcemanager.ClientRMService: Allocated new applicationId: 1\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:24,671 INFO conf.Configuration: resource-types.xml not found\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:24,671 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:24,689 INFO yarn.Client: Verifying our application has not requested more than the maximum memory capability of the cluster (63569 MB per container)\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:24,690 INFO yarn.Client: Will allocate AM container, with 896 MB memory including 384 MB overhead\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:24,690 INFO yarn.Client: Setting up container launch context for our AM\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:24,692 INFO yarn.Client: Setting up the launch environment for our AM container\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:24,698 INFO yarn.Client: Preparing resources for our AM container\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:24,718 WARN yarn.Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:27,910 INFO yarn.Client: Uploading resource file:/tmp/spark-a9207a71-c796-421d-9264-f0207b72ca3a/__spark_libs__7663445419400196768.zip -> file:/root/.sparkStaging/application_1614662779782_0001/__spark_libs__7663445419400196768.zip\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:27,910 INFO yarn.Client: Uploading resource file:/tmp/spark-a9207a71-c796-421d-9264-f0207b72ca3a/__spark_libs__7663445419400196768.zip -> file:/root/.sparkStaging/application_1614662779782_0001/__spark_libs__7663445419400196768.zip\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:29,841 INFO yarn.Client: Uploading resource file:/usr/lib/spark/python/lib/pyspark.zip -> file:/root/.sparkStaging/application_1614662779782_0001/pyspark.zip\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:29,847 INFO yarn.Client: Uploading resource file:/usr/lib/spark/python/lib/py4j-0.10.9-src.zip -> file:/root/.sparkStaging/application_1614662779782_0001/py4j-0.10.9-src.zip\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:29,944 INFO yarn.Client: Uploading resource file:/tmp/spark-a9207a71-c796-421d-9264-f0207b72ca3a/__spark_conf__3465334766446045071.zip -> file:/root/.sparkStaging/application_1614662779782_0001/__spark_conf__.zip\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:29,961 INFO spark.SecurityManager: Changing view acls to: root\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:29,961 INFO spark.SecurityManager: Changing modify acls to: root\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:29,961 INFO spark.SecurityManager: Changing view acls groups to: \u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:29,961 INFO spark.SecurityManager: Changing modify acls groups to: \u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:29,961 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:29,984 INFO yarn.Client: Submitting application application_1614662779782_0001 to ResourceManager\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:30,069 INFO capacity.CapacityScheduler: Application 'application_1614662779782_0001' is submitted without priority hence considering default queue/cluster priority: 0\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:30,069 INFO capacity.CapacityScheduler: Priority '0' is acceptable in queue : default for application: application_1614662779782_0001\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:30,085 WARN rmapp.RMAppImpl: The specific max attempts: 0 for application: 1 is invalid, because it is out of the range [1, 1]. Use the global max attempts instead.\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:30,087 INFO resourcemanager.ClientRMService: Application with id 1 submitted by user root\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:30,087 INFO rmapp.RMAppImpl: Storing application with id application_1614662779782_0001\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:30,089 INFO resourcemanager.RMAuditLogger: USER=root#011IP=10.0.189.145#011OPERATION=Submit Application Request#011TARGET=ClientRMService#011RESULT=SUCCESS#011APPID=application_1614662779782_0001#011QUEUENAME=default\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:29,841 INFO yarn.Client: Uploading resource file:/usr/lib/spark/python/lib/pyspark.zip -> file:/root/.sparkStaging/application_1614662779782_0001/pyspark.zip\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:29,847 INFO yarn.Client: Uploading resource file:/usr/lib/spark/python/lib/py4j-0.10.9-src.zip -> file:/root/.sparkStaging/application_1614662779782_0001/py4j-0.10.9-src.zip\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:29,944 INFO yarn.Client: Uploading resource file:/tmp/spark-a9207a71-c796-421d-9264-f0207b72ca3a/__spark_conf__3465334766446045071.zip -> file:/root/.sparkStaging/application_1614662779782_0001/__spark_conf__.zip\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:29,961 INFO spark.SecurityManager: Changing view acls to: root\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:29,961 INFO spark.SecurityManager: Changing modify acls to: root\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:29,961 INFO spark.SecurityManager: Changing view acls groups to: \u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:29,961 INFO spark.SecurityManager: Changing modify acls groups to: \u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:29,961 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:29,984 INFO yarn.Client: Submitting application application_1614662779782_0001 to ResourceManager\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:30,069 INFO capacity.CapacityScheduler: Application 'application_1614662779782_0001' is submitted without priority hence considering default queue/cluster priority: 0\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:30,069 INFO capacity.CapacityScheduler: Priority '0' is acceptable in queue : default for application: application_1614662779782_0001\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:30,085 WARN rmapp.RMAppImpl: The specific max attempts: 0 for application: 1 is invalid, because it is out of the range [1, 1]. Use the global max attempts instead.\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:30,087 INFO resourcemanager.ClientRMService: Application with id 1 submitted by user root\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:30,087 INFO rmapp.RMAppImpl: Storing application with id application_1614662779782_0001\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:30,089 INFO resourcemanager.RMAuditLogger: USER=root#011IP=10.0.189.145#011OPERATION=Submit Application Request#011TARGET=ClientRMService#011RESULT=SUCCESS#011APPID=application_1614662779782_0001#011QUEUENAME=default\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:30,096 INFO recovery.RMStateStore: Storing info for app: application_1614662779782_0001\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:30,096 INFO rmapp.RMAppImpl: application_1614662779782_0001 State change from NEW to NEW_SAVING on event = START\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:30,096 INFO rmapp.RMAppImpl: application_1614662779782_0001 State change from NEW_SAVING to SUBMITTED on event = APP_NEW_SAVED\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:30,098 INFO capacity.ParentQueue: Application added - appId: application_1614662779782_0001 user: root leaf-queue of parent: root #applications: 1\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:30,098 INFO capacity.CapacityScheduler: Accepted application application_1614662779782_0001 from user: root, in queue: default\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:30,110 INFO rmapp.RMAppImpl: application_1614662779782_0001 State change from SUBMITTED to ACCEPTED on event = APP_ACCEPTED\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:30,137 INFO resourcemanager.ApplicationMasterService: Registering app attempt : appattempt_1614662779782_0001_000001\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:30,138 INFO attempt.RMAppAttemptImpl: appattempt_1614662779782_0001_000001 State change from NEW to SUBMITTED on event = START\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:30,183 INFO capacity.LeafQueue: Application application_1614662779782_0001 from user: root activated in queue: default\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:30,183 INFO capacity.LeafQueue: Application added - appId: application_1614662779782_0001 user: root, leaf-queue: default #user-pending-applications: 0 #user-active-applications: 1 #queue-pending-applications: 0 #queue-active-applications: 1\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:30,183 INFO capacity.CapacityScheduler: Added Application Attempt appattempt_1614662779782_0001_000001 to scheduler from user root in queue default\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:30,190 INFO impl.YarnClientImpl: Submitted application application_1614662779782_0001\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:30,190 INFO attempt.RMAppAttemptImpl: appattempt_1614662779782_0001_000001 State change from SUBMITTED to SCHEDULED on event = ATTEMPT_ADDED\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:30,096 INFO recovery.RMStateStore: Storing info for app: application_1614662779782_0001\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:30,096 INFO rmapp.RMAppImpl: application_1614662779782_0001 State change from NEW to NEW_SAVING on event = START\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:30,096 INFO rmapp.RMAppImpl: application_1614662779782_0001 State change from NEW_SAVING to SUBMITTED on event = APP_NEW_SAVED\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:30,098 INFO capacity.ParentQueue: Application added - appId: application_1614662779782_0001 user: root leaf-queue of parent: root #applications: 1\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:30,098 INFO capacity.CapacityScheduler: Accepted application application_1614662779782_0001 from user: root, in queue: default\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:30,110 INFO rmapp.RMAppImpl: application_1614662779782_0001 State change from SUBMITTED to ACCEPTED on event = APP_ACCEPTED\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:30,137 INFO resourcemanager.ApplicationMasterService: Registering app attempt : appattempt_1614662779782_0001_000001\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:30,138 INFO attempt.RMAppAttemptImpl: appattempt_1614662779782_0001_000001 State change from NEW to SUBMITTED on event = START\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:30,183 INFO capacity.LeafQueue: Application application_1614662779782_0001 from user: root activated in queue: default\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:30,183 INFO capacity.LeafQueue: Application added - appId: application_1614662779782_0001 user: root, leaf-queue: default #user-pending-applications: 0 #user-active-applications: 1 #queue-pending-applications: 0 #queue-active-applications: 1\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:30,183 INFO capacity.CapacityScheduler: Added Application Attempt appattempt_1614662779782_0001_000001 to scheduler from user root in queue default\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:30,190 INFO impl.YarnClientImpl: Submitted application application_1614662779782_0001\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:30,190 INFO attempt.RMAppAttemptImpl: appattempt_1614662779782_0001_000001 State change from SUBMITTED to SCHEDULED on event = ATTEMPT_ADDED\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:30,911 INFO allocator.AbstractContainerAllocator: assignedContainer application attempt=appattempt_1614662779782_0001_000001 container=null queue=default clusterResource=<memory:63569, vCores:16> type=OFF_SWITCH requestedPartition=\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:30,915 INFO rmcontainer.RMContainerImpl: container_1614662779782_0001_01_000001 Container Transitioned from NEW to ALLOCATED\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:30,916 INFO fica.FiCaSchedulerNode: Assigned container container_1614662779782_0001_01_000001 of capacity <memory:896, vCores:1> on host algo-1:46605, which has 1 containers, <memory:896, vCores:1> used and <memory:62673, vCores:15> available after allocation\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:30,916 INFO resourcemanager.RMAuditLogger: USER=root#011OPERATION=AM Allocated Container#011TARGET=SchedulerApp#011RESULT=SUCCESS#011APPID=application_1614662779782_0001#011CONTAINERID=container_1614662779782_0001_01_000001#011RESOURCE=<memory:896, vCores:1>#011QUEUENAME=default\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:30,933 INFO security.NMTokenSecretManagerInRM: Sending NMToken for nodeId : algo-1:46605 for container : container_1614662779782_0001_01_000001\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:30,941 INFO rmcontainer.RMContainerImpl: container_1614662779782_0001_01_000001 Container Transitioned from ALLOCATED to ACQUIRED\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:30,941 INFO security.NMTokenSecretManagerInRM: Clear node set for appattempt_1614662779782_0001_000001\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:30,941 INFO capacity.ParentQueue: assignedContainer queue=root usedCapacity=0.014094921 absoluteUsedCapacity=0.014094921 used=<memory:896, vCores:1> cluster=<memory:63569, vCores:16>\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:30,941 INFO capacity.CapacityScheduler: Allocation proposal accepted\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:30,941 INFO attempt.RMAppAttemptImpl: Storing attempt: AppId: application_1614662779782_0001 AttemptId: appattempt_1614662779782_0001_000001 MasterContainer: Container: [ContainerId: container_1614662779782_0001_01_000001, AllocationRequestId: -1, Version: 0, NodeId: algo-1:46605, NodeHttpAddress: algo-1:8042, Resource: <memory:896, vCores:1>, Priority: 0, Token: Token { kind: ContainerToken, service: 10.0.189.145:46605 }, ExecutionType: GUARANTEED, ]\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:30,952 INFO attempt.RMAppAttemptImpl: appattempt_1614662779782_0001_000001 State change from SCHEDULED to ALLOCATED_SAVING on event = CONTAINER_ALLOCATED\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:30,956 INFO attempt.RMAppAttemptImpl: appattempt_1614662779782_0001_000001 State change from ALLOCATED_SAVING to ALLOCATED on event = ATTEMPT_NEW_SAVED\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:30,968 INFO amlauncher.AMLauncher: Launching masterappattempt_1614662779782_0001_000001\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:31,022 INFO amlauncher.AMLauncher: Setting up container Container: [ContainerId: container_1614662779782_0001_01_000001, AllocationRequestId: -1, Version: 0, NodeId: algo-1:46605, NodeHttpAddress: algo-1:8042, Resource: <memory:896, vCores:1>, Priority: 0, Token: Token { kind: ContainerToken, service: 10.0.189.145:46605 }, ExecutionType: GUARANTEED, ] for AM appattempt_1614662779782_0001_000001\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:31,022 INFO security.AMRMTokenSecretManager: Create AMRMToken for ApplicationAttempt: appattempt_1614662779782_0001_000001\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:31,025 INFO security.AMRMTokenSecretManager: Creating password for appattempt_1614662779782_0001_000001\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:31,142 INFO ipc.Server: Auth successful for appattempt_1614662779782_0001_000001 (auth:SIMPLE)\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:31,194 INFO yarn.Client: Application report for application_1614662779782_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:31,196 INFO yarn.Client: \u001b[0m\n",
      "\u001b[34m#011 client token: N/A\u001b[0m\n",
      "\u001b[34m#011 diagnostics: [Tue Mar 02 05:26:30 +0000 2021] Scheduler has assigned a container for AM, waiting for AM container to be launched\u001b[0m\n",
      "\u001b[34m#011 ApplicationMaster host: N/A\u001b[0m\n",
      "\u001b[34m#011 ApplicationMaster RPC port: -1\u001b[0m\n",
      "\u001b[34m#011 queue: default\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:30,911 INFO allocator.AbstractContainerAllocator: assignedContainer application attempt=appattempt_1614662779782_0001_000001 container=null queue=default clusterResource=<memory:63569, vCores:16> type=OFF_SWITCH requestedPartition=\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:30,915 INFO rmcontainer.RMContainerImpl: container_1614662779782_0001_01_000001 Container Transitioned from NEW to ALLOCATED\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:30,916 INFO fica.FiCaSchedulerNode: Assigned container container_1614662779782_0001_01_000001 of capacity <memory:896, vCores:1> on host algo-1:46605, which has 1 containers, <memory:896, vCores:1> used and <memory:62673, vCores:15> available after allocation\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:30,916 INFO resourcemanager.RMAuditLogger: USER=root#011OPERATION=AM Allocated Container#011TARGET=SchedulerApp#011RESULT=SUCCESS#011APPID=application_1614662779782_0001#011CONTAINERID=container_1614662779782_0001_01_000001#011RESOURCE=<memory:896, vCores:1>#011QUEUENAME=default\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:30,933 INFO security.NMTokenSecretManagerInRM: Sending NMToken for nodeId : algo-1:46605 for container : container_1614662779782_0001_01_000001\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:30,941 INFO rmcontainer.RMContainerImpl: container_1614662779782_0001_01_000001 Container Transitioned from ALLOCATED to ACQUIRED\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:30,941 INFO security.NMTokenSecretManagerInRM: Clear node set for appattempt_1614662779782_0001_000001\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:30,941 INFO capacity.ParentQueue: assignedContainer queue=root usedCapacity=0.014094921 absoluteUsedCapacity=0.014094921 used=<memory:896, vCores:1> cluster=<memory:63569, vCores:16>\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:30,941 INFO capacity.CapacityScheduler: Allocation proposal accepted\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:30,941 INFO attempt.RMAppAttemptImpl: Storing attempt: AppId: application_1614662779782_0001 AttemptId: appattempt_1614662779782_0001_000001 MasterContainer: Container: [ContainerId: container_1614662779782_0001_01_000001, AllocationRequestId: -1, Version: 0, NodeId: algo-1:46605, NodeHttpAddress: algo-1:8042, Resource: <memory:896, vCores:1>, Priority: 0, Token: Token { kind: ContainerToken, service: 10.0.189.145:46605 }, ExecutionType: GUARANTEED, ]\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:30,952 INFO attempt.RMAppAttemptImpl: appattempt_1614662779782_0001_000001 State change from SCHEDULED to ALLOCATED_SAVING on event = CONTAINER_ALLOCATED\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:30,956 INFO attempt.RMAppAttemptImpl: appattempt_1614662779782_0001_000001 State change from ALLOCATED_SAVING to ALLOCATED on event = ATTEMPT_NEW_SAVED\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:30,968 INFO amlauncher.AMLauncher: Launching masterappattempt_1614662779782_0001_000001\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:31,022 INFO amlauncher.AMLauncher: Setting up container Container: [ContainerId: container_1614662779782_0001_01_000001, AllocationRequestId: -1, Version: 0, NodeId: algo-1:46605, NodeHttpAddress: algo-1:8042, Resource: <memory:896, vCores:1>, Priority: 0, Token: Token { kind: ContainerToken, service: 10.0.189.145:46605 }, ExecutionType: GUARANTEED, ] for AM appattempt_1614662779782_0001_000001\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:31,022 INFO security.AMRMTokenSecretManager: Create AMRMToken for ApplicationAttempt: appattempt_1614662779782_0001_000001\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:31,025 INFO security.AMRMTokenSecretManager: Creating password for appattempt_1614662779782_0001_000001\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:31,142 INFO ipc.Server: Auth successful for appattempt_1614662779782_0001_000001 (auth:SIMPLE)\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:31,194 INFO yarn.Client: Application report for application_1614662779782_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:31,196 INFO yarn.Client: \u001b[0m\n",
      "\u001b[35m#011 client token: N/A\u001b[0m\n",
      "\u001b[35m#011 diagnostics: [Tue Mar 02 05:26:30 +0000 2021] Scheduler has assigned a container for AM, waiting for AM container to be launched\u001b[0m\n",
      "\u001b[35m#011 ApplicationMaster host: N/A\u001b[0m\n",
      "\u001b[35m#011 ApplicationMaster RPC port: -1\u001b[0m\n",
      "\u001b[35m#011 queue: default\u001b[0m\n",
      "\u001b[34m#011 start time: 1614662790085\u001b[0m\n",
      "\u001b[34m#011 final status: UNDEFINED\u001b[0m\n",
      "\u001b[34m#011 tracking URL: http://algo-1:8088/proxy/application_1614662779782_0001/\u001b[0m\n",
      "\u001b[34m#011 user: root\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:31,218 INFO containermanager.ContainerManagerImpl: Start request for container_1614662779782_0001_01_000001 by user root\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:31,261 INFO containermanager.ContainerManagerImpl: Creating a new application reference for app application_1614662779782_0001\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:31,268 INFO application.ApplicationImpl: Application application_1614662779782_0001 transitioned from NEW to INITING\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:31,268 INFO application.ApplicationImpl: Adding container_1614662779782_0001_01_000001 to application application_1614662779782_0001\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:31,268 INFO nodemanager.NMAuditLogger: USER=root#011IP=10.0.189.145#011OPERATION=Start Container Request#011TARGET=ContainerManageImpl#011RESULT=SUCCESS#011APPID=application_1614662779782_0001#011CONTAINERID=container_1614662779782_0001_01_000001\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:31,271 INFO application.ApplicationImpl: Application application_1614662779782_0001 transitioned from INITING to RUNNING\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:31,274 INFO container.ContainerImpl: Container container_1614662779782_0001_01_000001 transitioned from NEW to LOCALIZING\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:31,274 INFO containermanager.AuxServices: Got event CONTAINER_INIT for appId application_1614662779782_0001\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:31,281 INFO amlauncher.AMLauncher: Done launching container Container: [ContainerId: container_1614662779782_0001_01_000001, AllocationRequestId: -1, Version: 0, NodeId: algo-1:46605, NodeHttpAddress: algo-1:8042, Resource: <memory:896, vCores:1>, Priority: 0, Token: Token { kind: ContainerToken, service: 10.0.189.145:46605 }, ExecutionType: GUARANTEED, ] for AM appattempt_1614662779782_0001_000001\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:31,281 INFO attempt.RMAppAttemptImpl: appattempt_1614662779782_0001_000001 State change from ALLOCATED to LAUNCHED on event = LAUNCHED\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:31,281 INFO rmapp.RMAppImpl: update the launch time for applicationId: application_1614662779782_0001, attemptId: appattempt_1614662779782_0001_000001launchTime: 1614662791281\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:31,282 INFO recovery.RMStateStore: Updating info for app: application_1614662779782_0001\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:31,283 INFO localizer.ResourceLocalizationService: Created localizer for container_1614662779782_0001_01_000001\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:31,340 INFO localizer.ResourceLocalizationService: Writing credentials to the nmPrivate file /tmp/hadoop-root/nm-local-dir/nmPrivate/container_1614662779782_0001_01_000001.tokens\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:31,353 INFO nodemanager.DefaultContainerExecutor: Initializing user root\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:31,357 INFO nodemanager.DefaultContainerExecutor: Copying from /tmp/hadoop-root/nm-local-dir/nmPrivate/container_1614662779782_0001_01_000001.tokens to /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1614662779782_0001/container_1614662779782_0001_01_000001.tokens\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:31,358 INFO nodemanager.DefaultContainerExecutor: Localizer CWD set to /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1614662779782_0001 = file:/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1614662779782_0001\u001b[0m\n",
      "\u001b[35m#011 start time: 1614662790085\u001b[0m\n",
      "\u001b[35m#011 final status: UNDEFINED\u001b[0m\n",
      "\u001b[35m#011 tracking URL: http://algo-1:8088/proxy/application_1614662779782_0001/\u001b[0m\n",
      "\u001b[35m#011 user: root\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:31,218 INFO containermanager.ContainerManagerImpl: Start request for container_1614662779782_0001_01_000001 by user root\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:31,261 INFO containermanager.ContainerManagerImpl: Creating a new application reference for app application_1614662779782_0001\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:31,268 INFO application.ApplicationImpl: Application application_1614662779782_0001 transitioned from NEW to INITING\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:31,268 INFO application.ApplicationImpl: Adding container_1614662779782_0001_01_000001 to application application_1614662779782_0001\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:31,268 INFO nodemanager.NMAuditLogger: USER=root#011IP=10.0.189.145#011OPERATION=Start Container Request#011TARGET=ContainerManageImpl#011RESULT=SUCCESS#011APPID=application_1614662779782_0001#011CONTAINERID=container_1614662779782_0001_01_000001\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:31,271 INFO application.ApplicationImpl: Application application_1614662779782_0001 transitioned from INITING to RUNNING\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:31,274 INFO container.ContainerImpl: Container container_1614662779782_0001_01_000001 transitioned from NEW to LOCALIZING\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:31,274 INFO containermanager.AuxServices: Got event CONTAINER_INIT for appId application_1614662779782_0001\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:31,281 INFO amlauncher.AMLauncher: Done launching container Container: [ContainerId: container_1614662779782_0001_01_000001, AllocationRequestId: -1, Version: 0, NodeId: algo-1:46605, NodeHttpAddress: algo-1:8042, Resource: <memory:896, vCores:1>, Priority: 0, Token: Token { kind: ContainerToken, service: 10.0.189.145:46605 }, ExecutionType: GUARANTEED, ] for AM appattempt_1614662779782_0001_000001\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:31,281 INFO attempt.RMAppAttemptImpl: appattempt_1614662779782_0001_000001 State change from ALLOCATED to LAUNCHED on event = LAUNCHED\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:31,281 INFO rmapp.RMAppImpl: update the launch time for applicationId: application_1614662779782_0001, attemptId: appattempt_1614662779782_0001_000001launchTime: 1614662791281\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:31,282 INFO recovery.RMStateStore: Updating info for app: application_1614662779782_0001\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:31,283 INFO localizer.ResourceLocalizationService: Created localizer for container_1614662779782_0001_01_000001\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:31,340 INFO localizer.ResourceLocalizationService: Writing credentials to the nmPrivate file /tmp/hadoop-root/nm-local-dir/nmPrivate/container_1614662779782_0001_01_000001.tokens\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:31,353 INFO nodemanager.DefaultContainerExecutor: Initializing user root\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:31,357 INFO nodemanager.DefaultContainerExecutor: Copying from /tmp/hadoop-root/nm-local-dir/nmPrivate/container_1614662779782_0001_01_000001.tokens to /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1614662779782_0001/container_1614662779782_0001_01_000001.tokens\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:31,358 INFO nodemanager.DefaultContainerExecutor: Localizer CWD set to /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1614662779782_0001 = file:/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1614662779782_0001\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:31,906 INFO rmcontainer.RMContainerImpl: container_1614662779782_0001_01_000001 Container Transitioned from ACQUIRED to RUNNING\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:32,199 INFO yarn.Client: Application report for application_1614662779782_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:31,906 INFO rmcontainer.RMContainerImpl: container_1614662779782_0001_01_000001 Container Transitioned from ACQUIRED to RUNNING\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:32,199 INFO yarn.Client: Application report for application_1614662779782_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:33,580 INFO scheduler.ContainerScheduler: Starting container [container_1614662779782_0001_01_000001]\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:33,605 INFO container.ContainerImpl: Container container_1614662779782_0001_01_000001 transitioned from SCHEDULED to RUNNING\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:33,605 INFO monitor.ContainersMonitorImpl: Starting resource-monitoring for container_1614662779782_0001_01_000001\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:33,608 INFO nodemanager.DefaultContainerExecutor: launchContainer: [bash, /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1614662779782_0001/container_1614662779782_0001_01_000001/default_container_executor.sh]\u001b[0m\n",
      "\u001b[34mHandling create event for file: /var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000001/prelaunch.out\u001b[0m\n",
      "\u001b[34mHandling create event for file: /var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000001/prelaunch.err\u001b[0m\n",
      "\u001b[34mHandling create event for file: /var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000001/launch_container.sh\u001b[0m\n",
      "\u001b[34mHandling create event for file: /var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000001/directory.info\u001b[0m\n",
      "\u001b[34mHandling create event for file: /var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000001/stdout\u001b[0m\n",
      "\u001b[34mHandling create event for file: /var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000001/stderr\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:33,580 INFO scheduler.ContainerScheduler: Starting container [container_1614662779782_0001_01_000001]\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:33,605 INFO container.ContainerImpl: Container container_1614662779782_0001_01_000001 transitioned from SCHEDULED to RUNNING\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:33,605 INFO monitor.ContainersMonitorImpl: Starting resource-monitoring for container_1614662779782_0001_01_000001\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:33,608 INFO nodemanager.DefaultContainerExecutor: launchContainer: [bash, /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1614662779782_0001/container_1614662779782_0001_01_000001/default_container_executor.sh]\u001b[0m\n",
      "\u001b[35mHandling create event for file: /var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000001/prelaunch.out\u001b[0m\n",
      "\u001b[35mHandling create event for file: /var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000001/prelaunch.err\u001b[0m\n",
      "\u001b[35mHandling create event for file: /var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000001/launch_container.sh\u001b[0m\n",
      "\u001b[35mHandling create event for file: /var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000001/directory.info\u001b[0m\n",
      "\u001b[35mHandling create event for file: /var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000001/stdout\u001b[0m\n",
      "\u001b[35mHandling create event for file: /var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000001/stderr\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:34,204 INFO yarn.Client: Application report for application_1614662779782_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:34,204 INFO yarn.Client: Application report for application_1614662779782_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:35,207 INFO yarn.Client: Application report for application_1614662779782_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:35,207 INFO yarn.Client: Application report for application_1614662779782_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:35,750 INFO ipc.Server: Auth successful for appattempt_1614662779782_0001_000001 (auth:SIMPLE)\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:35,771 INFO resourcemanager.DefaultAMSProcessor: AM registration appattempt_1614662779782_0001_000001\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:35,772 INFO resourcemanager.RMAuditLogger: USER=root#011IP=10.0.189.145#011OPERATION=Register App Master#011TARGET=ApplicationMasterService#011RESULT=SUCCESS#011APPID=application_1614662779782_0001#011APPATTEMPTID=appattempt_1614662779782_0001_000001\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:35,772 INFO attempt.RMAppAttemptImpl: appattempt_1614662779782_0001_000001 State change from LAUNCHED to RUNNING on event = REGISTERED\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:35,772 INFO rmapp.RMAppImpl: application_1614662779782_0001 State change from ACCEPTED to RUNNING on event = ATTEMPT_REGISTERED\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:36,030 INFO cluster.YarnClientSchedulerBackend: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -> algo-1, PROXY_URI_BASES -> http://algo-1:8088/proxy/application_1614662779782_0001), /proxy/application_1614662779782_0001\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:36,120 INFO monitor.ContainersMonitorImpl: container_1614662779782_0001_01_000001's ip = 10.0.189.145, and hostname = algo-1\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:36,127 INFO monitor.ContainersMonitorImpl: Skipping monitoring container container_1614662779782_0001_01_000001 since CPU usage is not yet available.\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:36,210 INFO yarn.Client: Application report for application_1614662779782_0001 (state: RUNNING)\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:36,210 INFO yarn.Client: \u001b[0m\n",
      "\u001b[34m#011 client token: N/A\u001b[0m\n",
      "\u001b[34m#011 diagnostics: N/A\u001b[0m\n",
      "\u001b[34m#011 ApplicationMaster host: 10.0.189.145\u001b[0m\n",
      "\u001b[34m#011 ApplicationMaster RPC port: -1\u001b[0m\n",
      "\u001b[34m#011 queue: default\u001b[0m\n",
      "\u001b[34m#011 start time: 1614662790085\u001b[0m\n",
      "\u001b[34m#011 final status: UNDEFINED\u001b[0m\n",
      "\u001b[34m#011 tracking URL: http://algo-1:8088/proxy/application_1614662779782_0001/\u001b[0m\n",
      "\u001b[34m#011 user: root\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:36,212 INFO cluster.YarnClientSchedulerBackend: Application application_1614662779782_0001 has started running.\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:36,220 INFO util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 39837.\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:36,220 INFO netty.NettyBlockTransferService: Server created on 10.0.189.145:39837\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:36,222 INFO storage.BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:36,232 INFO storage.BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 10.0.189.145, 39837, None)\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:36,235 INFO storage.BlockManagerMasterEndpoint: Registering block manager 10.0.189.145:39837 with 1007.8 MiB RAM, BlockManagerId(driver, 10.0.189.145, 39837, None)\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:36,240 INFO storage.BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 10.0.189.145, 39837, None)\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:36,240 INFO storage.BlockManager: Initialized BlockManager: BlockManagerId(driver, 10.0.189.145, 39837, None)\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:36,371 INFO cluster.YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as NettyRpcEndpointRef(spark-client://YarnAM)\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:36,439 INFO ui.ServerInfo: Adding filter to /metrics/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:36,441 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1740b980{/metrics/json,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:36,473 INFO cluster.YarnClientSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000001/stderr] 2021-03-02 05:26:34,650 INFO util.SignalUtils: Registered signal handler for TERM\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000001/stderr] 2021-03-02 05:26:34,652 INFO util.SignalUtils: Registered signal handler for HUP\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000001/stderr] 2021-03-02 05:26:34,652 INFO util.SignalUtils: Registered signal handler for INT\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000001/stderr] 2021-03-02 05:26:35,046 INFO spark.SecurityManager: Changing view acls to: root\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000001/stderr] 2021-03-02 05:26:35,046 INFO spark.SecurityManager: Changing modify acls to: root\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000001/stderr] 2021-03-02 05:26:35,047 INFO spark.SecurityManager: Changing view acls groups to: \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000001/stderr] 2021-03-02 05:26:35,047 INFO spark.SecurityManager: Changing modify acls groups to: \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000001/stderr] 2021-03-02 05:26:35,047 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000001/stderr] 2021-03-02 05:26:35,158 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:35,750 INFO ipc.Server: Auth successful for appattempt_1614662779782_0001_000001 (auth:SIMPLE)\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:35,771 INFO resourcemanager.DefaultAMSProcessor: AM registration appattempt_1614662779782_0001_000001\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:35,772 INFO resourcemanager.RMAuditLogger: USER=root#011IP=10.0.189.145#011OPERATION=Register App Master#011TARGET=ApplicationMasterService#011RESULT=SUCCESS#011APPID=application_1614662779782_0001#011APPATTEMPTID=appattempt_1614662779782_0001_000001\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:35,772 INFO attempt.RMAppAttemptImpl: appattempt_1614662779782_0001_000001 State change from LAUNCHED to RUNNING on event = REGISTERED\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:35,772 INFO rmapp.RMAppImpl: application_1614662779782_0001 State change from ACCEPTED to RUNNING on event = ATTEMPT_REGISTERED\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:36,030 INFO cluster.YarnClientSchedulerBackend: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -> algo-1, PROXY_URI_BASES -> http://algo-1:8088/proxy/application_1614662779782_0001), /proxy/application_1614662779782_0001\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:36,120 INFO monitor.ContainersMonitorImpl: container_1614662779782_0001_01_000001's ip = 10.0.189.145, and hostname = algo-1\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:36,127 INFO monitor.ContainersMonitorImpl: Skipping monitoring container container_1614662779782_0001_01_000001 since CPU usage is not yet available.\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:36,210 INFO yarn.Client: Application report for application_1614662779782_0001 (state: RUNNING)\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:36,210 INFO yarn.Client: \u001b[0m\n",
      "\u001b[35m#011 client token: N/A\u001b[0m\n",
      "\u001b[35m#011 diagnostics: N/A\u001b[0m\n",
      "\u001b[35m#011 ApplicationMaster host: 10.0.189.145\u001b[0m\n",
      "\u001b[35m#011 ApplicationMaster RPC port: -1\u001b[0m\n",
      "\u001b[35m#011 queue: default\u001b[0m\n",
      "\u001b[35m#011 start time: 1614662790085\u001b[0m\n",
      "\u001b[35m#011 final status: UNDEFINED\u001b[0m\n",
      "\u001b[35m#011 tracking URL: http://algo-1:8088/proxy/application_1614662779782_0001/\u001b[0m\n",
      "\u001b[35m#011 user: root\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:36,212 INFO cluster.YarnClientSchedulerBackend: Application application_1614662779782_0001 has started running.\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:36,220 INFO util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 39837.\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:36,220 INFO netty.NettyBlockTransferService: Server created on 10.0.189.145:39837\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:36,222 INFO storage.BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:36,232 INFO storage.BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 10.0.189.145, 39837, None)\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:36,235 INFO storage.BlockManagerMasterEndpoint: Registering block manager 10.0.189.145:39837 with 1007.8 MiB RAM, BlockManagerId(driver, 10.0.189.145, 39837, None)\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:36,240 INFO storage.BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 10.0.189.145, 39837, None)\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:36,240 INFO storage.BlockManager: Initialized BlockManager: BlockManagerId(driver, 10.0.189.145, 39837, None)\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:36,371 INFO cluster.YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as NettyRpcEndpointRef(spark-client://YarnAM)\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:36,439 INFO ui.ServerInfo: Adding filter to /metrics/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:36,441 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1740b980{/metrics/json,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:36,473 INFO cluster.YarnClientSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000001/stderr] 2021-03-02 05:26:34,650 INFO util.SignalUtils: Registered signal handler for TERM\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000001/stderr] 2021-03-02 05:26:34,652 INFO util.SignalUtils: Registered signal handler for HUP\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000001/stderr] 2021-03-02 05:26:34,652 INFO util.SignalUtils: Registered signal handler for INT\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000001/stderr] 2021-03-02 05:26:35,046 INFO spark.SecurityManager: Changing view acls to: root\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000001/stderr] 2021-03-02 05:26:35,046 INFO spark.SecurityManager: Changing modify acls to: root\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000001/stderr] 2021-03-02 05:26:35,047 INFO spark.SecurityManager: Changing view acls groups to: \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000001/stderr] 2021-03-02 05:26:35,047 INFO spark.SecurityManager: Changing modify acls groups to: \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000001/stderr] 2021-03-02 05:26:35,047 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000001/stderr] 2021-03-02 05:26:35,158 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000001/stderr] 2021-03-02 05:26:35,313 INFO yarn.ApplicationMaster: ApplicationAttemptId: appattempt_1614662779782_0001_000001\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000001/stderr] 2021-03-02 05:26:35,586 INFO client.RMProxy: Connecting to ResourceManager at /10.0.189.145:8030\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000001/stderr] 2021-03-02 05:26:35,647 INFO yarn.YarnRMClient: Registering the ApplicationMaster\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000001/stderr] 2021-03-02 05:26:35,916 INFO client.TransportClientFactory: Successfully created connection to /10.0.189.145:38479 after 79 ms (0 ms spent in bootstraps)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000001/stderr] 2021-03-02 05:26:36,042 INFO yarn.ApplicationMaster: Preparing Local resources\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000001/stderr] 2021-03-02 05:26:36,244 INFO yarn.ApplicationMaster: \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000001/stderr] ===============================================================================\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000001/stderr] Default YARN executor launch context:\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000001/stderr]   env:\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000001/stderr]     CLASSPATH -> /usr/lib/hadoop-lzo/lib/*:/usr/lib/hadoop/hadoop-aws.jar:/usr/share/aws/aws-java-sdk/*:/usr/share/aws/emr/emrfs/conf:/usr/share/aws/emr/emrfs/lib/*:/usr/share/aws/emr/emrfs/auxlib/*:/usr/share/aws/emr/goodies/lib/emr-spark-goodies.jar:/usr/share/aws/emr/security/conf:/usr/share/aws/emr/security/lib/*:/usr/share/aws/hmclient/lib/aws-glue-datacatalog-spark-client.jar:/usr/share/java/Hive-JSON-Serde/hive-openx-serde.jar:/usr/share/aws/sagemaker-spark-sdk/lib/sagemaker-spark-sdk.jar:/usr/share/aws/emr/s3select/lib/emr-s3-select-spark-connector.jar<CPS>{{PWD}}<CPS>{{PWD}}/__spark_conf__<CPS>{{PWD}}/__spark_libs__/*<CPS>$HADOOP_CONF_DIR<CPS>$HADOOP_COMMON_HOME/share/hadoop/common/*<CPS>$HADOOP_COMMON_HOME/share/hadoop/common/lib/*<CPS>$HADOOP_HDFS_HOME/share/hadoop/hdfs/*<CPS>$HADOOP_HDFS_HOME/share/hadoop/hdfs/lib/*<CPS>$HADOOP_YARN_HOME/share/hadoop/yarn/*<CPS>$HADOOP_YARN_HOME/share/hadoop/yarn/lib/*<CPS>$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*<CPS>$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*<CPS>{{PWD}}/__spark_conf__/__hadoop_conf__\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000001/stderr]     SPARK_YARN_STAGING_DIR -> file:/root/.sparkStaging/application_1614662779782_0001\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000001/stderr]     SPARK_USER -> root\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000001/stderr]     PYTHONPATH -> {{PWD}}/pyspark.zip<CPS>{{PWD}}/py4j-0.10.9-src.zip\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000001/stderr] \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000001/stderr]   command:\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000001/stderr]     LD_LIBRARY_PATH=\\\"/usr/lib/hadoop/lib/native:/usr/lib/hadoop-lzo/lib/native:$LD_LIBRARY_PATH\\\" \\ \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000001/stderr]       {{JAVA_HOME}}/bin/java \\ \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000001/stderr]       -server \\ \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000001/stderr]       -Xmx55742m \\ \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000001/stderr]       '-verbose:gc' \\ \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000001/stderr]       '-XX:OnOutOfMemoryError=kill -9 %p' \\ \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000001/stderr]       '-XX:+PrintGCDetails' \\ \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000001/stderr]       '-XX:+PrintGCDateStamps' \\ \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000001/stderr]       '-XX:+UseParallelGC' \\ \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000001/stderr]       '-XX:InitiatingHeapOccupancyPercent=70' \\ \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000001/stderr]       '-XX:ConcGCThreads=4' \\ \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000001/stderr]       '-XX:ParallelGCThreads=12' \\ \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000001/stderr]       -Djava.io.tmpdir={{PWD}}/tmp \\ \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000001/stderr]       '-Dspark.driver.port=38479' \\ \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000001/stderr]       '-Dspark.port.maxRetries=50' \\ \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000001/stderr]       -Dspark.yarn.app.container.log.dir=<LOG_DIR> \\ \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000001/stderr]       org.apache.spark.executor.YarnCoarseGrainedExecutorBackend \\ \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000001/stderr]       --driver-url \\ \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000001/stderr]       spark://CoarseGrainedScheduler@10.0.189.145:38479 \\ \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000001/stderr]       --executor-id \\ \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000001/stderr]       <executorId> \\ \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000001/stderr]       --hostname \\ \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000001/stderr]       <hostname> \\ \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000001/stderr]       --cores \\ \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000001/stderr]       16 \\ \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000001/stderr]       --app-id \\ \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000001/stderr]       application_1614662779782_0001 \\ \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000001/stderr]       --resourceProfileId \\ \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000001/stderr]       0 \\ \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000001/stderr]       --user-class-path \\ \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000001/stderr]       file:$PWD/__app__.jar \\ \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000001/stderr]       1><LOG_DIR>/stdout \\ \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000001/stderr]       2><LOG_DIR>/stderr\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000001/stderr] \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000001/stderr]   resources:\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000001/stderr]     pyspark.zip -> resource { scheme: \"file\" port: -1 file: \"/root/.sparkStaging/application_1614662779782_0001/pyspark.zip\" } size: 732492 timestamp: 1614662789000 type: FILE visibility: PRIVATE\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000001/stderr] 2021-03-02 05:26:35,313 INFO yarn.ApplicationMaster: ApplicationAttemptId: appattempt_1614662779782_0001_000001\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000001/stderr] 2021-03-02 05:26:35,586 INFO client.RMProxy: Connecting to ResourceManager at /10.0.189.145:8030\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000001/stderr] 2021-03-02 05:26:35,647 INFO yarn.YarnRMClient: Registering the ApplicationMaster\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000001/stderr] 2021-03-02 05:26:35,916 INFO client.TransportClientFactory: Successfully created connection to /10.0.189.145:38479 after 79 ms (0 ms spent in bootstraps)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000001/stderr] 2021-03-02 05:26:36,042 INFO yarn.ApplicationMaster: Preparing Local resources\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000001/stderr] 2021-03-02 05:26:36,244 INFO yarn.ApplicationMaster: \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000001/stderr] ===============================================================================\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000001/stderr] Default YARN executor launch context:\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000001/stderr]   env:\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000001/stderr]     CLASSPATH -> /usr/lib/hadoop-lzo/lib/*:/usr/lib/hadoop/hadoop-aws.jar:/usr/share/aws/aws-java-sdk/*:/usr/share/aws/emr/emrfs/conf:/usr/share/aws/emr/emrfs/lib/*:/usr/share/aws/emr/emrfs/auxlib/*:/usr/share/aws/emr/goodies/lib/emr-spark-goodies.jar:/usr/share/aws/emr/security/conf:/usr/share/aws/emr/security/lib/*:/usr/share/aws/hmclient/lib/aws-glue-datacatalog-spark-client.jar:/usr/share/java/Hive-JSON-Serde/hive-openx-serde.jar:/usr/share/aws/sagemaker-spark-sdk/lib/sagemaker-spark-sdk.jar:/usr/share/aws/emr/s3select/lib/emr-s3-select-spark-connector.jar<CPS>{{PWD}}<CPS>{{PWD}}/__spark_conf__<CPS>{{PWD}}/__spark_libs__/*<CPS>$HADOOP_CONF_DIR<CPS>$HADOOP_COMMON_HOME/share/hadoop/common/*<CPS>$HADOOP_COMMON_HOME/share/hadoop/common/lib/*<CPS>$HADOOP_HDFS_HOME/share/hadoop/hdfs/*<CPS>$HADOOP_HDFS_HOME/share/hadoop/hdfs/lib/*<CPS>$HADOOP_YARN_HOME/share/hadoop/yarn/*<CPS>$HADOOP_YARN_HOME/share/hadoop/yarn/lib/*<CPS>$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*<CPS>$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*<CPS>{{PWD}}/__spark_conf__/__hadoop_conf__\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000001/stderr]     SPARK_YARN_STAGING_DIR -> file:/root/.sparkStaging/application_1614662779782_0001\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000001/stderr]     SPARK_USER -> root\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000001/stderr]     PYTHONPATH -> {{PWD}}/pyspark.zip<CPS>{{PWD}}/py4j-0.10.9-src.zip\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000001/stderr] \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000001/stderr]   command:\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000001/stderr]     LD_LIBRARY_PATH=\\\"/usr/lib/hadoop/lib/native:/usr/lib/hadoop-lzo/lib/native:$LD_LIBRARY_PATH\\\" \\ \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000001/stderr]       {{JAVA_HOME}}/bin/java \\ \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000001/stderr]       -server \\ \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000001/stderr]       -Xmx55742m \\ \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000001/stderr]       '-verbose:gc' \\ \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000001/stderr]       '-XX:OnOutOfMemoryError=kill -9 %p' \\ \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000001/stderr]       '-XX:+PrintGCDetails' \\ \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000001/stderr]       '-XX:+PrintGCDateStamps' \\ \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000001/stderr]       '-XX:+UseParallelGC' \\ \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000001/stderr]       '-XX:InitiatingHeapOccupancyPercent=70' \\ \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000001/stderr]       '-XX:ConcGCThreads=4' \\ \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000001/stderr]       '-XX:ParallelGCThreads=12' \\ \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000001/stderr]       -Djava.io.tmpdir={{PWD}}/tmp \\ \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000001/stderr]       '-Dspark.driver.port=38479' \\ \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000001/stderr]       '-Dspark.port.maxRetries=50' \\ \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000001/stderr]       -Dspark.yarn.app.container.log.dir=<LOG_DIR> \\ \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000001/stderr]       org.apache.spark.executor.YarnCoarseGrainedExecutorBackend \\ \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000001/stderr]       --driver-url \\ \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000001/stderr]       spark://CoarseGrainedScheduler@10.0.189.145:38479 \\ \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000001/stderr]       --executor-id \\ \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000001/stderr]       <executorId> \\ \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000001/stderr]       --hostname \\ \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000001/stderr]       <hostname> \\ \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000001/stderr]       --cores \\ \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000001/stderr]       16 \\ \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000001/stderr]       --app-id \\ \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000001/stderr]       application_1614662779782_0001 \\ \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000001/stderr]       --resourceProfileId \\ \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000001/stderr]       0 \\ \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000001/stderr]       --user-class-path \\ \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000001/stderr]       file:$PWD/__app__.jar \\ \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000001/stderr]       1><LOG_DIR>/stdout \\ \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000001/stderr]       2><LOG_DIR>/stderr\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000001/stderr] \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000001/stderr]   resources:\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000001/stderr]     pyspark.zip -> resource { scheme: \"file\" port: -1 file: \"/root/.sparkStaging/application_1614662779782_0001/pyspark.zip\" } size: 732492 timestamp: 1614662789000 type: FILE visibility: PRIVATE\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000001/stderr]     __spark_libs__ -> resource { scheme: \"file\" port: -1 file: \"/root/.sparkStaging/application_1614662779782_0001/__spark_libs__7663445419400196768.zip\" } size: 397947606 timestamp: 1614662788000 type: ARCHIVE visibility: PRIVATE\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000001/stderr]     py4j-0.10.9-src.zip -> resource { scheme: \"file\" port: -1 file: \"/root/.sparkStaging/application_1614662779782_0001/py4j-0.10.9-src.zip\" } size: 41587 timestamp: 1614662789000 type: FILE visibility: PRIVATE\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000001/stderr]     __spark_conf__ -> resource { scheme: \"file\" port: -1 file: \"/root/.sparkStaging/application_1614662779782_0001/__spark_conf__.zip\" } size: 262570 timestamp: 1614662789000 type: ARCHIVE visibility: PRIVATE\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000001/stderr] \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000001/stderr] ===============================================================================\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000001/stderr] 2021-03-02 05:26:36,321 INFO conf.Configuration: resource-types.xml not found\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000001/stderr] 2021-03-02 05:26:36,322 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000001/stderr] 2021-03-02 05:26:36,347 INFO yarn.YarnAllocator: Will request 1 executor container(s), each with 16 core(s) and 61316 MB memory (including 5574 MB of overhead)\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:36,678 INFO internal.SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/sagemaker-user/spark-warehouse').\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:36,679 INFO internal.SharedState: Warehouse path is 'file:/home/sagemaker-user/spark-warehouse'.\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:36,692 INFO ui.ServerInfo: Adding filter to /SQL: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:36,693 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@70c3eac8{/SQL,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:36,693 INFO ui.ServerInfo: Adding filter to /SQL/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:36,694 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@60adb0bd{/SQL/json,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:36,694 INFO ui.ServerInfo: Adding filter to /SQL/execution: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:36,695 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2ffdd5a1{/SQL/execution,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:36,695 INFO ui.ServerInfo: Adding filter to /SQL/execution/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:36,696 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@140e74a0{/SQL/execution/json,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:36,696 INFO ui.ServerInfo: Adding filter to /static/sql: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:36,698 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3fd73a11{/static/sql,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000001/stderr]     __spark_libs__ -> resource { scheme: \"file\" port: -1 file: \"/root/.sparkStaging/application_1614662779782_0001/__spark_libs__7663445419400196768.zip\" } size: 397947606 timestamp: 1614662788000 type: ARCHIVE visibility: PRIVATE\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000001/stderr]     py4j-0.10.9-src.zip -> resource { scheme: \"file\" port: -1 file: \"/root/.sparkStaging/application_1614662779782_0001/py4j-0.10.9-src.zip\" } size: 41587 timestamp: 1614662789000 type: FILE visibility: PRIVATE\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000001/stderr]     __spark_conf__ -> resource { scheme: \"file\" port: -1 file: \"/root/.sparkStaging/application_1614662779782_0001/__spark_conf__.zip\" } size: 262570 timestamp: 1614662789000 type: ARCHIVE visibility: PRIVATE\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000001/stderr] \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000001/stderr] ===============================================================================\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000001/stderr] 2021-03-02 05:26:36,321 INFO conf.Configuration: resource-types.xml not found\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000001/stderr] 2021-03-02 05:26:36,322 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000001/stderr] 2021-03-02 05:26:36,347 INFO yarn.YarnAllocator: Will request 1 executor container(s), each with 16 core(s) and 61316 MB memory (including 5574 MB of overhead)\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:36,678 INFO internal.SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/sagemaker-user/spark-warehouse').\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:36,679 INFO internal.SharedState: Warehouse path is 'file:/home/sagemaker-user/spark-warehouse'.\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:36,692 INFO ui.ServerInfo: Adding filter to /SQL: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:36,693 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@70c3eac8{/SQL,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:36,693 INFO ui.ServerInfo: Adding filter to /SQL/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:36,694 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@60adb0bd{/SQL/json,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:36,694 INFO ui.ServerInfo: Adding filter to /SQL/execution: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:36,695 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2ffdd5a1{/SQL/execution,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:36,695 INFO ui.ServerInfo: Adding filter to /SQL/execution/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:36,696 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@140e74a0{/SQL/execution/json,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:36,696 INFO ui.ServerInfo: Adding filter to /static/sql: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:36,698 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3fd73a11{/static/sql,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:36,918 INFO allocator.AbstractContainerAllocator: assignedContainer application attempt=appattempt_1614662779782_0001_000001 container=null queue=default clusterResource=<memory:63569, vCores:16> type=OFF_SWITCH requestedPartition=\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:36,918 INFO rmcontainer.RMContainerImpl: container_1614662779782_0001_01_000002 Container Transitioned from NEW to ALLOCATED\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:36,918 INFO fica.FiCaSchedulerNode: Assigned container container_1614662779782_0001_01_000002 of capacity <memory:61316, vCores:1> on host algo-1:46605, which has 2 containers, <memory:62212, vCores:2> used and <memory:1357, vCores:14> available after allocation\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:36,918 INFO resourcemanager.RMAuditLogger: USER=root#011OPERATION=AM Allocated Container#011TARGET=SchedulerApp#011RESULT=SUCCESS#011APPID=application_1614662779782_0001#011CONTAINERID=container_1614662779782_0001_01_000002#011RESOURCE=<memory:61316, vCores:1>#011QUEUENAME=default\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:36,919 INFO capacity.ParentQueue: assignedContainer queue=root usedCapacity=0.97865313 absoluteUsedCapacity=0.97865313 used=<memory:62212, vCores:2> cluster=<memory:63569, vCores:16>\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:36,919 INFO capacity.CapacityScheduler: Allocation proposal accepted\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:37,237 INFO security.NMTokenSecretManagerInRM: Sending NMToken for nodeId : algo-1:46605 for container : container_1614662779782_0001_01_000002\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:37,238 INFO rmcontainer.RMContainerImpl: container_1614662779782_0001_01_000002 Container Transitioned from ALLOCATED to ACQUIRED\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/appWARNING:root:Skipping reading sagemaker.read_csv as it's part of whitelist\u001b[0m\n",
      "\u001b[34mWARNING:root:Skipping reading sagemaker.spark.join_tables as it's part of whitelist\u001b[0m\n",
      "\u001b[34mWARNING:root:Skipping reading sagemaker.spark.concatenate_datasets as it's part of whitelist\u001b[0m\n",
      "\u001b[34mWARNING:root:Skipping reading sagemaker.spark.cast_type as it's part of whitelist\u001b[0m\n",
      "\u001b[34mWARNING:root:Skipping reading sagemaker.spark.infer_and_cast_type as it's part of whitelist\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:36,918 INFO allocator.AbstractContainerAllocator: assignedContainer application attempt=appattempt_1614662779782_0001_000001 container=null queue=default clusterResource=<memory:63569, vCores:16> type=OFF_SWITCH requestedPartition=\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:36,918 INFO rmcontainer.RMContainerImpl: container_1614662779782_0001_01_000002 Container Transitioned from NEW to ALLOCATED\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:36,918 INFO fica.FiCaSchedulerNode: Assigned container container_1614662779782_0001_01_000002 of capacity <memory:61316, vCores:1> on host algo-1:46605, which has 2 containers, <memory:62212, vCores:2> used and <memory:1357, vCores:14> available after allocation\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:36,918 INFO resourcemanager.RMAuditLogger: USER=root#011OPERATION=AM Allocated Container#011TARGET=SchedulerApp#011RESULT=SUCCESS#011APPID=application_1614662779782_0001#011CONTAINERID=container_1614662779782_0001_01_000002#011RESOURCE=<memory:61316, vCores:1>#011QUEUENAME=default\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:36,919 INFO capacity.ParentQueue: assignedContainer queue=root usedCapacity=0.97865313 absoluteUsedCapacity=0.97865313 used=<memory:62212, vCores:2> cluster=<memory:63569, vCores:16>\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:36,919 INFO capacity.CapacityScheduler: Allocation proposal accepted\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:37,237 INFO security.NMTokenSecretManagerInRM: Sending NMToken for nodeId : algo-1:46605 for container : container_1614662779782_0001_01_000002\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:37,238 INFO rmcontainer.RMContainerImpl: container_1614662779782_0001_01_000002 Container Transitioned from ALLOCATED to ACQUIRED\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/appWARNING:root:Skipping reading sagemaker.read_csv as it's part of whitelist\u001b[0m\n",
      "\u001b[35mWARNING:root:Skipping reading sagemaker.spark.join_tables as it's part of whitelist\u001b[0m\n",
      "\u001b[35mWARNING:root:Skipping reading sagemaker.spark.concatenate_datasets as it's part of whitelist\u001b[0m\n",
      "\u001b[35mWARNING:root:Skipping reading sagemaker.spark.cast_type as it's part of whitelist\u001b[0m\n",
      "\u001b[35mWARNING:root:Skipping reading sagemaker.spark.infer_and_cast_type as it's part of whitelist\u001b[0m\n",
      "\u001b[34mINFO:root:Resolving logical graph\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:37,363 INFO util.TypeUtil: JVM Runtime does not support Modules\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:37,388 INFO ipc.Server: Auth successful for appattempt_1614662779782_0001_000001 (auth:SIMPLE)\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:37,391 INFO containermanager.ContainerManagerImpl: Start request for container_1614662779782_0001_01_000002 by user root\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:37,392 INFO nodemanager.NMAuditLogger: USER=root#011IP=10.0.189.145#011OPERATION=Start Container Request#011TARGET=ContainerManageImpl#011RESULT=SUCCESS#011APPID=application_1614662779782_0001#011CONTAINERID=container_1614662779782_0001_01_000002\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:37,392 INFO application.ApplicationImpl: Adding container_1614662779782_0001_01_000002 to application application_1614662779782_0001\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:37,394 INFO container.ContainerImpl: Container container_1614662779782_0001_01_000002 transitioned from NEW to LOCALIZING\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:37,394 INFO containermanager.AuxServices: Got event CONTAINER_INIT for appId application_1614662779782_0001\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:37,394 INFO container.ContainerImpl: Container container_1614662779782_0001_01_000002 transitioned from LOCALIZING to SCHEDULED\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:37,395 INFO scheduler.ContainerScheduler: Starting container [container_1614662779782_0001_01_000002]\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:37,409 INFO container.ContainerImpl: Container container_1614662779782_0001_01_000002 transitioned from SCHEDULED to RUNNING\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:37,409 INFO monitor.ContainersMonitorImpl: Starting resource-monitoring for container_1614662779782_0001_01_000002\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:37,411 INFO nodemanager.DefaultContainerExecutor: launchContainer: [bash, /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1614662779782_0001/container_1614662779782_0001_01_000002/default_container_executor.sh]\u001b[0m\n",
      "\u001b[34mHandling create event for file: /var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/prelaunch.out\u001b[0m\n",
      "\u001b[34mHandling create event for file: /var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/prelaunch.err\u001b[0m\n",
      "\u001b[34mHandling create event for file: /var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/launch_container.sh\u001b[0m\n",
      "\u001b[34mHandling create event for file: /var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/directory.info\u001b[0m\n",
      "\u001b[34mHandling create event for file: /var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stdout\u001b[0m\n",
      "\u001b[34mHandling create event for file: /var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr\u001b[0m\n",
      "\u001b[35mINFO:root:Resolving logical graph\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:37,363 INFO util.TypeUtil: JVM Runtime does not support Modules\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:37,388 INFO ipc.Server: Auth successful for appattempt_1614662779782_0001_000001 (auth:SIMPLE)\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:37,391 INFO containermanager.ContainerManagerImpl: Start request for container_1614662779782_0001_01_000002 by user root\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:37,392 INFO nodemanager.NMAuditLogger: USER=root#011IP=10.0.189.145#011OPERATION=Start Container Request#011TARGET=ContainerManageImpl#011RESULT=SUCCESS#011APPID=application_1614662779782_0001#011CONTAINERID=container_1614662779782_0001_01_000002\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:37,392 INFO application.ApplicationImpl: Adding container_1614662779782_0001_01_000002 to application application_1614662779782_0001\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:37,394 INFO container.ContainerImpl: Container container_1614662779782_0001_01_000002 transitioned from NEW to LOCALIZING\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:37,394 INFO containermanager.AuxServices: Got event CONTAINER_INIT for appId application_1614662779782_0001\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:37,394 INFO container.ContainerImpl: Container container_1614662779782_0001_01_000002 transitioned from LOCALIZING to SCHEDULED\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:37,395 INFO scheduler.ContainerScheduler: Starting container [container_1614662779782_0001_01_000002]\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:37,409 INFO container.ContainerImpl: Container container_1614662779782_0001_01_000002 transitioned from SCHEDULED to RUNNING\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:37,409 INFO monitor.ContainersMonitorImpl: Starting resource-monitoring for container_1614662779782_0001_01_000002\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:37,411 INFO nodemanager.DefaultContainerExecutor: launchContainer: [bash, /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1614662779782_0001/container_1614662779782_0001_01_000002/default_container_executor.sh]\u001b[0m\n",
      "\u001b[35mHandling create event for file: /var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/prelaunch.out\u001b[0m\n",
      "\u001b[35mHandling create event for file: /var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/prelaunch.err\u001b[0m\n",
      "\u001b[35mHandling create event for file: /var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/launch_container.sh\u001b[0m\n",
      "\u001b[35mHandling create event for file: /var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/directory.info\u001b[0m\n",
      "\u001b[35mHandling create event for file: /var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stdout\u001b[0m\n",
      "\u001b[35mHandling create event for file: /var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:37,920 INFO rmcontainer.RMContainerImpl: container_1614662779782_0001_01_000002 Container Transitioned from ACQUIRED to RUNNING\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:37,920 INFO rmcontainer.RMContainerImpl: container_1614662779782_0001_01_000002 Container Transitioned from ACQUIRED to RUNNING\u001b[0m\n",
      "\u001b[34mINFO:root:Adding head\u001b[0m\n",
      "\u001b[34mINFO:root:Adding caching\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:38,927 INFO datasources.InMemoryFileIndex: It took 35 ms to list leaf files for 1 paths.\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:39,149 INFO monitor.ContainersMonitorImpl: container_1614662779782_0001_01_000002's ip = 10.0.189.145, and hostname = algo-1\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:39,154 INFO monitor.ContainersMonitorImpl: Skipping monitoring container container_1614662779782_0001_01_000002 since CPU usage is not yet available.\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:39,165 INFO spark.SparkContext: Starting job: parquet at NativeMethodAccessorImpl.java:0\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:39,185 INFO scheduler.DAGScheduler: Got job 0 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:39,185 INFO scheduler.DAGScheduler: Final stage: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0)\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:39,185 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:39,186 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:39,191 INFO scheduler.DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:39,242 INFO memory.MemoryStore: Block broadcast_0 stored as values in memory (estimated size 88.8 KiB, free 1007.8 MiB)\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:39,282 INFO memory.MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 31.9 KiB, free 1007.7 MiB)\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:39,285 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on 10.0.189.145:39837 (size: 31.9 KiB, free: 1007.8 MiB)\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:39,287 INFO spark.SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1240\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:39,302 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:39,302 INFO cluster.YarnScheduler: Adding task set 0.0 with 1 tasks\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:39,652 INFO resource.ResourceProfile: Default ResourceProfile created, executor resources: Map(memoryOverhead -> name: memoryOverhead, amount: 5574, script: , vendor: , cores -> name: cores, amount: 16, script: , vendor: , memory -> name: memory, amount: 55742, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\u001b[0m\n",
      "\u001b[35mINFO:root:Adding head\u001b[0m\n",
      "\u001b[35mINFO:root:Adding caching\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:38,927 INFO datasources.InMemoryFileIndex: It took 35 ms to list leaf files for 1 paths.\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:39,149 INFO monitor.ContainersMonitorImpl: container_1614662779782_0001_01_000002's ip = 10.0.189.145, and hostname = algo-1\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:39,154 INFO monitor.ContainersMonitorImpl: Skipping monitoring container container_1614662779782_0001_01_000002 since CPU usage is not yet available.\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:39,165 INFO spark.SparkContext: Starting job: parquet at NativeMethodAccessorImpl.java:0\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:39,185 INFO scheduler.DAGScheduler: Got job 0 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:39,185 INFO scheduler.DAGScheduler: Final stage: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0)\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:39,185 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:39,186 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:39,191 INFO scheduler.DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:39,242 INFO memory.MemoryStore: Block broadcast_0 stored as values in memory (estimated size 88.8 KiB, free 1007.8 MiB)\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:39,282 INFO memory.MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 31.9 KiB, free 1007.7 MiB)\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:39,285 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on 10.0.189.145:39837 (size: 31.9 KiB, free: 1007.8 MiB)\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:39,287 INFO spark.SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1240\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:39,302 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:39,302 INFO cluster.YarnScheduler: Adding task set 0.0 with 1 tasks\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:39,652 INFO resource.ResourceProfile: Default ResourceProfile created, executor resources: Map(memoryOverhead -> name: memoryOverhead, amount: 5574, script: , vendor: , cores -> name: cores, amount: 16, script: , vendor: , memory -> name: memory, amount: 55742, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:40,261 INFO scheduler.AppSchedulingInfo: checking for deactivate of application :application_1614662779782_0001\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:40,320 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.0.189.145:42168) with ID 1\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:38,522 INFO executor.CoarseGrainedExecutorBackend: Started daemon with process name: 1279@algo-1\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:38,529 INFO util.SignalUtils: Registered signal handler for TERM\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:38,530 INFO util.SignalUtils: Registered signal handler for HUP\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:38,531 INFO util.SignalUtils: Registered signal handler for INT\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:39,136 INFO spark.SecurityManager: Changing view acls to: root\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:39,136 INFO spark.SecurityManager: Changing modify acls to: root\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:39,137 INFO spark.SecurityManager: Changing view acls groups to: \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:39,137 INFO spark.SecurityManager: Changing modify acls groups to: \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:39,137 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:39,592 INFO client.TransportClientFactory: Successfully created connection to /10.0.189.145:38479 after 87 ms (0 ms spent in bootstraps)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:39,729 INFO spark.SecurityManager: Changing view acls to: root\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:39,729 INFO spark.SecurityManager: Changing modify acls to: root\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:39,729 INFO spark.SecurityManager: Changing view acls groups to: \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:39,729 INFO spark.SecurityManager: Changing modify acls groups to: \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:39,729 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:39,780 INFO client.TransportClientFactory: Successfully created connection to /10.0.189.145:38479 after 1 ms (0 ms spent in bootstraps)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:39,846 INFO storage.DiskBlockManager: Created local directory at /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1614662779782_0001/blockmgr-7ca910c7-0406-4b3b-a974-e5906365d05e\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:39,888 INFO memory.MemoryStore: MemoryStore started with capacity 28.9 GiB\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:40,184 INFO executor.YarnCoarseGrainedExecutorBackend: Connecting to driver: spark://CoarseGrainedScheduler@10.0.189.145:38479\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:40,194 INFO resource.ResourceUtils: ==============================================================\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:40,195 INFO resource.ResourceUtils: Resources for spark.executor:\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:40,196 INFO resource.ResourceUtils: ==============================================================\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:40,325 INFO executor.YarnCoarseGrainedExecutorBackend: Successfully registered with driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:40,330 INFO executor.Executor: Starting executor ID 1 on host algo-1\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:40,373 WARN executor.YarnCoarseGrainedExecutorBackend: eagerFSInit: Unable to eagerly init filesystem s3://does/not/exist\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] org.apache.hadoop.fs.UnsupportedFileSystemException: No FileSystem for scheme \"s3\"\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] #011at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3336)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] #011at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3356)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] #011at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:123)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] #011at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3407)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] #011at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3375)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] #011at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:486)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] #011at org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] #011at org.apache.spark.executor.CoarseGrainedExecutorBackend.$anonfun$eagerFSInit$1(CoarseGrainedExecutorBackend.scala:274)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] #011at scala.collection.parallel.mutable.ParArray$ParArrayIterator.flatmap2combiner(ParArray.scala:419)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] #011at scala.collection.parallel.ParIterableLike$FlatMap.leaf(ParIterableLike.scala:1074)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] #011at scala.collection.parallel.Task.$anonfun$tryLeaf$1(Tasks.scala:53)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] #011at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] #011at scala.util.control.Breaks$$anon$1.catchBreak(Breaks.scala:67)\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:40,445 INFO storage.BlockManagerMasterEndpoint: Registering block manager algo-1:40903 with 28.9 GiB RAM, BlockManagerId(1, algo-1, 40903, None)\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:40,525 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, algo-1, executor 1, partition 0, PROCESS_LOCAL, 7557 bytes)\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:40,261 INFO scheduler.AppSchedulingInfo: checking for deactivate of application :application_1614662779782_0001\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:40,320 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.0.189.145:42168) with ID 1\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:38,522 INFO executor.CoarseGrainedExecutorBackend: Started daemon with process name: 1279@algo-1\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:38,529 INFO util.SignalUtils: Registered signal handler for TERM\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:38,530 INFO util.SignalUtils: Registered signal handler for HUP\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:38,531 INFO util.SignalUtils: Registered signal handler for INT\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:39,136 INFO spark.SecurityManager: Changing view acls to: root\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:39,136 INFO spark.SecurityManager: Changing modify acls to: root\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:39,137 INFO spark.SecurityManager: Changing view acls groups to: \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:39,137 INFO spark.SecurityManager: Changing modify acls groups to: \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:39,137 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:39,592 INFO client.TransportClientFactory: Successfully created connection to /10.0.189.145:38479 after 87 ms (0 ms spent in bootstraps)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:39,729 INFO spark.SecurityManager: Changing view acls to: root\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:39,729 INFO spark.SecurityManager: Changing modify acls to: root\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:39,729 INFO spark.SecurityManager: Changing view acls groups to: \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:39,729 INFO spark.SecurityManager: Changing modify acls groups to: \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:39,729 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:39,780 INFO client.TransportClientFactory: Successfully created connection to /10.0.189.145:38479 after 1 ms (0 ms spent in bootstraps)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:39,846 INFO storage.DiskBlockManager: Created local directory at /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1614662779782_0001/blockmgr-7ca910c7-0406-4b3b-a974-e5906365d05e\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:39,888 INFO memory.MemoryStore: MemoryStore started with capacity 28.9 GiB\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:40,184 INFO executor.YarnCoarseGrainedExecutorBackend: Connecting to driver: spark://CoarseGrainedScheduler@10.0.189.145:38479\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:40,194 INFO resource.ResourceUtils: ==============================================================\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:40,195 INFO resource.ResourceUtils: Resources for spark.executor:\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:40,196 INFO resource.ResourceUtils: ==============================================================\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:40,325 INFO executor.YarnCoarseGrainedExecutorBackend: Successfully registered with driver\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:40,330 INFO executor.Executor: Starting executor ID 1 on host algo-1\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:40,373 WARN executor.YarnCoarseGrainedExecutorBackend: eagerFSInit: Unable to eagerly init filesystem s3://does/not/exist\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] org.apache.hadoop.fs.UnsupportedFileSystemException: No FileSystem for scheme \"s3\"\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] #011at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3336)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] #011at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3356)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] #011at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:123)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] #011at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3407)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] #011at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3375)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] #011at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:486)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] #011at org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] #011at org.apache.spark.executor.CoarseGrainedExecutorBackend.$anonfun$eagerFSInit$1(CoarseGrainedExecutorBackend.scala:274)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] #011at scala.collection.parallel.mutable.ParArray$ParArrayIterator.flatmap2combiner(ParArray.scala:419)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] #011at scala.collection.parallel.ParIterableLike$FlatMap.leaf(ParIterableLike.scala:1074)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] #011at scala.collection.parallel.Task.$anonfun$tryLeaf$1(Tasks.scala:53)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] #011at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] #011at scala.util.control.Breaks$$anon$1.catchBreak(Breaks.scala:67)\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:40,445 INFO storage.BlockManagerMasterEndpoint: Registering block manager algo-1:40903 with 28.9 GiB RAM, BlockManagerId(1, algo-1, 40903, None)\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:40,525 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, algo-1, executor 1, partition 0, PROCESS_LOCAL, 7557 bytes)\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:40,795 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on algo-1:40903 (size: 31.9 KiB, free: 28.9 GiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] #011at scala.collection.parallel.Task.tryLeaf(Tasks.scala:56)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] #011at scala.collection.parallel.Task.tryLeaf$(Tasks.scala:50)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] #011at scala.collection.parallel.ParIterableLike$FlatMap.tryLeaf(ParIterableLike.scala:1070)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] #011at scala.collection.parallel.FutureTasks.$anonfun$exec$5(Tasks.scala:499)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] #011at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] #011at scala.util.Success.$anonfun$map$1(Try.scala:255)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] #011at scala.util.Success.map(Try.scala:213)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] #011at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] #011at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] #011at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] #011at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] #011at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] #011at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] #011at java.lang.Thread.run(Thread.java:748)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:40,422 INFO util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 40903.\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:40,423 INFO netty.NettyBlockTransferService: Server created on algo-1:40903\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:40,424 INFO storage.BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:40,438 INFO storage.BlockManagerMaster: Registering BlockManager BlockManagerId(1, algo-1, 40903, None)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:40,447 INFO storage.BlockManagerMaster: Registered BlockManager BlockManagerId(1, algo-1, 40903, None)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:40,448 INFO storage.BlockManager: Initialized BlockManager: BlockManagerId(1, algo-1, 40903, None)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:40,547 INFO executor.YarnCoarseGrainedExecutorBackend: Got assigned task 0\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:40,555 INFO executor.Executor: Running task 0.0 in stage 0.0 (TID 0)\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:41,509 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 993 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:41,511 INFO cluster.YarnScheduler: Removed TaskSet 0.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:41,517 INFO scheduler.DAGScheduler: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0) finished in 2.312 s\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:41,520 INFO scheduler.DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:41,520 INFO cluster.YarnScheduler: Killing all running tasks in stage 0: Stage finished\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:41,521 INFO scheduler.DAGScheduler: Job 0 finished: parquet at NativeMethodAccessorImpl.java:0, took 2.355840 s\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:40,795 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on algo-1:40903 (size: 31.9 KiB, free: 28.9 GiB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] #011at scala.collection.parallel.Task.tryLeaf(Tasks.scala:56)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] #011at scala.collection.parallel.Task.tryLeaf$(Tasks.scala:50)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] #011at scala.collection.parallel.ParIterableLike$FlatMap.tryLeaf(ParIterableLike.scala:1070)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] #011at scala.collection.parallel.FutureTasks.$anonfun$exec$5(Tasks.scala:499)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] #011at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] #011at scala.util.Success.$anonfun$map$1(Try.scala:255)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] #011at scala.util.Success.map(Try.scala:213)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] #011at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] #011at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] #011at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] #011at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] #011at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] #011at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] #011at java.lang.Thread.run(Thread.java:748)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:40,422 INFO util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 40903.\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:40,423 INFO netty.NettyBlockTransferService: Server created on algo-1:40903\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:40,424 INFO storage.BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:40,438 INFO storage.BlockManagerMaster: Registering BlockManager BlockManagerId(1, algo-1, 40903, None)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:40,447 INFO storage.BlockManagerMaster: Registered BlockManager BlockManagerId(1, algo-1, 40903, None)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:40,448 INFO storage.BlockManager: Initialized BlockManager: BlockManagerId(1, algo-1, 40903, None)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:40,547 INFO executor.YarnCoarseGrainedExecutorBackend: Got assigned task 0\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:40,555 INFO executor.Executor: Running task 0.0 in stage 0.0 (TID 0)\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:41,509 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 993 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:41,511 INFO cluster.YarnScheduler: Removed TaskSet 0.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:41,517 INFO scheduler.DAGScheduler: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0) finished in 2.312 s\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:41,520 INFO scheduler.DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:41,520 INFO cluster.YarnScheduler: Killing all running tasks in stage 0: Stage finished\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:41,521 INFO scheduler.DAGScheduler: Job 0 finished: parquet at NativeMethodAccessorImpl.java:0, took 2.355840 s\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:41,954 INFO storage.BlockManagerInfo: Removed broadcast_0_piece0 on 10.0.189.145:39837 in memory (size: 31.9 KiB, free: 1007.8 MiB)\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:41,965 INFO storage.BlockManagerInfo: Removed broadcast_0_piece0 on algo-1:40903 in memory (size: 31.9 KiB, free: 28.9 GiB)\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:41,954 INFO storage.BlockManagerInfo: Removed broadcast_0_piece0 on 10.0.189.145:39837 in memory (size: 31.9 KiB, free: 1007.8 MiB)\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:41,965 INFO storage.BlockManagerInfo: Removed broadcast_0_piece0 on algo-1:40903 in memory (size: 31.9 KiB, free: 28.9 GiB)\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:43,130 INFO impl.MetricsSystemImpl: s3a-file-system metrics system started\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:43,130 INFO impl.MetricsSystemImpl: s3a-file-system metrics system started\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:44,087 INFO datasources.FileSourceStrategy: Pruning directories with: \u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:44,092 INFO datasources.FileSourceStrategy: Pushed Filters: \u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:44,092 INFO datasources.FileSourceStrategy: Post-Scan Filters: \u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:44,094 INFO datasources.FileSourceStrategy: Output Data Schema: struct<adid: string, event_name: string, event_datetime: string, product_id: string, order_id: string ... 3 more fields>\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:44,297 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:44,087 INFO datasources.FileSourceStrategy: Pruning directories with: \u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:44,092 INFO datasources.FileSourceStrategy: Pushed Filters: \u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:44,092 INFO datasources.FileSourceStrategy: Post-Scan Filters: \u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:44,094 INFO datasources.FileSourceStrategy: Output Data Schema: struct<adid: string, event_name: string, event_datetime: string, product_id: string, order_id: string ... 3 more fields>\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:44,297 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:44,297 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:44,298 INFO output.DirectFileOutputCommitter: Direct Write: DISABLED\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:44,298 INFO datasources.SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.DirectFileOutputCommitter\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:44,297 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:44,298 INFO output.DirectFileOutputCommitter: Direct Write: DISABLED\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:44,298 INFO datasources.SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.DirectFileOutputCommitter\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:45,116 INFO codegen.CodeGenerator: Code generated in 198.913158 ms\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:45,142 INFO memory.MemoryStore: Block broadcast_1 stored as values in memory (estimated size 318.6 KiB, free 1007.5 MiB)\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:45,153 INFO memory.MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 30.5 KiB, free 1007.5 MiB)\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:45,154 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on 10.0.189.145:39837 (size: 30.5 KiB, free: 1007.8 MiB)\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:45,154 INFO spark.SparkContext: Created broadcast 1 from csv at NativeMethodAccessorImpl.java:0\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:45,183 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 5653727 bytes, open cost is considered as scanning 4194304 bytes, number of split files: 22, prefetch: false\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:45,188 INFO execution.FileSourceScanExec: relation: None, fileSplitsInPartitionHistogram: Vector((1 fileSplits,22))\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:45,228 INFO spark.SparkContext: Starting job: csv at NativeMethodAccessorImpl.java:0\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:45,230 INFO scheduler.DAGScheduler: Got job 1 (csv at NativeMethodAccessorImpl.java:0) with 22 output partitions\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:45,230 INFO scheduler.DAGScheduler: Final stage: ResultStage 1 (csv at NativeMethodAccessorImpl.java:0)\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:45,230 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:45,230 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:45,231 INFO scheduler.DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[4] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:45,313 INFO memory.MemoryStore: Block broadcast_2 stored as values in memory (estimated size 188.9 KiB, free 1007.3 MiB)\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:45,315 INFO memory.MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 67.4 KiB, free 1007.3 MiB)\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:45,316 INFO storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on 10.0.189.145:39837 (size: 67.4 KiB, free: 1007.8 MiB)\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:45,316 INFO spark.SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1240\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:45,116 INFO codegen.CodeGenerator: Code generated in 198.913158 ms\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:45,142 INFO memory.MemoryStore: Block broadcast_1 stored as values in memory (estimated size 318.6 KiB, free 1007.5 MiB)\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:45,153 INFO memory.MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 30.5 KiB, free 1007.5 MiB)\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:45,154 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on 10.0.189.145:39837 (size: 30.5 KiB, free: 1007.8 MiB)\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:45,154 INFO spark.SparkContext: Created broadcast 1 from csv at NativeMethodAccessorImpl.java:0\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:45,183 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 5653727 bytes, open cost is considered as scanning 4194304 bytes, number of split files: 22, prefetch: false\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:45,188 INFO execution.FileSourceScanExec: relation: None, fileSplitsInPartitionHistogram: Vector((1 fileSplits,22))\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:45,228 INFO spark.SparkContext: Starting job: csv at NativeMethodAccessorImpl.java:0\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:45,230 INFO scheduler.DAGScheduler: Got job 1 (csv at NativeMethodAccessorImpl.java:0) with 22 output partitions\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:45,230 INFO scheduler.DAGScheduler: Final stage: ResultStage 1 (csv at NativeMethodAccessorImpl.java:0)\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:45,230 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:45,230 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:45,231 INFO scheduler.DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[4] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:45,313 INFO memory.MemoryStore: Block broadcast_2 stored as values in memory (estimated size 188.9 KiB, free 1007.3 MiB)\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:45,315 INFO memory.MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 67.4 KiB, free 1007.3 MiB)\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:45,316 INFO storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on 10.0.189.145:39837 (size: 67.4 KiB, free: 1007.8 MiB)\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:45,316 INFO spark.SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1240\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:45,317 INFO scheduler.DAGScheduler: Submitting 22 missing tasks from ResultStage 1 (MapPartitionsRDD[4] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:45,317 INFO cluster.YarnScheduler: Adding task set 1.0 with 22 tasks\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:45,321 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1, algo-1, executor 1, partition 0, PROCESS_LOCAL, 7808 bytes)\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:45,321 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 1.0 (TID 2, algo-1, executor 1, partition 1, PROCESS_LOCAL, 7808 bytes)\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:45,321 INFO scheduler.TaskSetManager: Starting task 2.0 in stage 1.0 (TID 3, algo-1, executor 1, partition 2, PROCESS_LOCAL, 7808 bytes)\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:45,322 INFO scheduler.TaskSetManager: Starting task 3.0 in stage 1.0 (TID 4, algo-1, executor 1, partition 3, PROCESS_LOCAL, 7808 bytes)\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:45,322 INFO scheduler.TaskSetManager: Starting task 4.0 in stage 1.0 (TID 5, algo-1, executor 1, partition 4, PROCESS_LOCAL, 7808 bytes)\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:45,322 INFO scheduler.TaskSetManager: Starting task 5.0 in stage 1.0 (TID 6, algo-1, executor 1, partition 5, PROCESS_LOCAL, 7808 bytes)\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:45,323 INFO scheduler.TaskSetManager: Starting task 6.0 in stage 1.0 (TID 7, algo-1, executor 1, partition 6, PROCESS_LOCAL, 7808 bytes)\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:45,323 INFO scheduler.TaskSetManager: Starting task 7.0 in stage 1.0 (TID 8, algo-1, executor 1, partition 7, PROCESS_LOCAL, 7808 bytes)\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:45,323 INFO scheduler.TaskSetManager: Starting task 8.0 in stage 1.0 (TID 9, algo-1, executor 1, partition 8, PROCESS_LOCAL, 7808 bytes)\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:45,324 INFO scheduler.TaskSetManager: Starting task 9.0 in stage 1.0 (TID 10, algo-1, executor 1, partition 9, PROCESS_LOCAL, 7808 bytes)\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:45,324 INFO scheduler.TaskSetManager: Starting task 10.0 in stage 1.0 (TID 11, algo-1, executor 1, partition 10, PROCESS_LOCAL, 7808 bytes)\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:45,324 INFO scheduler.TaskSetManager: Starting task 11.0 in stage 1.0 (TID 12, algo-1, executor 1, partition 11, PROCESS_LOCAL, 7808 bytes)\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:45,325 INFO scheduler.TaskSetManager: Starting task 12.0 in stage 1.0 (TID 13, algo-1, executor 1, partition 12, PROCESS_LOCAL, 7808 bytes)\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:45,325 INFO scheduler.TaskSetManager: Starting task 13.0 in stage 1.0 (TID 14, algo-1, executor 1, partition 13, PROCESS_LOCAL, 7808 bytes)\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:45,325 INFO scheduler.TaskSetManager: Starting task 14.0 in stage 1.0 (TID 15, algo-1, executor 1, partition 14, PROCESS_LOCAL, 7808 bytes)\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:45,326 INFO scheduler.TaskSetManager: Starting task 15.0 in stage 1.0 (TID 16, algo-1, executor 1, partition 15, PROCESS_LOCAL, 7808 bytes)\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:45,379 INFO storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on algo-1:40903 (size: 67.4 KiB, free: 28.9 GiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:40,675 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 0 with 1 pieces (estimated total size 4.0 MiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:40,739 INFO client.TransportClientFactory: Successfully created connection to /10.0.189.145:39837 after 1 ms (0 ms spent in bootstraps)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:40,790 INFO memory.MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 31.9 KiB, free 28.9 GiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:40,799 INFO broadcast.TorrentBroadcast: Reading broadcast variable 0 took 124 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:40,886 INFO memory.MemoryStore: Block broadcast_0 stored as values in memory (estimated size 88.8 KiB, free 28.9 GiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:41,496 INFO executor.Executor: Finished task 0.0 in stage 0.0 (TID 0). 1772 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:45,328 INFO executor.YarnCoarseGrainedExecutorBackend: Got assigned task 1\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:45,328 INFO executor.Executor: Running task 0.0 in stage 1.0 (TID 1)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:45,329 INFO executor.YarnCoarseGrainedExecutorBackend: Got assigned task 2\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:45,329 INFO executor.YarnCoarseGrainedExecutorBackend: Got assigned task 3\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:45,330 INFO executor.Executor: Running task 2.0 in stage 1.0 (TID 3)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:45,330 INFO executor.Executor: Running task 1.0 in stage 1.0 (TID 2)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:45,331 INFO executor.YarnCoarseGrainedExecutorBackend: Got assigned task 4\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:45,317 INFO scheduler.DAGScheduler: Submitting 22 missing tasks from ResultStage 1 (MapPartitionsRDD[4] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:45,317 INFO cluster.YarnScheduler: Adding task set 1.0 with 22 tasks\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:45,321 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1, algo-1, executor 1, partition 0, PROCESS_LOCAL, 7808 bytes)\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:45,321 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 1.0 (TID 2, algo-1, executor 1, partition 1, PROCESS_LOCAL, 7808 bytes)\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:45,321 INFO scheduler.TaskSetManager: Starting task 2.0 in stage 1.0 (TID 3, algo-1, executor 1, partition 2, PROCESS_LOCAL, 7808 bytes)\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:45,322 INFO scheduler.TaskSetManager: Starting task 3.0 in stage 1.0 (TID 4, algo-1, executor 1, partition 3, PROCESS_LOCAL, 7808 bytes)\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:45,322 INFO scheduler.TaskSetManager: Starting task 4.0 in stage 1.0 (TID 5, algo-1, executor 1, partition 4, PROCESS_LOCAL, 7808 bytes)\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:45,322 INFO scheduler.TaskSetManager: Starting task 5.0 in stage 1.0 (TID 6, algo-1, executor 1, partition 5, PROCESS_LOCAL, 7808 bytes)\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:45,323 INFO scheduler.TaskSetManager: Starting task 6.0 in stage 1.0 (TID 7, algo-1, executor 1, partition 6, PROCESS_LOCAL, 7808 bytes)\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:45,323 INFO scheduler.TaskSetManager: Starting task 7.0 in stage 1.0 (TID 8, algo-1, executor 1, partition 7, PROCESS_LOCAL, 7808 bytes)\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:45,323 INFO scheduler.TaskSetManager: Starting task 8.0 in stage 1.0 (TID 9, algo-1, executor 1, partition 8, PROCESS_LOCAL, 7808 bytes)\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:45,324 INFO scheduler.TaskSetManager: Starting task 9.0 in stage 1.0 (TID 10, algo-1, executor 1, partition 9, PROCESS_LOCAL, 7808 bytes)\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:45,324 INFO scheduler.TaskSetManager: Starting task 10.0 in stage 1.0 (TID 11, algo-1, executor 1, partition 10, PROCESS_LOCAL, 7808 bytes)\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:45,324 INFO scheduler.TaskSetManager: Starting task 11.0 in stage 1.0 (TID 12, algo-1, executor 1, partition 11, PROCESS_LOCAL, 7808 bytes)\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:45,325 INFO scheduler.TaskSetManager: Starting task 12.0 in stage 1.0 (TID 13, algo-1, executor 1, partition 12, PROCESS_LOCAL, 7808 bytes)\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:45,325 INFO scheduler.TaskSetManager: Starting task 13.0 in stage 1.0 (TID 14, algo-1, executor 1, partition 13, PROCESS_LOCAL, 7808 bytes)\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:45,325 INFO scheduler.TaskSetManager: Starting task 14.0 in stage 1.0 (TID 15, algo-1, executor 1, partition 14, PROCESS_LOCAL, 7808 bytes)\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:45,326 INFO scheduler.TaskSetManager: Starting task 15.0 in stage 1.0 (TID 16, algo-1, executor 1, partition 15, PROCESS_LOCAL, 7808 bytes)\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:45,379 INFO storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on algo-1:40903 (size: 67.4 KiB, free: 28.9 GiB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:40,675 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 0 with 1 pieces (estimated total size 4.0 MiB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:40,739 INFO client.TransportClientFactory: Successfully created connection to /10.0.189.145:39837 after 1 ms (0 ms spent in bootstraps)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:40,790 INFO memory.MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 31.9 KiB, free 28.9 GiB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:40,799 INFO broadcast.TorrentBroadcast: Reading broadcast variable 0 took 124 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:40,886 INFO memory.MemoryStore: Block broadcast_0 stored as values in memory (estimated size 88.8 KiB, free 28.9 GiB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:41,496 INFO executor.Executor: Finished task 0.0 in stage 0.0 (TID 0). 1772 bytes result sent to driver\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:45,328 INFO executor.YarnCoarseGrainedExecutorBackend: Got assigned task 1\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:45,328 INFO executor.Executor: Running task 0.0 in stage 1.0 (TID 1)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:45,329 INFO executor.YarnCoarseGrainedExecutorBackend: Got assigned task 2\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:45,329 INFO executor.YarnCoarseGrainedExecutorBackend: Got assigned task 3\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:45,330 INFO executor.Executor: Running task 2.0 in stage 1.0 (TID 3)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:45,330 INFO executor.Executor: Running task 1.0 in stage 1.0 (TID 2)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:45,331 INFO executor.YarnCoarseGrainedExecutorBackend: Got assigned task 4\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:45,332 INFO executor.YarnCoarseGrainedExecutorBackend: Got assigned task 5\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:45,332 INFO executor.Executor: Running task 3.0 in stage 1.0 (TID 4)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:45,332 INFO executor.Executor: Running task 4.0 in stage 1.0 (TID 5)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:45,333 INFO executor.YarnCoarseGrainedExecutorBackend: Got assigned task 6\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:45,333 INFO executor.YarnCoarseGrainedExecutorBackend: Got assigned task 7\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:45,334 INFO executor.Executor: Running task 5.0 in stage 1.0 (TID 6)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:45,334 INFO executor.YarnCoarseGrainedExecutorBackend: Got assigned task 8\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:45,334 INFO executor.Executor: Running task 6.0 in stage 1.0 (TID 7)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:45,334 INFO executor.Executor: Running task 7.0 in stage 1.0 (TID 8)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:45,335 INFO executor.YarnCoarseGrainedExecutorBackend: Got assigned task 9\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:45,335 INFO executor.Executor: Running task 8.0 in stage 1.0 (TID 9)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:45,336 INFO executor.YarnCoarseGrainedExecutorBackend: Got assigned task 10\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:45,338 INFO executor.YarnCoarseGrainedExecutorBackend: Got assigned task 11\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:45,338 INFO executor.Executor: Running task 9.0 in stage 1.0 (TID 10)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:45,339 INFO executor.Executor: Running task 10.0 in stage 1.0 (TID 11)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:45,339 INFO executor.YarnCoarseGrainedExecutorBackend: Got assigned task 12\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:45,340 INFO executor.Executor: Running task 11.0 in stage 1.0 (TID 12)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:45,340 INFO executor.YarnCoarseGrainedExecutorBackend: Got assigned task 13\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:45,344 INFO executor.YarnCoarseGrainedExecutorBackend: Got assigned task 14\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:45,344 INFO executor.Executor: Running task 12.0 in stage 1.0 (TID 13)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:45,346 INFO executor.YarnCoarseGrainedExecutorBackend: Got assigned task 15\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:45,346 INFO executor.Executor: Running task 13.0 in stage 1.0 (TID 14)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:45,349 INFO executor.YarnCoarseGrainedExecutorBackend: Got assigned task 16\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:45,349 INFO executor.Executor: Running task 14.0 in stage 1.0 (TID 15)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:45,364 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 2 with 1 pieces (estimated total size 4.0 MiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:45,365 INFO executor.Executor: Running task 15.0 in stage 1.0 (TID 16)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:45,377 INFO memory.MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 67.4 KiB, free 28.9 GiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:45,380 INFO broadcast.TorrentBroadcast: Reading broadcast variable 2 took 15 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:45,332 INFO executor.YarnCoarseGrainedExecutorBackend: Got assigned task 5\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:45,332 INFO executor.Executor: Running task 3.0 in stage 1.0 (TID 4)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:45,332 INFO executor.Executor: Running task 4.0 in stage 1.0 (TID 5)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:45,333 INFO executor.YarnCoarseGrainedExecutorBackend: Got assigned task 6\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:45,333 INFO executor.YarnCoarseGrainedExecutorBackend: Got assigned task 7\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:45,334 INFO executor.Executor: Running task 5.0 in stage 1.0 (TID 6)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:45,334 INFO executor.YarnCoarseGrainedExecutorBackend: Got assigned task 8\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:45,334 INFO executor.Executor: Running task 6.0 in stage 1.0 (TID 7)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:45,334 INFO executor.Executor: Running task 7.0 in stage 1.0 (TID 8)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:45,335 INFO executor.YarnCoarseGrainedExecutorBackend: Got assigned task 9\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:45,335 INFO executor.Executor: Running task 8.0 in stage 1.0 (TID 9)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:45,336 INFO executor.YarnCoarseGrainedExecutorBackend: Got assigned task 10\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:45,338 INFO executor.YarnCoarseGrainedExecutorBackend: Got assigned task 11\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:45,338 INFO executor.Executor: Running task 9.0 in stage 1.0 (TID 10)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:45,339 INFO executor.Executor: Running task 10.0 in stage 1.0 (TID 11)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:45,339 INFO executor.YarnCoarseGrainedExecutorBackend: Got assigned task 12\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:45,340 INFO executor.Executor: Running task 11.0 in stage 1.0 (TID 12)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:45,340 INFO executor.YarnCoarseGrainedExecutorBackend: Got assigned task 13\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:45,344 INFO executor.YarnCoarseGrainedExecutorBackend: Got assigned task 14\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:45,344 INFO executor.Executor: Running task 12.0 in stage 1.0 (TID 13)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:45,346 INFO executor.YarnCoarseGrainedExecutorBackend: Got assigned task 15\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:45,346 INFO executor.Executor: Running task 13.0 in stage 1.0 (TID 14)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:45,349 INFO executor.YarnCoarseGrainedExecutorBackend: Got assigned task 16\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:45,349 INFO executor.Executor: Running task 14.0 in stage 1.0 (TID 15)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:45,364 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 2 with 1 pieces (estimated total size 4.0 MiB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:45,365 INFO executor.Executor: Running task 15.0 in stage 1.0 (TID 16)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:45,377 INFO memory.MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 67.4 KiB, free 28.9 GiB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:45,380 INFO broadcast.TorrentBroadcast: Reading broadcast variable 2 took 15 ms\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:47,322 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on algo-1:40903 (size: 30.5 KiB, free: 28.9 GiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:45,381 INFO memory.MemoryStore: Block broadcast_2 stored as values in memory (estimated size 188.9 KiB, free 28.9 GiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:46,133 INFO codegen.CodeGenerator: Code generated in 208.686641 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:46,287 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:46,306 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:46,306 INFO impl.MetricsSystemImpl: s3a-file-system metrics system started\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:47,304 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:47,304 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:47,304 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:47,304 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:47,304 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:47,304 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:47,304 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:47,304 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:47,305 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:47,305 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:47,305 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:47,305 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:47,305 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:47,305 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:47,306 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:47,306 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:47,306 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:47,306 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:47,306 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:47,306 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:47,306 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:47,322 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on algo-1:40903 (size: 30.5 KiB, free: 28.9 GiB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:45,381 INFO memory.MemoryStore: Block broadcast_2 stored as values in memory (estimated size 188.9 KiB, free 28.9 GiB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:46,133 INFO codegen.CodeGenerator: Code generated in 208.686641 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:46,287 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:46,306 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:46,306 INFO impl.MetricsSystemImpl: s3a-file-system metrics system started\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:47,304 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:47,304 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:47,304 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:47,304 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:47,304 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:47,304 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:47,304 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:47,304 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:47,305 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:47,305 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:47,305 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:47,305 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:47,305 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:47,305 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:47,306 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:47,306 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:47,306 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:47,306 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:47,306 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:47,306 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:47,306 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:47,306 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:47,306 INFO output.DirectFileOutputCommitter: Direct Write: DISABLED\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:47,308 INFO datasources.SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.DirectFileOutputCommitter\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:47,308 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:47,308 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:47,308 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:47,308 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:47,308 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:47,308 INFO output.DirectFileOutputCommitter: Direct Write: DISABLED\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:47,309 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:47,309 INFO datasources.SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.DirectFileOutputCommitter\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:47,309 INFO output.DirectFileOutputCommitter: Direct Write: DISABLED\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:47,309 INFO output.DirectFileOutputCommitter: Direct Write: DISABLED\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:47,309 INFO output.DirectFileOutputCommitter: Direct Write: DISABLED\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:47,309 INFO output.DirectFileOutputCommitter: Direct Write: DISABLED\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:47,310 INFO datasources.SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.DirectFileOutputCommitter\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:47,310 INFO datasources.SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.DirectFileOutputCommitter\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:47,309 INFO output.DirectFileOutputCommitter: Direct Write: DISABLED\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:47,309 INFO output.DirectFileOutputCommitter: Direct Write: DISABLED\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:47,309 INFO output.DirectFileOutputCommitter: Direct Write: DISABLED\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:47,310 INFO datasources.SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.DirectFileOutputCommitter\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:47,310 INFO datasources.SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.DirectFileOutputCommitter\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:47,309 INFO output.DirectFileOutputCommitter: Direct Write: DISABLED\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:47,310 INFO datasources.SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.DirectFileOutputCommitter\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:47,309 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:47,311 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:47,310 INFO datasources.SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.DirectFileOutputCommitter\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:47,310 INFO datasources.SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.DirectFileOutputCommitter\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:47,309 INFO datasources.SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.DirectFileOutputCommitter\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:47,309 INFO output.DirectFileOutputCommitter: Direct Write: DISABLED\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:47,309 INFO output.DirectFileOutputCommitter: Direct Write: DISABLED\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:47,309 INFO output.DirectFileOutputCommitter: Direct Write: DISABLED\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:47,312 INFO datasources.FileScanRDD: TID: 9 - Reading current file: path: file:///opt/ml/processing/event_all/20210302_052556_00048_8ak59_54bbf1c6-7f11-40cd-b1d9-50cc826f5daf, range: 0-4015480, partition values: [empty row], isDataPresent: false\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:47,312 INFO datasources.SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.DirectFileOutputCommitter\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:47,309 INFO output.DirectFileOutputCommitter: Direct Write: DISABLED\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:47,312 INFO datasources.SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.DirectFileOutputCommitter\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:47,312 INFO datasources.FileScanRDD: TID: 4 - Reading current file: path: file:///opt/ml/processing/event_all/20210302_052556_00048_8ak59_1f49b21c-5d8c-4521-8e7a-1151e60f2c9b, range: 0-5077509, partition values: [empty row], isDataPresent: false\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:47,312 INFO datasources.FileScanRDD: TID: 3 - Reading current file: path: file:///opt/ml/processing/event_all/20210302_052556_00048_8ak59_2eba7bef-12a7-49fb-b4b3-9273b909db86, range: 0-5132605, partition values: [empty row], isDataPresent: false\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:47,312 INFO datasources.FileScanRDD: TID: 2 - Reading current file: path: file:///opt/ml/processing/event_all/20210302_052556_00048_8ak59_551c3e93-9050-4fcc-8e77-4b68eb402d03, range: 0-5194217, partition values: [empty row], isDataPresent: false\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:47,312 INFO datasources.FileScanRDD: TID: 12 - Reading current file: path: file:///opt/ml/processing/event_all/20210302_052556_00048_8ak59_49ce8e28-c157-4047-9f03-6d072440417b, range: 0-3696664, partition values: [empty row], isDataPresent: false\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:47,312 INFO datasources.SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.DirectFileOutputCommitter\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:47,312 INFO datasources.FileScanRDD: TID: 5 - Reading current file: path: file:///opt/ml/processing/event_all/20210302_052556_00048_8ak59_732faef9-ee6c-4228-a60f-f8175e9ef311, range: 0-4848446, partition values: [empty row], isDataPresent: false\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:47,312 INFO datasources.FileScanRDD: TID: 11 - Reading current file: path: file:///opt/ml/processing/event_all/20210302_052556_00048_8ak59_1ba11873-e510-441f-9ca5-0617e2fcbc83, range: 0-3738191, partition values: [empty row], isDataPresent: false\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:47,312 INFO datasources.SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.DirectFileOutputCommitter\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:47,312 INFO datasources.FileScanRDD: TID: 8 - Reading current file: path: file:///opt/ml/processing/event_all/20210302_052556_00048_8ak59_582a9961-777e-4a98-9299-789e0c88437a, range: 0-4165139, partition values: [empty row], isDataPresent: false\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:47,312 INFO datasources.FileScanRDD: TID: 15 - Reading current file: path: file:///opt/ml/processing/event_all/20210302_052556_00048_8ak59_7d7b1c0c-eae7-411f-a1b7-485b78519a06, range: 0-3580885, partition values: [empty row], isDataPresent: false\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:47,312 INFO datasources.FileScanRDD: TID: 6 - Reading current file: path: file:///opt/ml/processing/event_all/20210302_052556_00048_8ak59_d33cd433-1bd3-4bc4-a8fb-55583a5ddd9d, range: 0-4741819, partition values: [empty row], isDataPresent: false\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:47,311 INFO output.DirectFileOutputCommitter: Direct Write: DISABLED\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:47,313 INFO datasources.FileScanRDD: TID: 13 - Reading current file: path: file:///opt/ml/processing/event_all/20210302_052556_00048_8ak59_6d6a0f88-bece-4686-aea7-d664cf935bff, range: 0-3694389, partition values: [empty row], isDataPresent: false\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:47,306 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:47,306 INFO output.DirectFileOutputCommitter: Direct Write: DISABLED\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:47,308 INFO datasources.SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.DirectFileOutputCommitter\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:47,308 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:47,308 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:47,308 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:47,308 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:47,308 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:47,308 INFO output.DirectFileOutputCommitter: Direct Write: DISABLED\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:47,309 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:47,309 INFO datasources.SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.DirectFileOutputCommitter\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:47,309 INFO output.DirectFileOutputCommitter: Direct Write: DISABLED\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:47,309 INFO output.DirectFileOutputCommitter: Direct Write: DISABLED\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:47,309 INFO output.DirectFileOutputCommitter: Direct Write: DISABLED\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:47,309 INFO output.DirectFileOutputCommitter: Direct Write: DISABLED\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:47,310 INFO datasources.SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.DirectFileOutputCommitter\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:47,310 INFO datasources.SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.DirectFileOutputCommitter\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:47,309 INFO output.DirectFileOutputCommitter: Direct Write: DISABLED\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:47,309 INFO output.DirectFileOutputCommitter: Direct Write: DISABLED\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:47,309 INFO output.DirectFileOutputCommitter: Direct Write: DISABLED\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:47,310 INFO datasources.SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.DirectFileOutputCommitter\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:47,310 INFO datasources.SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.DirectFileOutputCommitter\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:47,309 INFO output.DirectFileOutputCommitter: Direct Write: DISABLED\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:47,310 INFO datasources.SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.DirectFileOutputCommitter\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:47,309 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:47,311 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:47,310 INFO datasources.SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.DirectFileOutputCommitter\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:47,310 INFO datasources.SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.DirectFileOutputCommitter\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:47,309 INFO datasources.SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.DirectFileOutputCommitter\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:47,309 INFO output.DirectFileOutputCommitter: Direct Write: DISABLED\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:47,309 INFO output.DirectFileOutputCommitter: Direct Write: DISABLED\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:47,309 INFO output.DirectFileOutputCommitter: Direct Write: DISABLED\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:47,312 INFO datasources.FileScanRDD: TID: 9 - Reading current file: path: file:///opt/ml/processing/event_all/20210302_052556_00048_8ak59_54bbf1c6-7f11-40cd-b1d9-50cc826f5daf, range: 0-4015480, partition values: [empty row], isDataPresent: false\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:47,312 INFO datasources.SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.DirectFileOutputCommitter\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:47,309 INFO output.DirectFileOutputCommitter: Direct Write: DISABLED\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:47,312 INFO datasources.SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.DirectFileOutputCommitter\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:47,312 INFO datasources.FileScanRDD: TID: 4 - Reading current file: path: file:///opt/ml/processing/event_all/20210302_052556_00048_8ak59_1f49b21c-5d8c-4521-8e7a-1151e60f2c9b, range: 0-5077509, partition values: [empty row], isDataPresent: false\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:47,312 INFO datasources.FileScanRDD: TID: 3 - Reading current file: path: file:///opt/ml/processing/event_all/20210302_052556_00048_8ak59_2eba7bef-12a7-49fb-b4b3-9273b909db86, range: 0-5132605, partition values: [empty row], isDataPresent: false\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:47,312 INFO datasources.FileScanRDD: TID: 2 - Reading current file: path: file:///opt/ml/processing/event_all/20210302_052556_00048_8ak59_551c3e93-9050-4fcc-8e77-4b68eb402d03, range: 0-5194217, partition values: [empty row], isDataPresent: false\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:47,312 INFO datasources.FileScanRDD: TID: 12 - Reading current file: path: file:///opt/ml/processing/event_all/20210302_052556_00048_8ak59_49ce8e28-c157-4047-9f03-6d072440417b, range: 0-3696664, partition values: [empty row], isDataPresent: false\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:47,312 INFO datasources.SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.DirectFileOutputCommitter\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:47,312 INFO datasources.FileScanRDD: TID: 5 - Reading current file: path: file:///opt/ml/processing/event_all/20210302_052556_00048_8ak59_732faef9-ee6c-4228-a60f-f8175e9ef311, range: 0-4848446, partition values: [empty row], isDataPresent: false\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:47,312 INFO datasources.FileScanRDD: TID: 11 - Reading current file: path: file:///opt/ml/processing/event_all/20210302_052556_00048_8ak59_1ba11873-e510-441f-9ca5-0617e2fcbc83, range: 0-3738191, partition values: [empty row], isDataPresent: false\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:47,312 INFO datasources.SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.DirectFileOutputCommitter\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:47,312 INFO datasources.FileScanRDD: TID: 8 - Reading current file: path: file:///opt/ml/processing/event_all/20210302_052556_00048_8ak59_582a9961-777e-4a98-9299-789e0c88437a, range: 0-4165139, partition values: [empty row], isDataPresent: false\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:47,312 INFO datasources.FileScanRDD: TID: 15 - Reading current file: path: file:///opt/ml/processing/event_all/20210302_052556_00048_8ak59_7d7b1c0c-eae7-411f-a1b7-485b78519a06, range: 0-3580885, partition values: [empty row], isDataPresent: false\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:47,312 INFO datasources.FileScanRDD: TID: 6 - Reading current file: path: file:///opt/ml/processing/event_all/20210302_052556_00048_8ak59_d33cd433-1bd3-4bc4-a8fb-55583a5ddd9d, range: 0-4741819, partition values: [empty row], isDataPresent: false\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:47,311 INFO output.DirectFileOutputCommitter: Direct Write: DISABLED\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:47,313 INFO datasources.FileScanRDD: TID: 13 - Reading current file: path: file:///opt/ml/processing/event_all/20210302_052556_00048_8ak59_6d6a0f88-bece-4686-aea7-d664cf935bff, range: 0-3694389, partition values: [empty row], isDataPresent: false\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:47,313 INFO datasources.FileScanRDD: TID: 14 - Reading current file: path: file:///opt/ml/processing/event_all/20210302_052556_00048_8ak59_335ec3f1-c451-4f59-8eb7-24fe52fd321a, range: 0-3684659, partition values: [empty row], isDataPresent: false\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:47,312 INFO datasources.FileScanRDD: TID: 7 - Reading current file: path: file:///opt/ml/processing/event_all/20210302_052556_00048_8ak59_78d79025-2884-48c3-b36f-9501cefb81d9, range: 0-4634216, partition values: [empty row], isDataPresent: false\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:47,313 INFO datasources.SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.DirectFileOutputCommitter\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:47,313 INFO datasources.FileScanRDD: TID: 16 - Reading current file: path: file:///opt/ml/processing/event_all/20210302_052556_00048_8ak59_33dbc6af-6944-4399-9081-5861de12a353, range: 0-3468795, partition values: [empty row], isDataPresent: false\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:47,313 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 1 with 1 pieces (estimated total size 4.0 MiB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:47,313 INFO datasources.FileScanRDD: TID: 14 - Reading current file: path: file:///opt/ml/processing/event_all/20210302_052556_00048_8ak59_335ec3f1-c451-4f59-8eb7-24fe52fd321a, range: 0-3684659, partition values: [empty row], isDataPresent: false\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:47,312 INFO datasources.FileScanRDD: TID: 7 - Reading current file: path: file:///opt/ml/processing/event_all/20210302_052556_00048_8ak59_78d79025-2884-48c3-b36f-9501cefb81d9, range: 0-4634216, partition values: [empty row], isDataPresent: false\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:47,313 INFO datasources.SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.DirectFileOutputCommitter\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:47,313 INFO datasources.FileScanRDD: TID: 16 - Reading current file: path: file:///opt/ml/processing/event_all/20210302_052556_00048_8ak59_33dbc6af-6944-4399-9081-5861de12a353, range: 0-3468795, partition values: [empty row], isDataPresent: false\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:47,313 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 1 with 1 pieces (estimated total size 4.0 MiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stdout] 2021-03-02T05:26:40.463+0000: [Full GC (Metadata GC Threshold) [PSYoungGen: 9432K->0K(403456K)] [ParOldGen: 11938K->17408K(631808K)] 21370K->17408K(1035264K), [Metaspace: 33997K->33995K(36864K)], 0.0213875 secs] [Times: user=0.08 sys=0.04, real=0.02 secs] \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stdout] 2021-03-02T05:26:45.351+0000: [GC (Allocation Failure) [PSYoungGen: 361984K->15739K(403456K)] 379392K->33155K(1035264K), 0.0106356 secs] [Times: user=0.04 sys=0.01, real=0.01 secs] \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stdout] 2021-03-02T05:26:45.791+0000: [GC (Metadata GC Threshold) [PSYoungGen: 290740K->16366K(489472K)] 308156K->51480K(1121280K), 0.0154411 secs] [Times: user=0.11 sys=0.02, real=0.01 secs] \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stdout] 2021-03-02T05:26:45.807+0000: [Full GC (Metadata GC Threshold) [PSYoungGen: 16366K->0K(489472K)] [ParOldGen: 35113K->46935K(866816K)] 51480K->46935K(1356288K), [Metaspace: 50634K->48242K(59392K)], 0.1022101 secs] [Times: user=0.79 sys=0.05, real=0.11 secs] \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stdout] 2021-03-02T05:26:47.453+0000: [GC (Allocation Failure) [PSYoungGen: 473088K->28151K(501248K)] 520023K->83498K(1368064K), 0.0230610 secs] [Times: user=0.06 sys=0.03, real=0.02 secs] \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stdout] 2021-03-02T05:26:48.053+0000: [GC (Allocation Failure) [PSYoungGen: 501239K->37347K(629760K)] 556586K->314134K(1496576K), 0.0370680 secs] [Times: user=0.36 sys=0.16, real=0.04 secs] \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stdout] 2021-03-02T05:26:48.226+0000: [GC (Allocation Failure) [PSYoungGen: 629731K->86655K(732160K)] 906518K->363450K(1598976K), 0.0137876 secs] [Times: user=0.12 sys=0.05, real=0.02 secs] \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stdout] 2021-03-02T05:26:48.302+0000: [GC (Allocation Failure) [PSYoungGen: 679039K->93688K(988672K)] 955834K->370492K(1855488K), 0.0147741 secs] [Times: user=0.13 sys=0.05, real=0.01 secs] \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stdout] 2021-03-02T05:26:40.463+0000: [Full GC (Metadata GC Threshold) [PSYoungGen: 9432K->0K(403456K)] [ParOldGen: 11938K->17408K(631808K)] 21370K->17408K(1035264K), [Metaspace: 33997K->33995K(36864K)], 0.0213875 secs] [Times: user=0.08 sys=0.04, real=0.02 secs] \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stdout] 2021-03-02T05:26:45.351+0000: [GC (Allocation Failure) [PSYoungGen: 361984K->15739K(403456K)] 379392K->33155K(1035264K), 0.0106356 secs] [Times: user=0.04 sys=0.01, real=0.01 secs] \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stdout] 2021-03-02T05:26:45.791+0000: [GC (Metadata GC Threshold) [PSYoungGen: 290740K->16366K(489472K)] 308156K->51480K(1121280K), 0.0154411 secs] [Times: user=0.11 sys=0.02, real=0.01 secs] \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stdout] 2021-03-02T05:26:45.807+0000: [Full GC (Metadata GC Threshold) [PSYoungGen: 16366K->0K(489472K)] [ParOldGen: 35113K->46935K(866816K)] 51480K->46935K(1356288K), [Metaspace: 50634K->48242K(59392K)], 0.1022101 secs] [Times: user=0.79 sys=0.05, real=0.11 secs] \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stdout] 2021-03-02T05:26:47.453+0000: [GC (Allocation Failure) [PSYoungGen: 473088K->28151K(501248K)] 520023K->83498K(1368064K), 0.0230610 secs] [Times: user=0.06 sys=0.03, real=0.02 secs] \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stdout] 2021-03-02T05:26:48.053+0000: [GC (Allocation Failure) [PSYoungGen: 501239K->37347K(629760K)] 556586K->314134K(1496576K), 0.0370680 secs] [Times: user=0.36 sys=0.16, real=0.04 secs] \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stdout] 2021-03-02T05:26:48.226+0000: [GC (Allocation Failure) [PSYoungGen: 629731K->86655K(732160K)] 906518K->363450K(1598976K), 0.0137876 secs] [Times: user=0.12 sys=0.05, real=0.02 secs] \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stdout] 2021-03-02T05:26:48.302+0000: [GC (Allocation Failure) [PSYoungGen: 679039K->93688K(988672K)] 955834K->370492K(1855488K), 0.0147741 secs] [Times: user=0.13 sys=0.05, real=0.01 secs] \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stdout] 2021-03-02T05:26:48.405+0000: [GC (Allocation Failure) [PSYoungGen: 93695tput.FileOutputCommitter: File Output Committer Algorithm version is 2\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:47,319 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:47,319 INFO output.DirectFileOutputCommitter: Direct Write: DISABLED\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:47,319 INFO datasources.SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.DirectFileOutputCommitter\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:47,319 INFO datasources.FileScanRDD: TID: 10 - Reading current file: path: file:///opt/ml/processing/event_all/20210302_052556_00048_8ak59_4717b00e-ab00-4aee-b2a9-aa2a7b9ab23b, range: 0-3764971, partition values: [empty row], isDataPresent: false\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:47,320 INFO memory.MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 30.5 KiB, free 28.9 GiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:47,323 INFO broadcast.TorrentBroadcast: Reading broadcast variable 1 took 10 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:47,373 INFO memory.MemoryStore: Block broadcast_1 stored as values in memory (estimated size 565.5 KiB, free 28.9 GiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:47,513 WARN parquet.CorruptStatistics: Ignoring statistics because created_by could not be parsed (see PARQUET-251): parquet-mr\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] org.apache.parquet.VersionParser$VersionParseException: Could not parse created_by: parquet-mr using format: (.*?)\\s+version\\s*(?:([^(]*?)\\s*(?:\\(\\s*build\\s*([^)]*?)\\s*\\))?)?\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] #011at org.apache.parquet.VersionParser.parse(VersionParser.java:112)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] #011at org.apache.parquet.CorruptStatistics.shouldIgnoreStatistics(CorruptStatistics.java:72)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] #011at org.apache.parquet.format.converter.ParquetMetadataConverter.fromParquetStatisticsInternal(ParquetMetadataConverter.java:435)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] #011at org.apache.parquet.format.converter.ParquetMetadataConverter.fromParquetStatistics(ParquetMetadataConverter.java:454)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] #011at org.apache.parquet.format.converter.ParquetMetadataConverter.fromParquetMetadata(ParquetMetadataConverter.java:914)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] #011at org.apache.parquet.format.converter.ParquetMetadataConverter.readParquetMetadata(ParquetMetadataConverter.java:885)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] #011at org.apache.parquet.hadoop.ParquetFileReader.readFooter(ParquetFileReader.java:533)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] #011at org.apache.parquet.hadoop.ParquetFileReader.readFooter(ParquetFileReader.java:506)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] #011at org.apache.parquet.hadoop.ParquetFileReader.readFooter(ParquetFileReader.java:500)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] #011at org.apache.parquet.hadoop.ParquetFileReader.readFooter(ParquetFileReader.java:449)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] #011at org.apache.spark.sql.execution.datasources.parquet.SpecificParquetRecordReaderBase.initialize(SpecificParquetRecordReaderBase.java:105)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] #011at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.initialize(VectorizedParquetRecordReader.java:144)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] #011at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat.$anonfun$buildReaderWithPartitionValues$2(ParquetFileFormat.scala:347)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] #011at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:148)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] #011at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:203)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] #011at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:122)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] #011at org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:559)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] #011at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] #011at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] #011at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stdout] 2021-03-02T05:26:48.405+0000: [GC (Allocation Failure) [PSYoungGen: 93695tput.FileOutputCommitter: File Output Committer Algorithm version is 2\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:47,319 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:47,319 INFO output.DirectFileOutputCommitter: Direct Write: DISABLED\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:47,319 INFO datasources.SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.DirectFileOutputCommitter\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:47,319 INFO datasources.FileScanRDD: TID: 10 - Reading current file: path: file:///opt/ml/processing/event_all/20210302_052556_00048_8ak59_4717b00e-ab00-4aee-b2a9-aa2a7b9ab23b, range: 0-3764971, partition values: [empty row], isDataPresent: false\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:47,320 INFO memory.MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 30.5 KiB, free 28.9 GiB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:47,323 INFO broadcast.TorrentBroadcast: Reading broadcast variable 1 took 10 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:47,373 INFO memory.MemoryStore: Block broadcast_1 stored as values in memory (estimated size 565.5 KiB, free 28.9 GiB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:47,513 WARN parquet.CorruptStatistics: Ignoring statistics because created_by could not be parsed (see PARQUET-251): parquet-mr\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] org.apache.parquet.VersionParser$VersionParseException: Could not parse created_by: parquet-mr using format: (.*?)\\s+version\\s*(?:([^(]*?)\\s*(?:\\(\\s*build\\s*([^)]*?)\\s*\\))?)?\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] #011at org.apache.parquet.VersionParser.parse(VersionParser.java:112)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] #011at org.apache.parquet.CorruptStatistics.shouldIgnoreStatistics(CorruptStatistics.java:72)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] #011at org.apache.parquet.format.converter.ParquetMetadataConverter.fromParquetStatisticsInternal(ParquetMetadataConverter.java:435)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] #011at org.apache.parquet.format.converter.ParquetMetadataConverter.fromParquetStatistics(ParquetMetadataConverter.java:454)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] #011at org.apache.parquet.format.converter.ParquetMetadataConverter.fromParquetMetadata(ParquetMetadataConverter.java:914)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] #011at org.apache.parquet.format.converter.ParquetMetadataConverter.readParquetMetadata(ParquetMetadataConverter.java:885)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] #011at org.apache.parquet.hadoop.ParquetFileReader.readFooter(ParquetFileReader.java:533)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] #011at org.apache.parquet.hadoop.ParquetFileReader.readFooter(ParquetFileReader.java:506)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] #011at org.apache.parquet.hadoop.ParquetFileReader.readFooter(ParquetFileReader.java:500)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] #011at org.apache.parquet.hadoop.ParquetFileReader.readFooter(ParquetFileReader.java:449)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] #011at org.apache.spark.sql.execution.datasources.parquet.SpecificParquetRecordReaderBase.initialize(SpecificParquetRecordReaderBase.java:105)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] #011at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.initialize(VectorizedParquetRecordReader.java:144)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] #011at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat.$anonfun$buildReaderWithPartitionValues$2(ParquetFileFormat.scala:347)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] #011at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:148)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] #011at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:203)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] #011at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:122)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] #011at org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:559)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] #011at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] #011at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] #011at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] #011at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:729)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] #011at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:260)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] #011at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:205)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] #011at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] #011at org.apache.spark.scheduler.Task.run(Task.scala:127)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] #011at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:444)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] #011at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] #011at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:447)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] #011at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:729)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] #011at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:260)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] #011at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:205)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] #011at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] #011at org.apache.spark.scheduler.Task.run(Task.scala:127)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] #011at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:444)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] #011at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] #011at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:447)\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:53,960 INFO scheduler.TaskSetManager: Starting task 16.0 in stage 1.0 (TID 17, algo-1, executor 1, partition 16, PROCESS_LOCAL, 7808 bytes)\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:53,965 INFO scheduler.TaskSetManager: Finished task 15.0 in stage 1.0 (TID 16) in 8640 ms on algo-1 (executor 1) (1/22)\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:53,980 INFO scheduler.TaskSetManager: Starting task 17.0 in stage 1.0 (TID 18, algo-1, executor 1, partition 17, PROCESS_LOCAL, 7808 bytes)\u001b[0m\n",
      "\u001b[34m2021-03-02 05:26:53,980 INFO scheduler.TaskSetManager: Finished task 14.0 in stage 1.0 (TID 15) in 8655 ms on algo-1 (executor 1) (2/22)\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:53,960 INFO scheduler.TaskSetManager: Starting task 16.0 in stage 1.0 (TID 17, algo-1, executor 1, partition 16, PROCESS_LOCAL, 7808 bytes)\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:53,965 INFO scheduler.TaskSetManager: Finished task 15.0 in stage 1.0 (TID 16) in 8640 ms on algo-1 (executor 1) (1/22)\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:53,980 INFO scheduler.TaskSetManager: Starting task 17.0 in stage 1.0 (TID 18, algo-1, executor 1, partition 17, PROCESS_LOCAL, 7808 bytes)\u001b[0m\n",
      "\u001b[35m2021-03-02 05:26:53,980 INFO scheduler.TaskSetManager: Finished task 14.0 in stage 1.0 (TID 15) in 8655 ms on algo-1 (executor 1) (2/22)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] #011at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] #011at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] #011at java.lang.Thread.run(Thread.java:748)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:47,558 INFO datasources.FileScanRDD: TID: 1 - Reading current file: path: file:///opt/ml/processing/event_all/20210302_052556_00048_8ak59_bb46c336-559d-4fd2-b783-3445bb4eed19, range: 0-5212189, partition values: [empty row], isDataPresent: false\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:47,622 INFO zlib.ZlibFactory: Successfully loaded & initialized native-zlib library\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:47,624 INFO compress.CodecPool: Got brand-new decompressor [.gz]\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:47,624 INFO compress.CodecPool: Got brand-new decompressor [.gz]\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:47,624 INFO compress.CodecPool: Got brand-new decompressor [.gz]\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:47,624 INFO compress.CodecPool: Got brand-new decompressor [.gz]\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:47,624 INFO compress.CodecPool: Got brand-new decompressor [.gz]\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:47,624 INFO compress.CodecPool: Got brand-new decompressor [.gz]\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:47,624 INFO compress.CodecPool: Got brand-new decompressor [.gz]\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:47,624 INFO compress.CodecPool: Got brand-new decompressor [.gz]\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:47,624 INFO compress.CodecPool: Got brand-new decompressor [.gz]\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:47,624 INFO compress.CodecPool: Got brand-new decompressor [.gz]\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:47,624 INFO compress.CodecPool: Got brand-new decompressor [.gz]\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:47,624 INFO compress.CodecPool: Got brand-new decompressor [.gz]\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:47,624 INFO compress.CodecPool: Got brand-new decompressor [.gz]\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:47,624 INFO compress.CodecPool: Got brand-new decompressor [.gz]\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:47,624 INFO compress.CodecPool: Got brand-new decompressor [.gz]\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:47,624 INFO compress.CodecPool: Got brand-new decompressor [.gz]\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:53,948 INFO output.FileOutputCommitter: Saved output of task 'attempt_20210302052645_0001_m_000015_16' to s3a://sagemaker-ap-northeast-2-029498593638/export-flow-02-05-22-00-ca04653e/output/data-wrangler-flow-processing-02-05-22-00-ca04653e/410bb997-de0d-4339-8039-53e1d78e46f7/default\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:53,949 INFO mapred.SparkHadoopMapRedUtil: attempt_20210302052645_0001_m_000015_16: Committed\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:53,957 INFO executor.Executor: Finished task 15.0 in stage 1.0 (TID 16). 2570 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:53,962 INFO executor.YarnCoarseGrainedExecutorBackend: Got assigned task 17\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:53,962 INFO executor.Executor: Running task 16.0 in stage 1.0 (TID 17)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:53,975 INFO output.FileOutputCommitter: Saved output of task 'attempt_20210302052645_0001_m_000014_15' to s3a://sagemaker-ap-northeast-2-029498593638/export-flow-02-05-22-00-ca04653e/output/data-wrangler-flow-processing-02-05-22-00-ca04653e/410bb997-de0d-4339-8039-53e1d78e46f7/default\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:53,975 INFO mapred.SparkHadoopMapRedUtil: attempt_20210302052645_0001_m_000014_15: Committed\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:53,976 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] #011at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] #011at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] #011at java.lang.Thread.run(Thread.java:748)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:47,558 INFO datasources.FileScanRDD: TID: 1 - Reading current file: path: file:///opt/ml/processing/event_all/20210302_052556_00048_8ak59_bb46c336-559d-4fd2-b783-3445bb4eed19, range: 0-5212189, partition values: [empty row], isDataPresent: false\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:47,622 INFO zlib.ZlibFactory: Successfully loaded & initialized native-zlib library\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:47,624 INFO compress.CodecPool: Got brand-new decompressor [.gz]\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:47,624 INFO compress.CodecPool: Got brand-new decompressor [.gz]\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:47,624 INFO compress.CodecPool: Got brand-new decompressor [.gz]\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:47,624 INFO compress.CodecPool: Got brand-new decompressor [.gz]\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:47,624 INFO compress.CodecPool: Got brand-new decompressor [.gz]\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:47,624 INFO compress.CodecPool: Got brand-new decompressor [.gz]\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:47,624 INFO compress.CodecPool: Got brand-new decompressor [.gz]\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:47,624 INFO compress.CodecPool: Got brand-new decompressor [.gz]\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:47,624 INFO compress.CodecPool: Got brand-new decompressor [.gz]\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:47,624 INFO compress.CodecPool: Got brand-new decompressor [.gz]\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:47,624 INFO compress.CodecPool: Got brand-new decompressor [.gz]\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:47,624 INFO compress.CodecPool: Got brand-new decompressor [.gz]\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:47,624 INFO compress.CodecPool: Got brand-new decompressor [.gz]\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:47,624 INFO compress.CodecPool: Got brand-new decompressor [.gz]\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:47,624 INFO compress.CodecPool: Got brand-new decompressor [.gz]\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:47,624 INFO compress.CodecPool: Got brand-new decompressor [.gz]\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:53,948 INFO output.FileOutputCommitter: Saved output of task 'attempt_20210302052645_0001_m_000015_16' to s3a://sagemaker-ap-northeast-2-029498593638/export-flow-02-05-22-00-ca04653e/output/data-wrangler-flow-processing-02-05-22-00-ca04653e/410bb997-de0d-4339-8039-53e1d78e46f7/default\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:53,949 INFO mapred.SparkHadoopMapRedUtil: attempt_20210302052645_0001_m_000015_16: Committed\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:53,957 INFO executor.Executor: Finished task 15.0 in stage 1.0 (TID 16). 2570 bytes result sent to driver\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:53,962 INFO executor.YarnCoarseGrainedExecutorBackend: Got assigned task 17\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:53,962 INFO executor.Executor: Running task 16.0 in stage 1.0 (TID 17)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:53,975 INFO output.FileOutputCommitter: Saved output of task 'attempt_20210302052645_0001_m_000014_15' to s3a://sagemaker-ap-northeast-2-029498593638/export-flow-02-05-22-00-ca04653e/output/data-wrangler-flow-processing-02-05-22-00-ca04653e/410bb997-de0d-4339-8039-53e1d78e46f7/default\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:53,975 INFO mapred.SparkHadoopMapRedUtil: attempt_20210302052645_0001_m_000014_15: Committed\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:53,976 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:53,976 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:53,976 INFO output.DirectFileOutputCommitter: Direct Write: DISABLED\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:53,976 INFO datasources.SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.DirectFileOutputCommitter\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:53,977 INFO datasources.FileScanRDD: TID: 17 - Reading current file: path: file:///opt/ml/processing/event_all/20210302_052556_00048_8ak59_1b89a3cd-dd21-4ff3-8fa3-e60404e4faa3, range: 0-3408058, partition values: [empty row], isDataPresent: false\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:53,977 INFO executor.Executor: Finished task 14.0 in stage 1.0 (TID 15). 2527 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:53,982 INFO executor.YarnCoarseGrainedExecutorBackend: Got assigned task 18\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:53,982 INFO executor.Executor: Running task 17.0 in stage 1.0 (TID 18)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:53,992 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:53,976 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:53,976 INFO output.DirectFileOutputCommitter: Direct Write: DISABLED\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:53,976 INFO datasources.SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.DirectFileOutputCommitter\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:53,977 INFO datasources.FileScanRDD: TID: 17 - Reading current file: path: file:///opt/ml/processing/event_all/20210302_052556_00048_8ak59_1b89a3cd-dd21-4ff3-8fa3-e60404e4faa3, range: 0-3408058, partition values: [empty row], isDataPresent: false\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:53,977 INFO executor.Executor: Finished task 14.0 in stage 1.0 (TID 15). 2527 bytes result sent to driver\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:53,982 INFO executor.YarnCoarseGrainedExecutorBackend: Got assigned task 18\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:53,982 INFO executor.Executor: Running task 17.0 in stage 1.0 (TID 18)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:53,992 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\u001b[0m\n",
      "\u001b[34m2021-03-02 05:27:00,792 INFO scheduler.TaskSetManager: Starting task 18.0 in stage 1.0 (TID 19, algo-1, executor 1, partition 18, PROCESS_LOCAL, 7808 bytes)\u001b[0m\n",
      "\u001b[34m2021-03-02 05:27:00,795 INFO scheduler.TaskSetManager: Finished task 11.0 in stage 1.0 (TID 12) in 15471 ms on algo-1 (executor 1) (3/22)\u001b[0m\n",
      "\u001b[35m2021-03-02 05:27:00,792 INFO scheduler.TaskSetManager: Starting task 18.0 in stage 1.0 (TID 19, algo-1, executor 1, partition 18, PROCESS_LOCAL, 7808 bytes)\u001b[0m\n",
      "\u001b[35m2021-03-02 05:27:00,795 INFO scheduler.TaskSetManager: Finished task 11.0 in stage 1.0 (TID 12) in 15471 ms on algo-1 (executor 1) (3/22)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:53,992 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:53,992 INFO output.DirectFileOutputCommitter: Direct Write: DISABLED\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:53,992 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:53,992 INFO output.DirectFileOutputCommitter: Direct Write: DISABLED\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:53,992 INFO datasources.SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.DirectFileOutputCommitter\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:53,993 INFO datasources.FileScanRDD: TID: 18 - Reading current file: path: file:///opt/ml/processing/event_all/20210302_052556_00048_8ak59_3166c0b1-7ef2-42a6-bf23-1d61959c64da, range: 0-3402464, partition values: [empty row], isDataPresent: false\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:54,180 INFO output.FileOutputCommitter: Saved output of task 'attempt_20210302052645_0001_m_000011_12' to s3a://sagemaker-ap-northeast-2-029498593638/export-flow-02-05-22-00-ca04653e/output/data-wrangler-flow-processing-02-05-22-00-ca04653e/410bb997-de0d-4339-8039-53e1d78e46f7/default\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:55,403 INFO output.FileOutputCommitter: Saved output of task 'attempt_20210302052645_0001_m_000009_10' to s3a://sagemaker-ap-northeast-2-029498593638/export-flow-02-05-22-00-ca04653e/output/data-wrangler-flow-processing-02-05-22-00-ca04653e/410bb997-de0d-4339-8039-53e1d78e46f7/default\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:27:00,589 INFO mapred.SparkHadoopMapRedUtil: attempt_20210302052645_0001_m_000011_12: Committed\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:54,974 INFO output.FileOutputCommitter: Saved output of task 'attempt_20210302052645_0001_m_000010_11' to s3a://sagemaker-ap-northeast-2-029498593638/export-flow-02-05-22-00-ca04653e/output/data-wrangler-flow-processing-02-05-22-00-ca04653e/410bb997-de0d-4339-8039-53e1d78e46f7/default\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:54,265 INFO output.FileOutputCommitter: Saved output of task 'attempt_20210302052645_0001_m_000013_14' to s3a://sagemaker-ap-northeast-2-029498593638/export-flow-02-05-22-00-ca04653e/output/data-wrangler-flow-processing-02-05-22-00-ca04653e/410bb997-de0d-4339-8039-53e1d78e46f7/default\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:54,203 INFO output.FileOutputCommitter: Saved output of task 'attempt_20210302052645_0001_m_000012_13' to s3a://sagemaker-ap-northeast-2-029498593638/export-flow-02-05-22-00-ca04653e/output/data-wrangler-flow-processing-02-05-22-00-ca04653e/410bb997-de0d-4339-8039-53e1d78e46f7/default\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:27:00,778 INFO mapred.SparkHadoopMapRedUtil: attempt_20210302052645_0001_m_000013_14: Committed\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:27:00,779 INFO mapred.SparkHadoopMapRedUtil: attempt_20210302052645_0001_m_000012_13: Committed\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:27:00,778 INFO mapred.SparkHadoopMapRedUtil: attempt_20210302052645_0001_m_000010_11: Committed\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:27:00,595 INFO executor.Executor: Finished task 11.0 in stage 1.0 (TID 12). 2527 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m2021-03-02 05:27:03,408 INFO scheduler.TaskSetManager: Starting task 19.0 in stage 1.0 (TID 20, algo-1, executor 1, partition 19, PROCESS_LOCAL, 7808 bytes)\u001b[0m\n",
      "\u001b[34m2021-03-02 05:27:03,409 INFO scheduler.TaskSetManager: Finished task 10.0 in stage 1.0 (TID 11) in 18085 ms on algo-1 (executor 1) (4/22)\u001b[0m\n",
      "\u001b[34m2021-03-02 05:27:03,413 INFO scheduler.TaskSetManager: Starting task 20.0 in stage 1.0 (TID 21, algo-1, executor 1, partition 20, PROCESS_LOCAL, 7808 bytes)\u001b[0m\n",
      "\u001b[34m2021-03-02 05:27:03,416 INFO scheduler.TaskSetManager: Starting task 21.0 in stage 1.0 (TID 22, algo-1, executor 1, partition 21, PROCESS_LOCAL, 7808 bytes)\u001b[0m\n",
      "\u001b[34m2021-03-02 05:27:03,417 INFO scheduler.TaskSetManager: Finished task 13.0 in stage 1.0 (TID 14) in 18092 ms on algo-1 (executor 1) (5/22)\u001b[0m\n",
      "\u001b[34m2021-03-02 05:27:03,417 INFO scheduler.TaskSetManager: Finished task 12.0 in stage 1.0 (TID 13) in 18093 ms on algo-1 (executor 1) (6/22)\u001b[0m\n",
      "\u001b[34m2021-03-02 05:27:03,430 INFO scheduler.TaskSetManager: Finished task 8.0 in stage 1.0 (TID 9) in 18107 ms on algo-1 (executor 1) (7/22)\u001b[0m\n",
      "\u001b[34m2021-03-02 05:27:03,432 INFO scheduler.TaskSetManager: Finished task 9.0 in stage 1.0 (TID 10) in 18108 ms on algo-1 (executor 1) (8/22)\u001b[0m\n",
      "\u001b[34m2021-03-02 05:27:03,473 INFO scheduler.TaskSetManager: Finished task 7.0 in stage 1.0 (TID 8) in 18150 ms on algo-1 (executor 1) (9/22)\u001b[0m\n",
      "\u001b[34m2021-03-02 05:27:03,473 INFO scheduler.TaskSetManager: Finished task 6.0 in stage 1.0 (TID 7) in 18150 ms on algo-1 (executor 1) (10/22)\u001b[0m\n",
      "\u001b[34m2021-03-02 05:27:03,474 INFO scheduler.TaskSetManager: Finished task 4.0 in stage 1.0 (TID 5) in 18152 ms on algo-1 (executor 1) (11/22)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:53,992 INFO datasources.SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.DirectFileOutputCommitter\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:53,993 INFO datasources.FileScanRDD: TID: 18 - Reading current file: path: file:///opt/ml/processing/event_all/20210302_052556_00048_8ak59_3166c0b1-7ef2-42a6-bf23-1d61959c64da, range: 0-3402464, partition values: [empty row], isDataPresent: false\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:54,180 INFO output.FileOutputCommitter: Saved output of task 'attempt_20210302052645_0001_m_000011_12' to s3a://sagemaker-ap-northeast-2-029498593638/export-flow-02-05-22-00-ca04653e/output/data-wrangler-flow-processing-02-05-22-00-ca04653e/410bb997-de0d-4339-8039-53e1d78e46f7/default\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:55,403 INFO output.FileOutputCommitter: Saved output of task 'attempt_20210302052645_0001_m_000009_10' to s3a://sagemaker-ap-northeast-2-029498593638/export-flow-02-05-22-00-ca04653e/output/data-wrangler-flow-processing-02-05-22-00-ca04653e/410bb997-de0d-4339-8039-53e1d78e46f7/default\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:27:00,589 INFO mapred.SparkHadoopMapRedUtil: attempt_20210302052645_0001_m_000011_12: Committed\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:54,974 INFO output.FileOutputCommitter: Saved output of task 'attempt_20210302052645_0001_m_000010_11' to s3a://sagemaker-ap-northeast-2-029498593638/export-flow-02-05-22-00-ca04653e/output/data-wrangler-flow-processing-02-05-22-00-ca04653e/410bb997-de0d-4339-8039-53e1d78e46f7/default\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:54,265 INFO output.FileOutputCommitter: Saved output of task 'attempt_20210302052645_0001_m_000013_14' to s3a://sagemaker-ap-northeast-2-029498593638/export-flow-02-05-22-00-ca04653e/output/data-wrangler-flow-processing-02-05-22-00-ca04653e/410bb997-de0d-4339-8039-53e1d78e46f7/default\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:26:54,203 INFO output.FileOutputCommitter: Saved output of task 'attempt_20210302052645_0001_m_000012_13' to s3a://sagemaker-ap-northeast-2-029498593638/export-flow-02-05-22-00-ca04653e/output/data-wrangler-flow-processing-02-05-22-00-ca04653e/410bb997-de0d-4339-8039-53e1d78e46f7/default\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:27:00,778 INFO mapred.SparkHadoopMapRedUtil: attempt_20210302052645_0001_m_000013_14: Committed\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:27:00,779 INFO mapred.SparkHadoopMapRedUtil: attempt_20210302052645_0001_m_000012_13: Committed\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:27:00,778 INFO mapred.SparkHadoopMapRedUtil: attempt_20210302052645_0001_m_000010_11: Committed\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:27:00,595 INFO executor.Executor: Finished task 11.0 in stage 1.0 (TID 12). 2527 bytes result sent to driver\u001b[0m\n",
      "\u001b[35m2021-03-02 05:27:03,408 INFO scheduler.TaskSetManager: Starting task 19.0 in stage 1.0 (TID 20, algo-1, executor 1, partition 19, PROCESS_LOCAL, 7808 bytes)\u001b[0m\n",
      "\u001b[35m2021-03-02 05:27:03,409 INFO scheduler.TaskSetManager: Finished task 10.0 in stage 1.0 (TID 11) in 18085 ms on algo-1 (executor 1) (4/22)\u001b[0m\n",
      "\u001b[35m2021-03-02 05:27:03,413 INFO scheduler.TaskSetManager: Starting task 20.0 in stage 1.0 (TID 21, algo-1, executor 1, partition 20, PROCESS_LOCAL, 7808 bytes)\u001b[0m\n",
      "\u001b[35m2021-03-02 05:27:03,416 INFO scheduler.TaskSetManager: Starting task 21.0 in stage 1.0 (TID 22, algo-1, executor 1, partition 21, PROCESS_LOCAL, 7808 bytes)\u001b[0m\n",
      "\u001b[35m2021-03-02 05:27:03,417 INFO scheduler.TaskSetManager: Finished task 13.0 in stage 1.0 (TID 14) in 18092 ms on algo-1 (executor 1) (5/22)\u001b[0m\n",
      "\u001b[35m2021-03-02 05:27:03,417 INFO scheduler.TaskSetManager: Finished task 12.0 in stage 1.0 (TID 13) in 18093 ms on algo-1 (executor 1) (6/22)\u001b[0m\n",
      "\u001b[35m2021-03-02 05:27:03,430 INFO scheduler.TaskSetManager: Finished task 8.0 in stage 1.0 (TID 9) in 18107 ms on algo-1 (executor 1) (7/22)\u001b[0m\n",
      "\u001b[35m2021-03-02 05:27:03,432 INFO scheduler.TaskSetManager: Finished task 9.0 in stage 1.0 (TID 10) in 18108 ms on algo-1 (executor 1) (8/22)\u001b[0m\n",
      "\u001b[35m2021-03-02 05:27:03,473 INFO scheduler.TaskSetManager: Finished task 7.0 in stage 1.0 (TID 8) in 18150 ms on algo-1 (executor 1) (9/22)\u001b[0m\n",
      "\u001b[35m2021-03-02 05:27:03,473 INFO scheduler.TaskSetManager: Finished task 6.0 in stage 1.0 (TID 7) in 18150 ms on algo-1 (executor 1) (10/22)\u001b[0m\n",
      "\u001b[35m2021-03-02 05:27:03,474 INFO scheduler.TaskSetManager: Finished task 4.0 in stage 1.0 (TID 5) in 18152 ms on algo-1 (executor 1) (11/22)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:27:00,590 INFO mapred.SparkHadoopMapRedUtil: attempt_20210302052645_0001_m_000009_10: Committed\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:27:00,818 INFO executor.Executor: Finished task 10.0 in stage 1.0 (TID 11). 2527 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:27:02,710 INFO output.FileOutputCommitter: Saved output of task 'attempt_20210302052645_0001_m_000006_7' to s3a://sagemaker-ap-northeast-2-029498593638/export-flow-02-05-22-00-ca04653e/output/data-wrangler-flow-processing-02-05-22-00-ca04653e/410bb997-de0d-4339-8039-53e1d78e46f7/default\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:27:02,684 INFO output.FileOutputCommitter: Saved output of task 'attempt_20210302052645_0001_m_000004_5' to s3a://sagemaker-ap-northeast-2-029498593638/export-flow-02-05-22-00-ca04653e/output/data-wrangler-flow-processing-02-05-22-00-ca04653e/410bb997-de0d-4339-8039-53e1d78e46f7/default\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:27:03,402 INFO mapred.SparkHadoopMapRedUtil: attempt_20210302052645_0001_m_000004_5: Committed\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:27:02,551 INFO output.FileOutputCommitter: Saved output of task 'attempt_20210302052645_0001_m_000007_8' to s3a://sagemaker-ap-northeast-2-029498593638/export-flow-02-05-22-00-ca04653e/output/data-wrangler-flow-processing-02-05-22-00-ca04653e/410bb997-de0d-4339-8039-53e1d78e46f7/default\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:27:02,513 INFO output.FileOutputCommitter: Saved output of task 'attempt_20210302052645_0001_m_000008_9' to s3a://sagemaker-ap-northeast-2-029498593638/export-flow-02-05-22-00-ca04653e/output/data-wrangler-flow-processing-02-05-22-00-ca04653e/410bb997-de0d-4339-8039-53e1d78e46f7/default\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:27:00,821 INFO executor.Executor: Finished task 9.0 in stage 1.0 (TID 10). 2527 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:27:00,821 INFO executor.Executor: Finished task 13.0 in stage 1.0 (TID 14). 2570 bytes result sent to driver\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:27:00,590 INFO mapred.SparkHadoopMapRedUtil: attempt_20210302052645_0001_m_000009_10: Committed\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:27:00,818 INFO executor.Executor: Finished task 10.0 in stage 1.0 (TID 11). 2527 bytes result sent to driver\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:27:02,710 INFO output.FileOutputCommitter: Saved output of task 'attempt_20210302052645_0001_m_000006_7' to s3a://sagemaker-ap-northeast-2-029498593638/export-flow-02-05-22-00-ca04653e/output/data-wrangler-flow-processing-02-05-22-00-ca04653e/410bb997-de0d-4339-8039-53e1d78e46f7/default\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:27:02,684 INFO output.FileOutputCommitter: Saved output of task 'attempt_20210302052645_0001_m_000004_5' to s3a://sagemaker-ap-northeast-2-029498593638/export-flow-02-05-22-00-ca04653e/output/data-wrangler-flow-processing-02-05-22-00-ca04653e/410bb997-de0d-4339-8039-53e1d78e46f7/default\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:27:03,402 INFO mapred.SparkHadoopMapRedUtil: attempt_20210302052645_0001_m_000004_5: Committed\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:27:02,551 INFO output.FileOutputCommitter: Saved output of task 'attempt_20210302052645_0001_m_000007_8' to s3a://sagemaker-ap-northeast-2-029498593638/export-flow-02-05-22-00-ca04653e/output/data-wrangler-flow-processing-02-05-22-00-ca04653e/410bb997-de0d-4339-8039-53e1d78e46f7/default\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:27:02,513 INFO output.FileOutputCommitter: Saved output of task 'attempt_20210302052645_0001_m_000008_9' to s3a://sagemaker-ap-northeast-2-029498593638/export-flow-02-05-22-00-ca04653e/output/data-wrangler-flow-processing-02-05-22-00-ca04653e/410bb997-de0d-4339-8039-53e1d78e46f7/default\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:27:00,821 INFO executor.Executor: Finished task 9.0 in stage 1.0 (TID 10). 2527 bytes result sent to driver\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:27:00,821 INFO executor.Executor: Finished task 13.0 in stage 1.0 (TID 14). 2570 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:27:00,820 INFO executor.YarnCoarseGrainedExecutorBackend: Got assigned task 19\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:27:00,818 INFO executor.Executor: Finished task 12.0 in stage 1.0 (TID 13). 2527 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:27:03,408 INFO mapred.SparkHadoopMapRedUtil: attempt_20210302052645_0001_m_000008_9: Committed\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:27:03,410 INFO executor.Executor: Running task 18.0 in stage 1.0 (TID 19)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:27:03,411 INFO executor.YarnCoarseGrainedExecutorBackend: Got assigned task 20\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:27:03,408 INFO mapred.SparkHadoopMapRedUtil: attempt_20210302052645_0001_m_000007_8: Committed\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:27:03,402 INFO mapred.SparkHadoopMapRedUtil: attempt_20210302052645_0001_m_000006_7: Committed\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:27:03,427 INFO executor.Executor: Finished task 8.0 in stage 1.0 (TID 9). 2527 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:27:03,428 INFO executor.YarnCoarseGrainedExecutorBackend: Got assigned task 21\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:27:03,428 INFO executor.Executor: Running task 20.0 in stage 1.0 (TID 21)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:27:03,428 INFO executor.YarnCoarseGrainedExecutorBackend: Got assigned task 22\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:27:03,428 INFO executor.Executor: Running task 19.0 in stage 1.0 (TID 20)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:27:03,430 INFO executor.Executor: Finished task 7.0 in stage 1.0 (TID 8). 2527 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:27:03,433 INFO executor.Executor: Running task 21.0 in stage 1.0 (TID 22)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:27:03,467 INFO executor.Executor: Finished task 6.0 in stage 1.0 (TID 7). 2527 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:27:03,469 INFO executor.Executor: Finished task 4.0 in stage 1.0 (TID 5). 2527 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:27:03,472 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:27:03,472 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:27:03,472 INFO output.DirectFileOutputCommitter: Direct Write: DISABLED\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:27:03,472 INFO datasources.SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.DirectFileOutputCommitter\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:27:03,473 INFO datasources.FileScanRDD: TID: 20 - Reading current file: path: file:///opt/ml/processing/event_all/20210302_052556_00048_8ak59_66a8e543-5f92-449b-9a57-f4fdf234d99f, range: 0-3274255, partition values: [empty row], isDataPresent: false\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:27:03,480 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:27:03,480 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:27:03,480 INFO output.DirectFileOutputCommitter: Direct Write: DISABLED\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:27:03,480 INFO datasources.SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.DirectFileOutputCommitter\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:27:00,820 INFO executor.YarnCoarseGrainedExecutorBackend: Got assigned task 19\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:27:00,818 INFO executor.Executor: Finished task 12.0 in stage 1.0 (TID 13). 2527 bytes result sent to driver\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:27:03,408 INFO mapred.SparkHadoopMapRedUtil: attempt_20210302052645_0001_m_000008_9: Committed\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:27:03,410 INFO executor.Executor: Running task 18.0 in stage 1.0 (TID 19)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:27:03,411 INFO executor.YarnCoarseGrainedExecutorBackend: Got assigned task 20\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:27:03,408 INFO mapred.SparkHadoopMapRedUtil: attempt_20210302052645_0001_m_000007_8: Committed\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:27:03,402 INFO mapred.SparkHadoopMapRedUtil: attempt_20210302052645_0001_m_000006_7: Committed\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:27:03,427 INFO executor.Executor: Finished task 8.0 in stage 1.0 (TID 9). 2527 bytes result sent to driver\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:27:03,428 INFO executor.YarnCoarseGrainedExecutorBackend: Got assigned task 21\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:27:03,428 INFO executor.Executor: Running task 20.0 in stage 1.0 (TID 21)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:27:03,428 INFO executor.YarnCoarseGrainedExecutorBackend: Got assigned task 22\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:27:03,428 INFO executor.Executor: Running task 19.0 in stage 1.0 (TID 20)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:27:03,430 INFO executor.Executor: Finished task 7.0 in stage 1.0 (TID 8). 2527 bytes result sent to driver\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:27:03,433 INFO executor.Executor: Running task 21.0 in stage 1.0 (TID 22)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:27:03,467 INFO executor.Executor: Finished task 6.0 in stage 1.0 (TID 7). 2527 bytes result sent to driver\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:27:03,469 INFO executor.Executor: Finished task 4.0 in stage 1.0 (TID 5). 2527 bytes result sent to driver\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:27:03,472 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:27:03,472 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:27:03,472 INFO output.DirectFileOutputCommitter: Direct Write: DISABLED\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:27:03,472 INFO datasources.SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.DirectFileOutputCommitter\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:27:03,473 INFO datasources.FileScanRDD: TID: 20 - Reading current file: path: file:///opt/ml/processing/event_all/20210302_052556_00048_8ak59_66a8e543-5f92-449b-9a57-f4fdf234d99f, range: 0-3274255, partition values: [empty row], isDataPresent: false\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:27:03,480 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:27:03,480 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:27:03,480 INFO output.DirectFileOutputCommitter: Direct Write: DISABLED\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:27:03,480 INFO datasources.SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.DirectFileOutputCommitter\u001b[0m\n",
      "\u001b[34m2021-03-02 05:27:05,725 INFO scheduler.TaskSetManager: Finished task 3.0 in stage 1.0 (TID 4) in 20403 ms on algo-1 (executor 1) (12/22)\u001b[0m\n",
      "\u001b[34m2021-03-02 05:27:05,739 INFO scheduler.TaskSetManager: Finished task 5.0 in stage 1.0 (TID 6) in 20417 ms on algo-1 (executor 1) (13/22)\u001b[0m\n",
      "\u001b[34m2021-03-02 05:27:05,753 INFO scheduler.TaskSetManager: Finished task 2.0 in stage 1.0 (TID 3) in 20432 ms on algo-1 (executor 1) (14/22)\u001b[0m\n",
      "\u001b[35m2021-03-02 05:27:05,725 INFO scheduler.TaskSetManager: Finished task 3.0 in stage 1.0 (TID 4) in 20403 ms on algo-1 (executor 1) (12/22)\u001b[0m\n",
      "\u001b[35m2021-03-02 05:27:05,739 INFO scheduler.TaskSetManager: Finished task 5.0 in stage 1.0 (TID 6) in 20417 ms on algo-1 (executor 1) (13/22)\u001b[0m\n",
      "\u001b[35m2021-03-02 05:27:05,753 INFO scheduler.TaskSetManager: Finished task 2.0 in stage 1.0 (TID 3) in 20432 ms on algo-1 (executor 1) (14/22)\u001b[0m\n",
      "\u001b[34m2021-03-02 05:27:05,872 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 1.0 (TID 2) in 20551 ms on algo-1 (executor 1) (15/22)\u001b[0m\n",
      "\u001b[34m2021-03-02 05:27:05,948 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 20629 ms on algo-1 (executor 1) (16/22)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:27:03,480 INFO datasources.FileScanRDD: TID: 19 - Reading current file: path: file:///opt/ml/processing/event_all/20210302_052556_00048_8ak59_f9aec2dd-076c-4b45-bd15-1da69209efb7, range: 0-3379306, partition values: [empty row], isDataPresent: false\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:27:03,491 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:27:03,491 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:27:03,491 INFO output.DirectFileOutputCommitter: Direct Write: DISABLED\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:27:03,491 INFO datasources.SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.DirectFileOutputCommitter\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:27:03,491 INFO datasources.FileScanRDD: TID: 21 - Reading current file: path: file:///opt/ml/processing/event_all/20210302_052556_00048_8ak59_5cf89946-1549-4be6-85df-c97782d4dcf2, range: 0-3269154, partition values: [empty row], isDataPresent: false\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:27:03,493 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:27:03,493 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:27:03,494 INFO output.DirectFileOutputCommitter: Direct Write: DISABLED\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:27:03,494 INFO datasources.SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.DirectFileOutputCommitter\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:27:03,494 INFO datasources.FileScanRDD: TID: 22 - Reading current file: path: file:///opt/ml/processing/event_all/20210302_052556_00048_8ak59_60e5f943-c98d-4c73-8aab-d171161f32cd, range: 0-3261176, partition values: [empty row], isDataPresent: false\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:27:05,721 INFO output.FileOutputCommitter: Saved output of task 'attempt_20210302052645_0001_m_000003_4' to s3a://sagemaker-ap-northeast-2-029498593638/export-flow-02-05-22-00-ca04653e/output/data-wrangler-flow-processing-02-05-22-00-ca04653e/410bb997-de0d-4339-8039-53e1d78e46f7/default\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:27:05,721 INFO mapred.SparkHadoopMapRedUtil: attempt_20210302052645_0001_m_000003_4: Committed\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:27:05,722 INFO executor.Executor: Finished task 3.0 in stage 1.0 (TID 4). 2527 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m2021-03-02 05:27:06,420 INFO scheduler.TaskSetManager: Finished task 17.0 in stage 1.0 (TID 18) in 12440 ms on algo-1 (executor 1) (17/22)\u001b[0m\n",
      "\u001b[34m2021-03-02 05:27:06,557 INFO scheduler.TaskSetManager: Finished task 16.0 in stage 1.0 (TID 17) in 12597 ms on algo-1 (executor 1) (18/22)\u001b[0m\n",
      "\u001b[35m2021-03-02 05:27:05,872 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 1.0 (TID 2) in 20551 ms on algo-1 (executor 1) (15/22)\u001b[0m\n",
      "\u001b[35m2021-03-02 05:27:05,948 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 20629 ms on algo-1 (executor 1) (16/22)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:27:03,480 INFO datasources.FileScanRDD: TID: 19 - Reading current file: path: file:///opt/ml/processing/event_all/20210302_052556_00048_8ak59_f9aec2dd-076c-4b45-bd15-1da69209efb7, range: 0-3379306, partition values: [empty row], isDataPresent: false\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:27:03,491 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:27:03,491 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:27:03,491 INFO output.DirectFileOutputCommitter: Direct Write: DISABLED\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:27:03,491 INFO datasources.SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.DirectFileOutputCommitter\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:27:03,491 INFO datasources.FileScanRDD: TID: 21 - Reading current file: path: file:///opt/ml/processing/event_all/20210302_052556_00048_8ak59_5cf89946-1549-4be6-85df-c97782d4dcf2, range: 0-3269154, partition values: [empty row], isDataPresent: false\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:27:03,493 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:27:03,493 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:27:03,494 INFO output.DirectFileOutputCommitter: Direct Write: DISABLED\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:27:03,494 INFO datasources.SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.DirectFileOutputCommitter\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:27:03,494 INFO datasources.FileScanRDD: TID: 22 - Reading current file: path: file:///opt/ml/processing/event_all/20210302_052556_00048_8ak59_60e5f943-c98d-4c73-8aab-d171161f32cd, range: 0-3261176, partition values: [empty row], isDataPresent: false\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:27:05,721 INFO output.FileOutputCommitter: Saved output of task 'attempt_20210302052645_0001_m_000003_4' to s3a://sagemaker-ap-northeast-2-029498593638/export-flow-02-05-22-00-ca04653e/output/data-wrangler-flow-processing-02-05-22-00-ca04653e/410bb997-de0d-4339-8039-53e1d78e46f7/default\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:27:05,721 INFO mapred.SparkHadoopMapRedUtil: attempt_20210302052645_0001_m_000003_4: Committed\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:27:05,722 INFO executor.Executor: Finished task 3.0 in stage 1.0 (TID 4). 2527 bytes result sent to driver\u001b[0m\n",
      "\u001b[35m2021-03-02 05:27:06,420 INFO scheduler.TaskSetManager: Finished task 17.0 in stage 1.0 (TID 18) in 12440 ms on algo-1 (executor 1) (17/22)\u001b[0m\n",
      "\u001b[35m2021-03-02 05:27:06,557 INFO scheduler.TaskSetManager: Finished task 16.0 in stage 1.0 (TID 17) in 12597 ms on algo-1 (executor 1) (18/22)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:27:05,736 INFO output.FileOutputCommitter: Saved output of task 'attempt_20210302052645_0001_m_000005_6' to s3a://sagemaker-ap-northeast-2-029498593638/export-flow-02-05-22-00-ca04653e/output/data-wrangler-flow-processing-02-05-22-00-ca04653e/410bb997-de0d-4339-8039-53e1d78e46f7/default\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:27:05,736 INFO mapred.SparkHadoopMapRedUtil: attempt_20210302052645_0001_m_000005_6: Committed\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:27:05,737 INFO executor.Executor: Finished task 5.0 in stage 1.0 (TID 6). 2527 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:27:05,751 INFO output.FileOutputCommitter: Saved output of task 'attempt_20210302052645_0001_m_000002_3' to s3a://sagemaker-ap-northeast-2-029498593638/export-flow-02-05-22-00-ca04653e/output/data-wrangler-flow-processing-02-05-22-00-ca04653e/410bb997-de0d-4339-8039-53e1d78e46f7/default\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:27:05,751 INFO mapred.SparkHadoopMapRedUtil: attempt_20210302052645_0001_m_000002_3: Committed\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:27:05,752 INFO executor.Executor: Finished task 2.0 in stage 1.0 (TID 3). 2527 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:27:05,869 INFO output.FileOutputCommitter: Saved output of task 'attempt_20210302052645_0001_m_000001_2' to s3a://sagemaker-ap-northeast-2-029498593638/export-flow-02-05-22-00-ca04653e/output/data-wrangler-flow-processing-02-05-22-00-ca04653e/410bb997-de0d-4339-8039-53e1d78e46f7/default\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:27:05,736 INFO output.FileOutputCommitter: Saved output of task 'attempt_20210302052645_0001_m_000005_6' to s3a://sagemaker-ap-northeast-2-029498593638/export-flow-02-05-22-00-ca04653e/output/data-wrangler-flow-processing-02-05-22-00-ca04653e/410bb997-de0d-4339-8039-53e1d78e46f7/default\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:27:05,736 INFO mapred.SparkHadoopMapRedUtil: attempt_20210302052645_0001_m_000005_6: Committed\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:27:05,737 INFO executor.Executor: Finished task 5.0 in stage 1.0 (TID 6). 2527 bytes result sent to driver\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:27:05,751 INFO output.FileOutputCommitter: Saved output of task 'attempt_20210302052645_0001_m_000002_3' to s3a://sagemaker-ap-northeast-2-029498593638/export-flow-02-05-22-00-ca04653e/output/data-wrangler-flow-processing-02-05-22-00-ca04653e/410bb997-de0d-4339-8039-53e1d78e46f7/default\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:27:05,751 INFO mapred.SparkHadoopMapRedUtil: attempt_20210302052645_0001_m_000002_3: Committed\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:27:05,752 INFO executor.Executor: Finished task 2.0 in stage 1.0 (TID 3). 2527 bytes result sent to driver\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:27:05,869 INFO output.FileOutputCommitter: Saved output of task 'attempt_20210302052645_0001_m_000001_2' to s3a://sagemaker-ap-northeast-2-029498593638/export-flow-02-05-22-00-ca04653e/output/data-wrangler-flow-processing-02-05-22-00-ca04653e/410bb997-de0d-4339-8039-53e1d78e46f7/default\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:27:05,869 INFO mapred.SparkHadoopMapRedUtil: attempt_20210302052645_0001_m_000001_2: Committed\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:27:05,870 INFO executor.Executor: Finished task 1.0 in stage 1.0 (TID 2). 2527 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:27:05,943 INFO output.FileOutputCommitter: Saved output of task 'attempt_20210302052645_0001_m_000000_1' to s3a://sagemaker-ap-northeast-2-029498593638/export-flow-02-05-22-00-ca04653e/output/data-wrangler-flow-processing-02-05-22-00-ca04653e/410bb997-de0d-4339-8039-53e1d78e46f7/default\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:27:05,944 INFO mapred.SparkHadoopMapRedUtil: attempt_20210302052645_0001_m_000000_1: Committed\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:27:05,945 INFO executor.Executor: Finished task 0.0 in stage 1.0 (TID 1). 2527 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:27:06,417 INFO output.FileOutputCommitter: Saved output of task 'attempt_20210302052645_0001_m_000017_18' to s3a://sagemaker-ap-northeast-2-029498593638/export-flow-02-05-22-00-ca04653e/output/data-wrangler-flow-processing-02-05-22-00-ca04653e/410bb997-de0d-4339-8039-53e1d78e46f7/default\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:27:06,417 INFO mapred.SparkHadoopMapRedUtil: attempt_20210302052645_0001_m_000017_18: Committed\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:27:06,418 INFO executor.Executor: Finished task 17.0 in stage 1.0 (TID 18). 2527 bytes result sent to driver\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:27:05,869 INFO mapred.SparkHadoopMapRedUtil: attempt_20210302052645_0001_m_000001_2: Committed\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:27:05,870 INFO executor.Executor: Finished task 1.0 in stage 1.0 (TID 2). 2527 bytes result sent to driver\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:27:05,943 INFO output.FileOutputCommitter: Saved output of task 'attempt_20210302052645_0001_m_000000_1' to s3a://sagemaker-ap-northeast-2-029498593638/export-flow-02-05-22-00-ca04653e/output/data-wrangler-flow-processing-02-05-22-00-ca04653e/410bb997-de0d-4339-8039-53e1d78e46f7/default\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:27:05,944 INFO mapred.SparkHadoopMapRedUtil: attempt_20210302052645_0001_m_000000_1: Committed\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:27:05,945 INFO executor.Executor: Finished task 0.0 in stage 1.0 (TID 1). 2527 bytes result sent to driver\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:27:06,417 INFO output.FileOutputCommitter: Saved output of task 'attempt_20210302052645_0001_m_000017_18' to s3a://sagemaker-ap-northeast-2-029498593638/export-flow-02-05-22-00-ca04653e/output/data-wrangler-flow-processing-02-05-22-00-ca04653e/410bb997-de0d-4339-8039-53e1d78e46f7/default\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:27:06,417 INFO mapred.SparkHadoopMapRedUtil: attempt_20210302052645_0001_m_000017_18: Committed\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:27:06,418 INFO executor.Executor: Finished task 17.0 in stage 1.0 (TID 18). 2527 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m2021-03-02 05:27:08,508 INFO scheduler.TaskSetManager: Finished task 19.0 in stage 1.0 (TID 20) in 5101 ms on algo-1 (executor 1) (19/22)\u001b[0m\n",
      "\u001b[35m2021-03-02 05:27:08,508 INFO scheduler.TaskSetManager: Finished task 19.0 in stage 1.0 (TID 20) in 5101 ms on algo-1 (executor 1) (19/22)\u001b[0m\n",
      "\u001b[34m2021-03-02 05:27:09,761 INFO scheduler.TaskSetManager: Finished task 20.0 in stage 1.0 (TID 21) in 6348 ms on algo-1 (executor 1) (20/22)\u001b[0m\n",
      "\u001b[35m2021-03-02 05:27:09,761 INFO scheduler.TaskSetManager: Finished task 20.0 in stage 1.0 (TID 21) in 6348 ms on algo-1 (executor 1) (20/22)\u001b[0m\n",
      "\u001b[34m2021-03-02 05:27:11,330 INFO scheduler.TaskSetManager: Finished task 21.0 in stage 1.0 (TID 22) in 7916 ms on algo-1 (executor 1) (21/22)\u001b[0m\n",
      "\u001b[34m2021-03-02 05:27:11,352 INFO scheduler.TaskSetManager: Finished task 18.0 in stage 1.0 (TID 19) in 10560 ms on algo-1 (executor 1) (22/22)\u001b[0m\n",
      "\u001b[34m2021-03-02 05:27:11,352 INFO cluster.YarnScheduler: Removed TaskSet 1.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2021-03-02 05:27:11,353 INFO scheduler.DAGScheduler: ResultStage 1 (csv at NativeMethodAccessorImpl.java:0) finished in 26.116 s\u001b[0m\n",
      "\u001b[34m2021-03-02 05:27:11,353 INFO scheduler.DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2021-03-02 05:27:11,353 INFO cluster.YarnScheduler: Killing all running tasks in stage 1: Stage finished\u001b[0m\n",
      "\u001b[34m2021-03-02 05:27:11,353 INFO scheduler.DAGScheduler: Job 1 finished: csv at NativeMethodAccessorImpl.java:0, took 26.124589 s\u001b[0m\n",
      "\u001b[35m2021-03-02 05:27:11,330 INFO scheduler.TaskSetManager: Finished task 21.0 in stage 1.0 (TID 22) in 7916 ms on algo-1 (executor 1) (21/22)\u001b[0m\n",
      "\u001b[35m2021-03-02 05:27:11,352 INFO scheduler.TaskSetManager: Finished task 18.0 in stage 1.0 (TID 19) in 10560 ms on algo-1 (executor 1) (22/22)\u001b[0m\n",
      "\u001b[35m2021-03-02 05:27:11,352 INFO cluster.YarnScheduler: Removed TaskSet 1.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[35m2021-03-02 05:27:11,353 INFO scheduler.DAGScheduler: ResultStage 1 (csv at NativeMethodAccessorImpl.java:0) finished in 26.116 s\u001b[0m\n",
      "\u001b[35m2021-03-02 05:27:11,353 INFO scheduler.DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[35m2021-03-02 05:27:11,353 INFO cluster.YarnScheduler: Killing all running tasks in stage 1: Stage finished\u001b[0m\n",
      "\u001b[35m2021-03-02 05:27:11,353 INFO scheduler.DAGScheduler: Job 1 finished: csv at NativeMethodAccessorImpl.java:0, took 26.124589 s\u001b[0m\n",
      "\u001b[34m2021-03-02 05:27:12,027 INFO datasources.FileFormatWriter: Write Job 5a49f5fc-1dd1-495d-8a8a-b3fecbeb9bf9 committed.\u001b[0m\n",
      "\u001b[34m2021-03-02 05:27:12,030 INFO datasources.FileFormatWriter: Finished processing stats for write job 5a49f5fc-1dd1-495d-8a8a-b3fecbeb9bf9.\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:27:06,554 INFO output.FileOutputCommitter: Saved output of task 'attempt_20210302052645_0001_m_000016_17' to s3a://sagemaker-ap-northeast-2-029498593638/export-flow-02-05-22-00-ca04INFO:sagemaker_dataprep:{\"event_type\": \"mohave.backend.spark.performance\", \"request_context\": {\"client_request_id\": null}, \"app_context\": {\"wrangler_version\": \"1.2.1\", \"spark_version\": \"3.0.0+amzn.0\"}, \"engine\": \"spark\", \"metadata\": {\"operator_name\": \"sagemaker.athena_source_0.1\"}, \"spark\": {\"stage_ids\": [0, 1], \"stages\": [{\"stage_id\": 0, \"task_metrics\": [{\"executor_runtime\": 367.0, \"executor_deserialize_time\": 559.0, \"scheduler_delay\": 64.0, \"gc_time\": 0.0, \"result_serialization_time\": 3.0, \"peak_execution_memory\": 0.0, \"result_size\": 1772.0, \"bytes_read\": 0.0, \"records_read\": 0.0, \"bytes_written\": 0.0, \"records_written\": 0.0, \"attempt_id\": 0, \"attempt_status\": \"COMPLETE\"}]}, {\"stage_id\": 1, \"task_metrics\": [{\"executor_runtime\": 6268.0, \"executor_deserialize_time\": 11.0, \"scheduler_delay\": 6.0, \"gc_time\": 21.0, \"result_serialization_time\": 0.0, \"peak_execution_memory\": 0.0, \"result_size\": 2527.0, \"bytes_read\": 3269771.0, \"records_read\": 694491.0, \"bytes_written\": 70331191.0, \"records_written\": 694491.0, \"attempt_id\": 0, \"attempt_status\": \"COMPLETE\"}]}], \"aggregate_metrics\": [{\"executor_id\": \"driver\", \"total_runtime\": 0, \"total_storage_memory\": 1056807321, \"storage_memory_used\": 100265, \"peak_memory_metrics\": {\"storage_memory\": 620009, \"execution_memory\": 0}}, {\"executor_id\": \"1\", \"total_runtime\": 336090, \"total_storage_memory\": 30984477081, \"storage_memory_used\": 100265, \"peak_memory_metrics\": {\"storage_memory\": 872817, \"execution_memory\": 0}}]}}\u001b[0m\n",
      "\u001b[34m2021-03-02 05:27:12,360 INFO spark.SparkContext: Invoking stop() from shutdown hook\u001b[0m\n",
      "\u001b[34m2021-03-02 05:27:12,366 INFO server.AbstractConnector: Stopped Spark@715c241{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}\u001b[0m\n",
      "\u001b[34m2021-03-02 05:27:12,367 INFO ui.SparkUI: Stopped Spark web UI at http://10.0.189.145:4040\u001b[0m\n",
      "\u001b[34m2021-03-02 05:27:12,371 INFO cluster.YarnClientSchedulerBackend: Interrupting monitor thread\u001b[0m\n",
      "\u001b[34m2021-03-02 05:27:12,390 INFO cluster.YarnClientSchedulerBackend: Shutting down all executors\u001b[0m\n",
      "\u001b[35m2021-03-02 05:27:12,027 INFO datasources.FileFormatWriter: Write Job 5a49f5fc-1dd1-495d-8a8a-b3fecbeb9bf9 committed.\u001b[0m\n",
      "\u001b[35m2021-03-02 05:27:12,030 INFO datasources.FileFormatWriter: Finished processing stats for write job 5a49f5fc-1dd1-495d-8a8a-b3fecbeb9bf9.\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stderr] 2021-03-02 05:27:06,554 INFO output.FileOutputCommitter: Saved output of task 'attempt_20210302052645_0001_m_000016_17' to s3a://sagemaker-ap-northeast-2-029498593638/export-flow-02-05-22-00-ca04INFO:sagemaker_dataprep:{\"event_type\": \"mohave.backend.spark.performance\", \"request_context\": {\"client_request_id\": null}, \"app_context\": {\"wrangler_version\": \"1.2.1\", \"spark_version\": \"3.0.0+amzn.0\"}, \"engine\": \"spark\", \"metadata\": {\"operator_name\": \"sagemaker.athena_source_0.1\"}, \"spark\": {\"stage_ids\": [0, 1], \"stages\": [{\"stage_id\": 0, \"task_metrics\": [{\"executor_runtime\": 367.0, \"executor_deserialize_time\": 559.0, \"scheduler_delay\": 64.0, \"gc_time\": 0.0, \"result_serialization_time\": 3.0, \"peak_execution_memory\": 0.0, \"result_size\": 1772.0, \"bytes_read\": 0.0, \"records_read\": 0.0, \"bytes_written\": 0.0, \"records_written\": 0.0, \"attempt_id\": 0, \"attempt_status\": \"COMPLETE\"}]}, {\"stage_id\": 1, \"task_metrics\": [{\"executor_runtime\": 6268.0, \"executor_deserialize_time\": 11.0, \"scheduler_delay\": 6.0, \"gc_time\": 21.0, \"result_serialization_time\": 0.0, \"peak_execution_memory\": 0.0, \"result_size\": 2527.0, \"bytes_read\": 3269771.0, \"records_read\": 694491.0, \"bytes_written\": 70331191.0, \"records_written\": 694491.0, \"attempt_id\": 0, \"attempt_status\": \"COMPLETE\"}]}], \"aggregate_metrics\": [{\"executor_id\": \"driver\", \"total_runtime\": 0, \"total_storage_memory\": 1056807321, \"storage_memory_used\": 100265, \"peak_memory_metrics\": {\"storage_memory\": 620009, \"execution_memory\": 0}}, {\"executor_id\": \"1\", \"total_runtime\": 336090, \"total_storage_memory\": 30984477081, \"storage_memory_used\": 100265, \"peak_memory_metrics\": {\"storage_memory\": 872817, \"execution_memory\": 0}}]}}\u001b[0m\n",
      "\u001b[35m2021-03-02 05:27:12,360 INFO spark.SparkContext: Invoking stop() from shutdown hook\u001b[0m\n",
      "\u001b[35m2021-03-02 05:27:12,366 INFO server.AbstractConnector: Stopped Spark@715c241{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}\u001b[0m\n",
      "\u001b[35m2021-03-02 05:27:12,367 INFO ui.SparkUI: Stopped Spark web UI at http://10.0.189.145:4040\u001b[0m\n",
      "\u001b[35m2021-03-02 05:27:12,371 INFO cluster.YarnClientSchedulerBackend: Interrupting monitor thread\u001b[0m\n",
      "\u001b[35m2021-03-02 05:27:12,390 INFO cluster.YarnClientSchedulerBackend: Shutting down all executors\u001b[0m\n",
      "\u001b[34m2021-03-02 05:27:12,390 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Asking each executor to shut down\u001b[0m\n",
      "\u001b[34m2K->22724K(995328K)] 1213756K->374059K(1862144K), 0.0188431 secs] [Times: user=0.18 sys=0.04, real=0.02 secs] \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stdout] 2021-03-02T05:26:48.516+0000: [GC (Allocation Failure) [PSYoungGen: 865988K->23130K(1423872K)] 1217323K->384425K(2290688K), 0.0123641 secs] [Times: user=0.13 sys=0.02, real=0.02 secs] \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stdout] 2021-03-02T05:26:48.674+0000: [GC (Allocation Failure) [PSYoungGen: 1306202K->29218K(1426432K)] 1667497K->408919K(2293248K), 0.0125995 secs] [Times: user=0.12 sys=0.03, real=0.01 secs] \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stdout] 2021-03-02T05:26:48.813+0000: [GC (Allocation Failure) [PSYoungGen: 1312290K->27149K(2049536K)] 1691991K->418874K(2916352K), 0.0075867 secs] [Times: user=0.06 sys=0.02, real=0.01 secs] \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stdout] 2021-03-02T05:26:49.026+0000: [GC (Allocation Failure) [PSYoungGen: 1961485K->26718K(2066432K)] 2353210K->421604K(2933248K), 0.0077218 secs] [Times: user=0.05 sys=0.02, real=0.01 secs] \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stdout] 2021-03-02T05:26:49.217+0000: [GC (Allocation Failure) [PSYoungGen: 1961054K->28073K(2950656K)] 2355940K->425640K(3817472K), 0.0065495 secs] [Times: user=0.07 sys=0.00, real=0.01 secs] \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stdout] 2021-03-02T05:26:49.526+0000: [GC (Allocation Failure) [PSYoungGen: 2889641K->29063K(2982400K)] 3287208K->429228K(3849216K), 0.0078654 secs] [Times: user=0.06 sys=0.01, real=0.01 secs] \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stdout] 2021-03-02T05:26:49.798+0000: [GC (Allocation Failure) [PSYoungGen: 2890631K->28004K(3790336K)] 3290796K->431497K(4657152K), 0.0073278 secs] [Times: user=0.04 sys=0.04, real=0.01 secs] \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stdout] 2021-03-02T05:26:50.185+0000: [GC (Allocation Failure) [PSYoungGen: 3741540K->29255K(3823616K)] 4145033K->436052K(4690432K), 0.0093265 secs] [Times: user=0.08 sys=0.01, real=0.01 secs] \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stdout] 2021-03-02T05:26:50.754+0000: [GC (Allocation Failure) [PSYoungGen: 3742791K->24280K(4760576K)] 4149588K->435126K(5627392K), 0.0081861 secs] [Times: user=0.07 sys=0.01, real=0.01 secs] \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stdout] 2021-03-02T05:26:52.201+0000: [GC (Allocation Failure) [PSYoungGen: 4744408K->17707K(4820480K)] 5155254K->436084K(5687296K), 0.0104229 secs] [Times: user=0.08 sys=0.02, real=0.01 secs] \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stdout] 2021-03-02T05:27:03.522+0000: [GC (Allocation Failure) [PSYoungGen: 4737835K->51254K(5830144K)] 5156212K->478417K(6696960K), 0.0140619 secs] [Times: user=0.08 sys=0.03, real=0.01 secs] \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stdout] 2021-03-02T05:27:05.936+0000: [GC (Allocation Failure) [PSYoungGen: 5829686K->1440K(5954048K)] 6256849K->441638K(6820864K), 0.0068597 secs] [Times: user=0.03 sys=0.03, real=0.01 secs] \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stdout] Heap\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stdout]  PSYoungGen      total 5954048K, used 417550K [0x00007f700fb80000, 0x00007f71c2900000, 0x00007f7499000000)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stdout]   eden space 5856768K, 7% used [0x00007f700fb80000,0x00007f70291dbb80,0x00007f7175300000)\u001b[0m\n",
      "\u001b[34m2021-03-02 05:27:12,496 INFO cluster.YarnClientSchedulerBackend: YARN client scheduler backend Stopped\u001b[0m\n",
      "\u001b[34m2021-03-02 05:27:12,506 INFO spark.MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\u001b[0m\n",
      "\u001b[34m2021-03-02 05:27:12,516 INFO memory.MemoryStore: MemoryStore cleared\u001b[0m\n",
      "\u001b[34m2021-03-02 05:27:12,517 INFO storage.BlockManager: BlockManager stopped\u001b[0m\n",
      "\u001b[34m2021-03-02 05:27:12,520 INFO storage.BlockManagerMaster: BlockManagerMaster stopped\u001b[0m\n",
      "\u001b[34m2021-03-02 05:27:12,524 INFO scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\u001b[0m\n",
      "\u001b[34m2021-03-02 05:27:12,532 INFO spark.SparkContext: Successfully stopped SparkContext\u001b[0m\n",
      "\u001b[34m2021-03-02 05:27:12,532 INFO util.ShutdownHookManager: Shutdown hook called\u001b[0m\n",
      "\u001b[34m2021-03-02 05:27:12,533 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-a9207a71-c796-421d-9264-f0207b72ca3a/pyspark-48959058-364f-400e-ad4b-957e1d3d4269\u001b[0m\n",
      "\u001b[34m2021-03-02 05:27:12,537 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-cb92f7ce-caaa-4feb-91bd-80b98247d4af\u001b[0m\n",
      "\u001b[34m2021-03-02 05:27:12,542 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-a9207a71-c796-421d-9264-f0207b72ca3a\u001b[0m\n",
      "\u001b[34m2021-03-02 05:27:12,548 INFO impl.MetricsSystemImpl: Stopping s3a-file-system metrics system...\u001b[0m\n",
      "\u001b[34m2021-03-02 05:27:12,549 INFO impl.MetricsSystemImpl: s3a-file-system metrics system stopped.\u001b[0m\n",
      "\u001b[34m2021-03-02 05:27:12,549 INFO impl.MetricsSystemImpl: s3a-file-system metrics system shutdown complete.\u001b[0m\n",
      "\u001b[34m2021-03-02 05:27:12,553 INFO attempt.RMAppAttemptImpl: Updating application attempt appattempt_1614662779782_0001_000001 with final state: FINISHING, and exit status: -1000\u001b[0m\n",
      "\u001b[34m2021-03-02 05:27:12,554 INFO attempt.RMAppAttemptImpl: appattempt_1614662779782_0001_000001 State change from RUNNING to FINAL_SAVING on event = UNREGISTERED\u001b[0m\n",
      "\u001b[34m2021-03-02 05:27:12,554 INFO rmapp.RMAppImpl: Updating application application_1614662779782_0001 with final state: FINISHING\u001b[0m\n",
      "\u001b[34m2021-03-02 05:27:12,554 INFO recovery.RMStateStore: Updating info for app: application_1614662779782_0001\u001b[0m\n",
      "\u001b[34m2021-03-02 05:27:12,554 INFO rmapp.RMAppImpl: application_1614662779782_0001 State change from RUNNING to FINAL_SAVING on event = ATTEMPT_UNREGISTERED\u001b[0m\n",
      "\u001b[34m2021-03-02 05:27:12,555 INFO attempt.RMAppAttemptImpl: appattempt_1614662779782_0001_000001 State change from FINAL_SAVING to FINISHING on event = ATTEMPT_UPDATE_SAVED\u001b[0m\n",
      "\u001b[34m2021-03-02 05:27:12,555 INFO rmapp.RMAppImpl: application_1614662779782_0001 State change from FINAL_SAVING to FINISHING on event = APP_UPDATE_SAVED\u001b[0m\n",
      "\u001b[34m2021-03-02 05:27:12,640 INFO launcher.ContainerLaunch: Container container_1614662779782_0001_01_000002 succeeded \u001b[0m\n",
      "\u001b[34m2021-03-02 05:27:12,649 INFO container.ContainerImpl: Container container_1614662779782_0001_01_000002 transitioned from RUNNING to EXITED_WITH_SUCCESS\u001b[0m\n",
      "\u001b[34m2021-03-02 05:27:12,651 INFO launcher.ContainerCleanup: Cleaning up container container_1614662779782_0001_01_000002\u001b[0m\n",
      "\u001b[34m2021-03-02 05:27:12,652 INFO nodemanager.NMAuditLogger: USER=root#011OPERATION=Container Finished - Succeeded#011TARGET=ContainerImpl#011RESULT=SUCCESS#011APPID=application_1614662779782_0001#011CONTAINERID=container_1614662779782_0001_01_000002\u001b[0m\n",
      "\u001b[34m2021-03-02 05:27:12,653 INFO nodemanager.DefaultContainerExecutor: Deleting absolute path : /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1614662779782_0001/container_1614662779782_0001_01_000002\u001b[0m\n",
      "\u001b[34m2021-03-02 05:27:12,654 INFO container.ContainerImpl: Container container_1614662779782_0001_01_000002 transitioned from EXITED_WITH_SUCCESS to DONE\u001b[0m\n",
      "\u001b[34m2021-03-02 05:27:12,654 INFO application.ApplicationImpl: Removing container_1614662779782_0001_01_000002 from application application_1614662779782_0001\u001b[0m\n",
      "\u001b[34m2021-03-02 05:27:12,654 INFO monitor.ContainersMonitorImpl: Stopping resource-monitoring for container_1614662779782_0001_01_000002\u001b[0m\n",
      "\u001b[34m2021-03-02 05:27:12,655 INFO containermanager.AuxServices: Got event CONTAINER_STOP for appId application_1614662779782_0001\u001b[0m\n",
      "\u001b[34m2021-03-02 05:27:12,656 INFO resourcemanager.ApplicationMasterService: application_1614662779782_0001 unregistered successfully. \u001b[0m\n",
      "\u001b[34m2021-03-02 05:27:12,657 INFO rmcontainer.RMContainerImpl: container_1614662779782_0001_01_000002 Container Transitioned from RUNNING to COMPLETED\u001b[0m\n",
      "\u001b[34m2021-03-02 05:27:12,657 INFO resourcemanager.RMAuditLogger: USER=root#011OPERATION=AM Released Container#011TARGET=SchedulerApp#011RESULT=SUCCESS#011APPID=application_1614662779782_0001#011CONTAINERID=container_1614662779782_0001_01_000002#011RESOURCE=<memory:61316, vCores:1>#011QUEUENAME=default\u001b[0m\n",
      "\u001b[34m2021-03-02 05:27:12,668 INFO nodemanager.DefaultContainerExecutor: Deleting absolute path : /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1614662779782_0001/container_1614662779782_0001_01_000002/launch_container.sh\u001b[0m\n",
      "\u001b[34m2021-03-02 05:27:12,668 WARN nodemanager.DefaultContainerExecutor: delete returned false for path: [/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1614662779782_0001/container_1614662779782_0001_01_000002/launch_container.sh]\u001b[0m\n",
      "\u001b[35m2021-03-02 05:27:12,390 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Asking each executor to shut down\u001b[0m\n",
      "\u001b[35m2K->22724K(995328K)] 1213756K->374059K(1862144K), 0.0188431 secs] [Times: user=0.18 sys=0.04, real=0.02 secs] \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stdout] 2021-03-02T05:26:48.516+0000: [GC (Allocation Failure) [PSYoungGen: 865988K->23130K(1423872K)] 1217323K->384425K(2290688K), 0.0123641 secs] [Times: user=0.13 sys=0.02, real=0.02 secs] \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stdout] 2021-03-02T05:26:48.674+0000: [GC (Allocation Failure) [PSYoungGen: 1306202K->29218K(1426432K)] 1667497K->408919K(2293248K), 0.0125995 secs] [Times: user=0.12 sys=0.03, real=0.01 secs] \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stdout] 2021-03-02T05:26:48.813+0000: [GC (Allocation Failure) [PSYoungGen: 1312290K->27149K(2049536K)] 1691991K->418874K(2916352K), 0.0075867 secs] [Times: user=0.06 sys=0.02, real=0.01 secs] \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stdout] 2021-03-02T05:26:49.026+0000: [GC (Allocation Failure) [PSYoungGen: 1961485K->26718K(2066432K)] 2353210K->421604K(2933248K), 0.0077218 secs] [Times: user=0.05 sys=0.02, real=0.01 secs] \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stdout] 2021-03-02T05:26:49.217+0000: [GC (Allocation Failure) [PSYoungGen: 1961054K->28073K(2950656K)] 2355940K->425640K(3817472K), 0.0065495 secs] [Times: user=0.07 sys=0.00, real=0.01 secs] \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stdout] 2021-03-02T05:26:49.526+0000: [GC (Allocation Failure) [PSYoungGen: 2889641K->29063K(2982400K)] 3287208K->429228K(3849216K), 0.0078654 secs] [Times: user=0.06 sys=0.01, real=0.01 secs] \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stdout] 2021-03-02T05:26:49.798+0000: [GC (Allocation Failure) [PSYoungGen: 2890631K->28004K(3790336K)] 3290796K->431497K(4657152K), 0.0073278 secs] [Times: user=0.04 sys=0.04, real=0.01 secs] \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stdout] 2021-03-02T05:26:50.185+0000: [GC (Allocation Failure) [PSYoungGen: 3741540K->29255K(3823616K)] 4145033K->436052K(4690432K), 0.0093265 secs] [Times: user=0.08 sys=0.01, real=0.01 secs] \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stdout] 2021-03-02T05:26:50.754+0000: [GC (Allocation Failure) [PSYoungGen: 3742791K->24280K(4760576K)] 4149588K->435126K(5627392K), 0.0081861 secs] [Times: user=0.07 sys=0.01, real=0.01 secs] \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stdout] 2021-03-02T05:26:52.201+0000: [GC (Allocation Failure) [PSYoungGen: 4744408K->17707K(4820480K)] 5155254K->436084K(5687296K), 0.0104229 secs] [Times: user=0.08 sys=0.02, real=0.01 secs] \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stdout] 2021-03-02T05:27:03.522+0000: [GC (Allocation Failure) [PSYoungGen: 4737835K->51254K(5830144K)] 5156212K->478417K(6696960K), 0.0140619 secs] [Times: user=0.08 sys=0.03, real=0.01 secs] \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stdout] 2021-03-02T05:27:05.936+0000: [GC (Allocation Failure) [PSYoungGen: 5829686K->1440K(5954048K)] 6256849K->441638K(6820864K), 0.0068597 secs] [Times: user=0.03 sys=0.03, real=0.01 secs] \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stdout] Heap\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stdout]  PSYoungGen      total 5954048K, used 417550K [0x00007f700fb80000, 0x00007f71c2900000, 0x00007f7499000000)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1614662779782_0001/container_1614662779782_0001_01_000002/stdout]   eden space 5856768K, 7% used [0x00007f700fb80000,0x00007f70291dbb80,0x00007f7175300000)\u001b[0m\n",
      "\u001b[35m2021-03-02 05:27:12,496 INFO cluster.YarnClientSchedulerBackend: YARN client scheduler backend Stopped\u001b[0m\n",
      "\u001b[35m2021-03-02 05:27:12,506 INFO spark.MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\u001b[0m\n",
      "\u001b[35m2021-03-02 05:27:12,516 INFO memory.MemoryStore: MemoryStore cleared\u001b[0m\n",
      "\u001b[35m2021-03-02 05:27:12,517 INFO storage.BlockManager: BlockManager stopped\u001b[0m\n",
      "\u001b[35m2021-03-02 05:27:12,520 INFO storage.BlockManagerMaster: BlockManagerMaster stopped\u001b[0m\n",
      "\u001b[35m2021-03-02 05:27:12,524 INFO scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\u001b[0m\n",
      "\u001b[35m2021-03-02 05:27:12,532 INFO spark.SparkContext: Successfully stopped SparkContext\u001b[0m\n",
      "\u001b[35m2021-03-02 05:27:12,532 INFO util.ShutdownHookManager: Shutdown hook called\u001b[0m\n",
      "\u001b[35m2021-03-02 05:27:12,533 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-a9207a71-c796-421d-9264-f0207b72ca3a/pyspark-48959058-364f-400e-ad4b-957e1d3d4269\u001b[0m\n",
      "\u001b[35m2021-03-02 05:27:12,537 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-cb92f7ce-caaa-4feb-91bd-80b98247d4af\u001b[0m\n",
      "\u001b[35m2021-03-02 05:27:12,542 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-a9207a71-c796-421d-9264-f0207b72ca3a\u001b[0m\n",
      "\u001b[35m2021-03-02 05:27:12,548 INFO impl.MetricsSystemImpl: Stopping s3a-file-system metrics system...\u001b[0m\n",
      "\u001b[35m2021-03-02 05:27:12,549 INFO impl.MetricsSystemImpl: s3a-file-system metrics system stopped.\u001b[0m\n",
      "\u001b[35m2021-03-02 05:27:12,549 INFO impl.MetricsSystemImpl: s3a-file-system metrics system shutdown complete.\u001b[0m\n",
      "\u001b[35m2021-03-02 05:27:12,553 INFO attempt.RMAppAttemptImpl: Updating application attempt appattempt_1614662779782_0001_000001 with final state: FINISHING, and exit status: -1000\u001b[0m\n",
      "\u001b[35m2021-03-02 05:27:12,554 INFO attempt.RMAppAttemptImpl: appattempt_1614662779782_0001_000001 State change from RUNNING to FINAL_SAVING on event = UNREGISTERED\u001b[0m\n",
      "\u001b[35m2021-03-02 05:27:12,554 INFO rmapp.RMAppImpl: Updating application application_1614662779782_0001 with final state: FINISHING\u001b[0m\n",
      "\u001b[35m2021-03-02 05:27:12,554 INFO recovery.RMStateStore: Updating info for app: application_1614662779782_0001\u001b[0m\n",
      "\u001b[35m2021-03-02 05:27:12,554 INFO rmapp.RMAppImpl: application_1614662779782_0001 State change from RUNNING to FINAL_SAVING on event = ATTEMPT_UNREGISTERED\u001b[0m\n",
      "\u001b[35m2021-03-02 05:27:12,555 INFO attempt.RMAppAttemptImpl: appattempt_1614662779782_0001_000001 State change from FINAL_SAVING to FINISHING on event = ATTEMPT_UPDATE_SAVED\u001b[0m\n",
      "\u001b[35m2021-03-02 05:27:12,555 INFO rmapp.RMAppImpl: application_1614662779782_0001 State change from FINAL_SAVING to FINISHING on event = APP_UPDATE_SAVED\u001b[0m\n",
      "\u001b[35m2021-03-02 05:27:12,640 INFO launcher.ContainerLaunch: Container container_1614662779782_0001_01_000002 succeeded \u001b[0m\n",
      "\u001b[35m2021-03-02 05:27:12,649 INFO container.ContainerImpl: Container container_1614662779782_0001_01_000002 transitioned from RUNNING to EXITED_WITH_SUCCESS\u001b[0m\n",
      "\u001b[35m2021-03-02 05:27:12,651 INFO launcher.ContainerCleanup: Cleaning up container container_1614662779782_0001_01_000002\u001b[0m\n",
      "\u001b[35m2021-03-02 05:27:12,652 INFO nodemanager.NMAuditLogger: USER=root#011OPERATION=Container Finished - Succeeded#011TARGET=ContainerImpl#011RESULT=SUCCESS#011APPID=application_1614662779782_0001#011CONTAINERID=container_1614662779782_0001_01_000002\u001b[0m\n",
      "\u001b[35m2021-03-02 05:27:12,653 INFO nodemanager.DefaultContainerExecutor: Deleting absolute path : /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1614662779782_0001/container_1614662779782_0001_01_000002\u001b[0m\n",
      "\u001b[35m2021-03-02 05:27:12,654 INFO container.ContainerImpl: Container container_1614662779782_0001_01_000002 transitioned from EXITED_WITH_SUCCESS to DONE\u001b[0m\n",
      "\u001b[35m2021-03-02 05:27:12,654 INFO application.ApplicationImpl: Removing container_1614662779782_0001_01_000002 from application application_1614662779782_0001\u001b[0m\n",
      "\u001b[35m2021-03-02 05:27:12,654 INFO monitor.ContainersMonitorImpl: Stopping resource-monitoring for container_1614662779782_0001_01_000002\u001b[0m\n",
      "\u001b[35m2021-03-02 05:27:12,655 INFO containermanager.AuxServices: Got event CONTAINER_STOP for appId application_1614662779782_0001\u001b[0m\n",
      "\u001b[35m2021-03-02 05:27:12,656 INFO resourcemanager.ApplicationMasterService: application_1614662779782_0001 unregistered successfully. \u001b[0m\n",
      "\u001b[35m2021-03-02 05:27:12,657 INFO rmcontainer.RMContainerImpl: container_1614662779782_0001_01_000002 Container Transitioned from RUNNING to COMPLETED\u001b[0m\n",
      "\u001b[35m2021-03-02 05:27:12,657 INFO resourcemanager.RMAuditLogger: USER=root#011OPERATION=AM Released Container#011TARGET=SchedulerApp#011RESULT=SUCCESS#011APPID=application_1614662779782_0001#011CONTAINERID=container_1614662779782_0001_01_000002#011RESOURCE=<memory:61316, vCores:1>#011QUEUENAME=default\u001b[0m\n",
      "\u001b[35m2021-03-02 05:27:12,668 INFO nodemanager.DefaultContainerExecutor: Deleting absolute path : /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1614662779782_0001/container_1614662779782_0001_01_000002/launch_container.sh\u001b[0m\n",
      "\u001b[35m2021-03-02 05:27:12,668 WARN nodemanager.DefaultContainerExecutor: delete returned false for path: [/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1614662779782_0001/container_1614662779782_0001_01_000002/launch_container.sh]\u001b[0m\n",
      "\u001b[34m2021-03-02 05:27:12,668 INFO nodemanager.DefaultContainerExecutor: Deleting absolute path : /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1614662779782_0001/container_1614662779782_0001_01_000002/container_tokens\u001b[0m\n",
      "\u001b[34m2021-03-02 05:27:12,668 WARN nodemanager.DefaultContainerExecutor: delete returned false for path: [/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1614662779782_0001/container_1614662779782_0001_01_000002/container_tokens]\u001b[0m\n",
      "\u001b[34m2021-03-02 05:27:12,669 INFO nodemanager.DefaultContainerExecutor: Deleting absolute path : /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1614662779782_0001/container_1614662779782_0001_01_000002/sysfs\u001b[0m\n",
      "\u001b[34m2021-03-02 05:27:12,669 WARN nodemanager.DefaultContainerExecutor: delete returned false for path: [/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1614662779782_0001/container_1614662779782_0001_01_000002/sysfs]\u001b[0m\n",
      "\u001b[34m03-02 05:27 smspark-submit INFO     spark submit was successful. primary node exiting.\u001b[0m\n",
      "\u001b[35m2021-03-02 05:27:12,668 INFO nodemanager.DefaultContainerExecutor: Deleting absolute path : /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1614662779782_0001/container_1614662779782_0001_01_000002/container_tokens\u001b[0m\n",
      "\u001b[35m2021-03-02 05:27:12,668 WARN nodemanager.DefaultContainerExecutor: delete returned false for path: [/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1614662779782_0001/container_1614662779782_0001_01_000002/container_tokens]\u001b[0m\n",
      "\u001b[35m2021-03-02 05:27:12,669 INFO nodemanager.DefaultContainerExecutor: Deleting absolute path : /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1614662779782_0001/container_1614662779782_0001_01_000002/sysfs\u001b[0m\n",
      "\u001b[35m2021-03-02 05:27:12,669 WARN nodemanager.DefaultContainerExecutor: delete returned false for path: [/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1614662779782_0001/container_1614662779782_0001_01_000002/sysfs]\u001b[0m\n",
      "\u001b[35m03-02 05:27 smspark-submit INFO     spark submit was successful. primary node exiting.\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.processing import Processor\n",
    "\n",
    "processor = Processor(\n",
    "    role=iam_role,\n",
    "    image_uri=container_uri,\n",
    "    instance_count=instance_count,\n",
    "    instance_type=instance_type,\n",
    "    sagemaker_session=sess\n",
    ")\n",
    "\n",
    "processor.run(\n",
    "    inputs=create_processing_inputs(processing_dir, flow, flow_uri),\n",
    "    outputs=[create_processing_output(output_name, output_path, processing_dir)],\n",
    "    arguments=create_container_arguments(output_name, output_content_type),\n",
    "    wait=True,\n",
    "    logs=True,\n",
    "    job_name=processing_job_name\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kick off SageMaker Training Job (Optional)\n",
    "Data Wrangler is a SageMaker tool for processing data to be used for Machine Learning. Now that\n",
    "the data has been processed, users will want to train a model using the data. The following shows\n",
    "an example of doing so using a popular algorithm XGBoost.\n",
    "\n",
    "It is important to note that the following XGBoost objective ['binary', 'regression',\n",
    "'multiclass'], hyperparameters, or content_type may not be suitable for the output data, and will\n",
    "require changes to train a proper model. Furthermore, for CSV training, the algorithm assumes that\n",
    "the target variable is in the first column. For more information on SageMaker XGBoost, please see\n",
    "https://docs.aws.amazon.com/sagemaker/latest/dg/xgboost.html.\n",
    "\n",
    "### Find Training Data path\n",
    "The below demonstrates how to recursively search the output directory to find the data location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_client = boto3.client(\"s3\")\n",
    "list_response = s3_client.list_objects_v2(Bucket=bucket, Prefix=output_prefix)\n",
    "\n",
    "training_path = None\n",
    "\n",
    "for content in list_response[\"Contents\"]:\n",
    "    if \"_SUCCESS\" not in content[\"Key\"]:\n",
    "        training_path = content[\"Key\"]\n",
    "\n",
    "print(training_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, the Training Job hyperparameters are set. For more information on XGBoost Hyperparameters,\n",
    "see https://xgboost.readthedocs.io/en/latest/parameter.html."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "region = boto3.Session().region_name\n",
    "container = sagemaker.image_uris.retrieve(\"xgboost\", region, \"1.2-1\")\n",
    "hyperparameters = {\n",
    "    \"max_depth\":\"5\",\n",
    "    \"objective\": \"reg:squarederror\",\n",
    "    \"num_round\": \"10\",\n",
    "}\n",
    "train_content_type = (\n",
    "    \"application/x-parquet\" if output_content_type.upper() == \"PARQUET\"\n",
    "    else \"text/csv\"\n",
    ")\n",
    "train_input = sagemaker.inputs.TrainingInput(\n",
    "    s3_data=f\"s3://{bucket}/{training_path}\",\n",
    "    content_type=train_content_type,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The TrainingJob configurations are set using the SageMaker Python SDK Estimator, and which is fit\n",
    "using the training data from the ProcessingJob that was run earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = sagemaker.estimator.Estimator(\n",
    "    container,\n",
    "    iam_role,\n",
    "    hyperparameters=hyperparameters,\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.m5.2xlarge\",\n",
    ")\n",
    "estimator.fit({\"train\": train_input})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleanup\n",
    "Uncomment the following code cell to revert the SageMaker Python SDK to the original version used\n",
    "before running this notebook. This notebook upgrades the SageMaker Python SDK to 2.x, which may\n",
    "cause other example notebooks to break. To learn more about the changes introduced in the\n",
    "SageMaker Python SDK 2.x update, see\n",
    "[Use Version 2.x of the SageMaker Python SDK.](https://sagemaker.readthedocs.io/en/stable/v2.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# _ = subprocess.check_call(\n",
    "#         [sys.executable, \"-m\", \"pip\", \"install\", f\"sagemaker=={original_version}\"]\n",
    "#     )"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:ap-northeast-2:806072073708:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
